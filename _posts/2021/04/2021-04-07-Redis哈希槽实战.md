---
toc:
  beginning: true
giscus_comments: true
layout: post
title: "Redis哈希槽实战"
date: "2021-04-07"
categories: 
  - "Backend Redis"
---

## Redis哈希槽实战

面试题:在高并发的互联网公司中,有1亿条数据需要缓存,请问如何设计存储这批数据?
单台服务器肯定存储不了这么大的数据,一般是分布式存储,就像数据库的分库分表一样存储,那针对缓存redis如何分布式存储这么大的数据?

业界的做法一般有3种:

1.哈希取余分区

针对redis来说1亿条数据,一般是对应1亿个key value,我们把他分别存储在N个节点,如上图N=3,然后用户每次读写操作,根据节点N使用公式hash (key) %N计算出哈希值,
用来决定数据映射到哪一个节点上。
这种方案的优缺点:

优点:简单粗暴,只要提前预估好数据量,然后规划好节点,例如3台、30台、300台节点,就能保证未来一段时间内的数据支撑

缺点:节点扩容或收缩节点的时候就麻烦了,因为每次节点有变动数据节点映射关系需要重新计算,会导致数据的重新迁移,例如原先是3台,后面要新增到8台,要把所有的历史数据按hash (key) %8,重新洗一遍,非常麻烦。

2.一致性哈希分区

3.一致性哈希算法中首先有一个哈希函数,哈希函数产生hash值,所有可能的哈希值构成一个哈希空间,哈希空间为[0,2^32-1],这本来是一个"线性"的空间,但是在算法中通过恰当逻辑控制,使其首尾相衔接,也即是0-2^32,这样就构造一个逻辑上的环形空间。
    {% include figure.liquid loading="eager" path="assets/img/2021/04/image-e57aec29ab734f87863efe8aabde44ae.png" class="img-fluid rounded z-depth-1" zoomable=true width="70%"%}

4.节点映射将集群中的各IP节点映射到环上的某个一位置。假设有四个节点Node A, B, C、D,经过ip地址的哈希函数计算(例如:ash(192.168.1.13)) ,它们的位置如下:
   {% include figure.liquid loading="eager" path="assets/img/2021/04/image-682b5b1211214be8a1ab5055131d60b7.png" class="img-fluid rounded z-depth-1" zoomable=true width="70%"%}
5.路由规则包括存储(setx)和取值(getx)规则。当需要存储一个<key-value>对时,首先计算键key的hash值: hash(key),这个hash值必然对应于一致性hash环上的某个位置,然后沿着这个值按顺时针找到第一个节点,并将该键值对存储在该节点上.
    - 例如有4个存储对象Object A,B、 C,D,经过对Key的哈希计算后,它们的位置如图3)
    - 对于各个Object,它所真正的存储位置是按顺时针找到的第一个存储节点
    - 例如Object A顺时针找到的第一个节点是Node A,所以Node A负责存储Object A, Object B存储在Node B.
    {% include figure.liquid loading="eager" path="assets/img/2021/04/image-e18fe059146a45569ed2471a80e97246.png" class="img-fluid rounded z-depth-1" zoomable=true width="70%"%}

- 一致性哈希如何实现容错性和扩展性?(图4)
  - 容错性

  假设Node C节点挂掉了, Object C的存储丢失,如果要重新把数据补回来时, Object C就会顺时针找到的最新节点是Node D也就是说Node C挂掉了,受影响仅仅包括Node B到Node C区间的数据,并且这些数据会转移到Node D进行存储。

  - 扩展性

  假设现在数据量大了,需要增加一台节点Node x, Node x的位置在Node A到Node B之间,那么受到影响的仅仅是Node A到Node x间的数据,重新把A到x之间的数据洗到Node x上即可


**优点:**

与哈希取余分区相比,容错性和扩展性更灵活,例如node c瘫痪,只影响Node B到Node C区间的数据,影响面小;再例如增加一台节点Node x,只影响到Node A到Node B之间的数据,不会导致哈希取余全部数据重洗。

**缺点:数据倾斜不一致性:**

如果在分片的集群中,节点太少,并且分布不均,一致性哈希算法就会出现部分节点数据太多,部分节点数据太少。也就是说无法控制节点存储数据的分配。如(图5) ,大部分数据都在A上了, B的数据比较少

6.哈希槽分区
   由于一致性哈希分区存在数据倾斜不一致性的问题,故引入了槽的概念

- 哈希槽的概念
  把哈希槽均匀分段,分配给redis节点(图2)
1. redis节点1负责存储5461个哈希槽的数据,编号0号至5460号哈希槽.
2. redis节点2负责存储5462个哈希槽的数据,编号5461号至10922号哈希槽。
3. redis节点3负责存储5461个哈希槽的数据,编号10923号至16383号哈希槽

{% include figure.liquid loading="eager" path="assets/img/2021/04/image-d66ab59086524c42a544d5d2137b5d47.png" class="img-fluid rounded z-depth-1" zoomable=true width="70%"%}

redis节点3，负责存储5461个哈希槽的数据,编号10923号至16383号哈希槽。

- 计算每条数据的slot空间位置如(图3)

将数据key进行哈希取值,映射已经固定大小的hash slot空间上，例如可以采用spring redis的API

```xml
 <dependency>
 <groupld>org.springframework.boot</groupld>
 <artifactid>spring-boot-starter-data-redis </artifactld>
 </dependency>
```

例如、我们往redis设置3条数据

redisTemplate.opsForValue).set("A", "agan1"")<br>
redisTemplate.opsForValue0.set("B","agan2")<br>
redisTemplate.opsForValue0.set("C","agan3")<br>

然后计算key ABC的的slot槽位置

io.lettuce.core.cluster.SlotHash.getSlot(A")-6373<br>
io.lettuce.core.cluster.SlotHash.getSlot("B")= 10374<br>
io.lettuce.core.cluster.SlotHash.getSlot(C)=14503<br>

故，

key AB落在slot空间的5461至10922区间上,并最终存储在Node2上<br>
key C落在slot空间的10923至16383区间上,并最终存储在Node3上<br>


{% include figure.liquid loading="eager" path="assets/img/2021/04/image-cba2a12e430a461ea21d1a8543254a1a.png" class="img-fluid rounded z-depth-1" zoomable=true width="70%"%}

- Redis哈希槽分区的特点:
1. 解耦数据和节点之间的关系,例如数据的读写只要计算出槽号就可以,节点的扩容和收缩只要重新均衡分配槽区间即可;故简化了节点扩容和收缩难度
2. 节点自身维护槽的映射关系,不需要客户端(spring)或者代理服务维护槽分区和数据。
3. 支持节点、槽、键之间的映射意询,用于数据路由、在线伸缩等场景。

-----------------

-----------------

## 哈希槽案例实战：Redis集群哈希槽安装部署

### 1. 利用docker创建容器，创建6个redis实例

```bash
docker create --name redis-node-1 --net host --privileged=true -v /data/redis/share/redis-node-1:/data redis:5.0.7 --cluster-enabled yes --appendonly yes --port 6381
docker create --name redis-node-2 --net host --privileged=true -v /data/redis/share/redis-node-2:/data redis:5.0.7 --cluster-enabled yes --appendonly yes --port 6382
docker create --name redis-node-3 --net host --privileged=true -v /data/redis/share/redis-node-3:/data redis:5.0.7 --cluster-enabled yes --appendonly yes --port 6383
docker create --name redis-node-4 --net host --privileged=true -v /data/redis/share/redis-node-4:/data redis:5.0.7 --cluster-enabled yes --appendonly yes --port 6384
docker create --name redis-node-5 --net host --privileged=true -v /data/redis/share/redis-node-5:/data redis:5.0.7 --cluster-enabled yes --appendonly yes --port 6385
docker create --name redis-node-6 --net host --privileged=true -v /data/redis/share/redis-node-6:/data redis:5.0.7 --cluster-enabled yes --appendonly yes --port 6386

```

>docker create		//创建容器的命令<br>
--name redis-node-1     //容器的名字 例如redis-node-1<br>
--net host		//使用宿主机的IP和i口<br>
--privileged=true 	//docker容器获取宿主机root权限<br>
-v /data/redis/share/redis-node-4:/data //容器的data目录映射到宿机/data/redis/share/redis-node-1<br>
redis:5.0.7		//redis镜像名称和版本号<br>
--cluster-enabled yes   //redis.conf的配置:开启redis集群<br>
--appendonly yes	//redis.conf的配置:开启数据持久化<br>
--port 6381		//redis.conf的配置: redis端口号<br>


### 2. 启动容器

```bash
docker start redis-node-1 redis-node-2 redis-node-3 redis-node-4 redis-node-5 redis-node-6
```



### 3. 查看容器(6台容器)

```bash
docker ps
```

### 4. 进入节点redis-node-1容器中

```bash
docker exec -it redis-node-1 /bin/bash
```

**参数create表示创建一个新的集群, --replisas 1表示为每个master创建一个slave。**

```bash
redis-cli --cluster create 192.168.1.138:6381 192.168.1.138:6382 192.168.1.138:6383 192.168.1.138:6384 192.168.1.138:6385 192.168.1.138:6386 --cluster-replicas 1
```

执行结果：

{% include figure.liquid loading="eager" path="assets/img/2021/04/image-655a2db1ec3849bc8c41fc06e5048992.png" class="img-fluid rounded z-depth-1" zoomable=true width="70%"%}

{% include figure.liquid loading="eager" path="assets/img/2021/04/image-ed3f2dc8e30a47d09221ef9e94cc47a0.png" class="img-fluid rounded z-depth-1" zoomable=true width="70%"%}

### 5. 使用cluster info命令查看集群状态

- 先进入容器

```bash
redis-cli -h 192.168.1.138 -p 6381 -c
```

- 查看info

```bash
cluster info
```

{% include figure.liquid loading="eager" path="assets/img/2021/04/image-db96af3857d742fbbfa055d8119349ec.png" class="img-fluid rounded z-depth-1" zoomable=true width="70%"%}


### 6. 使用cluster nodes查看节点的状态

```bash
cluster nodes
```

{% include figure.liquid loading="eager" path="assets/img/2021/04/image-7c5b6ba8e12340dcafcc6a0304e9dd51.png" class="img-fluid rounded z-depth-1" zoomable=true width="70%"%}



## 哈希槽案例实战：Redis主从切换

### 1. 客户端验证

```bash
set user:100 agan
set user:200 alext
```

{% include figure.liquid loading="eager" path="assets/img/2021/04/image-fdb56886b62d458ea7313f4793ff9062.png" class="img-fluid rounded z-depth-1" zoomable=true width="70%"%}


### 2. 查看集群信息

```bash
exit

redis-cli --cluster check 192.168.1.138:6381 
```

{% include figure.liquid loading="eager" path="assets/img/2021/04/image-04aaa9f41dde431492416ae03fcd151a.png" class="img-fluid rounded z-depth-1" zoomable=true width="70%"%}


### 3.  主从切换

```bash
cluster nodes
```
{% include figure.liquid loading="eager" path="assets/img/2021/04/image-abcf2f191c92418a926e8255302415ad.png" class="img-fluid rounded z-depth-1" zoomable=true width="70%"%}

**从上可以看出6381是主，6384是从，我们先停掉6381查看效果**

- 查看镜像

```bash
docker ps
```

{% include figure.liquid loading="eager" path="assets/img/2021/04/image-87227ed6982e407db198271c4749812c.png" class="img-fluid rounded z-depth-1" zoomable=true width="70%"%}

- 停掉

```bash
docker stop redis-node-1
```

- 进入容器，进入集群

```bash
docker exec -it redis-node-2 /bin/bash

redis-cli -h 192.168.1.138 -p 6382 -c
```

- 查看目前的集群效果，查看节点信息

```bash
cluster nodes
```

{% include figure.liquid loading="eager" path="assets/img/2021/04/image-a96ce3f7b01e47e4b5123f275f75a842.png" class="img-fluid rounded z-depth-1" zoomable=true width="70%"%}


## 哈希槽案例实战：集群扩容演练

由于之前把6381变成fail，6386变成master<br>
我们先进行还原，6381重新变成master，6386变成slave

```bash
redis-cli -h 192.168.1.138 -p 6382 -c 
cluster nodes
```

- 先把6381启动,然后停止6386

```bash
docker start redis-node-1

docker stop redis-node-6
```

{% include figure.liquid loading="eager" path="assets/img/2021/04/image-17fac9b3268f401ea6d288d2224c1a57.png" class="img-fluid rounded z-depth-1" zoomable=true width="70%"%}

- 再启动6386，成为slave

```bash
docker start redis-node-6
```
{% include figure.liquid loading="eager" path="assets/img/2021/04/image-0fa697e738ab45debe66c293b4f6b3e6.png" class="img-fluid rounded z-depth-1" zoomable=true width="70%"%}


**目前架构情况**

{% include figure.liquid loading="eager" path="assets/img/2021/04/image-dacff31761b34216a238e97b35a9ff16.png" class="img-fluid rounded z-depth-1" zoomable=true width="70%"%}


### 1. 查看集群信息

```bash
exit

redis-cli --cluster check 192.168.1.138:6381
```

{% include figure.liquid loading="eager" path="assets/img/2021/04/image-fdcf80b8833c4d3783c048de4ca31cad.png" class="img-fluid rounded z-depth-1" zoomable=true width="70%"%}


### 2. 增加2个redis 节点7和8

利用docker，创建2个redis集群

```bash
docker create --name redis-node-7 --net host --privileged=true -v /data/redis/share/redis-node-7:/data redis:5.0.7 --cluster-enabled yes --appendonly yes --port 6387

docker create --name redis-node-8 --net host --privileged=true -v /data/redis/share/redis-node-8:/data redis:5.0.7 --cluster-enabled yes --appendonly yes --port 6388
```

### 3. 启动容器

```bash
docker start redis-node-7 redis-node-8
```

{% include figure.liquid loading="eager" path="assets/img/2021/04/image-8381a2d3122a4d3994450b22924483dc.png" class="img-fluid rounded z-depth-1" zoomable=true width="70%"%}

### 4. 先进入节点redis-node-7容器中

```bash
docker exec -it redis-node-7 /bin/bash
```

### 5. 添加master节点

192.168.1.138:6387代表新增的，192.168.1.138:6381代表原集群的任意一个节点

```bash
redis-cli --cluster add-node 192.168.1.138:6387 192.168.1.138:6381
```

{% include figure.liquid loading="eager" path="assets/img/2021/04/image-57883c9615a04871ab9bb1ffe5c7a3e2.png" class="img-fluid rounded z-depth-1" zoomable=true width="70%"%}


**再一次检查集群**

```bash
redis-cli --cluster check 192.168.1.138:6381 
```

{% include figure.liquid loading="eager" path="assets/img/2021/04/image-56ceb428f891437a9b268c11bc684656.png" class="img-fluid rounded z-depth-1" zoomable=true width="70%"%}

{% include figure.liquid loading="eager" path="assets/img/2021/04/image-a37c3a457fb94e5d919ba9da4dbb12e8.png" class="img-fluid rounded z-depth-1" zoomable=true width="70%"%}

## 哈希槽案例实战：分配哈希槽

### 1. 进入集群

新的节点加入集群,就需要重新分配槽,用```redis-cli-cluster reshard ip:port```命令来重新分配槽;

ip: port指集群中任意一个节点就行,如下:


```bash
redis-cli --cluster reshard 192.168.1.138:6381 
```

{% include figure.liquid loading="eager" path="assets/img/2021/04/image-c10cb3c7b1f045cd853bb16f5f53d3fa.png" class="img-fluid rounded z-depth-1" zoomable=true width="70%"%}

### 2. 重新查看集群

```bash
redis-cli --cluster check 192.168.1.138:6381 
```

{% include figure.liquid loading="eager" path="assets/img/2021/04/image-ba760510f44142548ecdd2f6db1185d2.png" class="img-fluid rounded z-depth-1" zoomable=true width="70%"%}

{% include figure.liquid loading="eager" path="assets/img/2021/04/image-1b127831a8a541579f5be9b70883d830.png" class="img-fluid rounded z-depth-1" zoomable=true width="70%"%}

### 3. 为master4(node7)配置slave4节点

add-node:后面的分别跟着新加入的slave和slave对应的master <br>
cluster-slave:表示加入的是slave节点<br>
--cluster-master-id:表示slave对应的master的node ID

```bash
redis-cli --cluster add-node 192.168.1.138:6388 192.168.1.138:6387 --cluster-slave --cluster-master-id  55ec78fb6b6e501e717ad88643b64
```

### 4. 查看集群情况

```bash
redis-cli --cluster check 192.168.1.138:6387
```

{% include figure.liquid loading="eager" path="assets/img/2021/04/image-cb77bc8f0d6445b49f719419c27aa0c1.png" class="img-fluid rounded z-depth-1" zoomable=true width="70%"%}


## 哈希槽案例实战：哈希槽下线，收缩
删除集群中的2个节点，node7与node8

### 1. 查看集群节点信息
```bash
redis-cli --cluster check 192.168.1.138:6381
```

### 2. 把6387master对呀的slave6388删除

del-node后面跟着slave节点的ip:port 和node ID

```bash
redis-cli --cluster del-node 192.168.1.138:6388   84acba0a54a5ac8ee0541eb973ef2ecdbbfa0e16
```
{% include figure.liquid loading="eager" path="assets/img/2021/04/image-906f8cc159dd4dd98d2bec1075f80ec5.png" class="img-fluid rounded z-depth-1" zoomable=true width="70%"%}

### 3. 查看集群情况

```bash
redis-cli --cluster check 192.168.1.138:6381
```
{% include figure.liquid loading="eager" path="assets/img/2021/04/image-643b4582d0654f01a4ca8e779d4275e4.png" class="img-fluid rounded z-depth-1" zoomable=true width="70%"%}


### 4. 查看容器，发现node8没了

```bash
docker ps
```

{% include figure.liquid loading="eager" path="assets/img/2021/04/image-302726e17923416383d3fd6b6603bd25.png" class="img-fluid rounded z-depth-1" zoomable=true width="70%"%}


### 5. 重新分配master的slot

```bash
redis-cli --cluster reshard 192.168.1.138:6381 
```

{% include figure.liquid loading="eager" path="assets/img/2021/04/image-a71a0188d67b4479a599cfbe7801e9a7.png" class="img-fluid rounded z-depth-1" zoomable=true width="70%"%}

- 查看集群节点情况

```bash
redis-cli --cluster check 192.168.1.138:6381
```
{% include figure.liquid loading="eager" path="assets/img/2021/04/image-80be9d370bc64cc3a4634d4d038e334e.png" class="img-fluid rounded z-depth-1" zoomable=true width="70%"%}

### 6. 删除master节点node7

del-node后面跟着slave节点的ip:port 和node ID

```bash
redis-cli --cluster del-node 192.168.1.138:6387     0a925be15fcb5f749b9014e86ebf84d26797306b
```
{% include figure.liquid loading="eager" path="assets/img/2021/04/image-1f554a4c8a57466480bed58db2ad41a3.png" class="img-fluid rounded z-depth-1" zoomable=true width="70%"%}

### 7.继续查看集群情况

```bash
exit

docker exec -it redis-node-2 /bin/bash

redis-cli --cluster check 192.168.1.138:6381
```

{% include figure.liquid loading="eager" path="assets/img/2021/04/image-be8174bdad31455db23c36feae02a88e.png" class="img-fluid rounded z-depth-1" zoomable=true width="70%"%}



----------------------------------

## 哈希槽案例实战：spring集成redis集群

1.redis的序列化

2.配置文件

```properties
##Redis 配置
##Redis数据库索引(默认为0
spring.redis.database=1
##Redis服务器地址
#spring.redis.host=127.0.0.1
##Redis服务器连接端口
spring.redis.port=4637
##Redis服务器连接密码(默认为空)
#spring.redis.password=agan
##连接池最大连接罚(使用负值表示没有限制)
spring.redis.lettuce.pool.max-active=8
#带连接池中的最大空闲连接
spring.redis.lettuce.pool.max-idle=8
##连接池最大阻塞等待时间(使用负值表示没有限制)
spring.redis.lettuce.pool.max-wait=-1ms
#带连接池中的最小空闲连接
spring.redis.lettuce.pool.min-idle=0
#spring.redis.sentinel.master=# Name of Redis server.
#spring.redis.sentinel.nodes= # Comma-separated list of host:port pairs.
Spring. redis. timeout=1m
spring.redis.cluster.nodes=192.168.1.138:6381,192.168.1.138:6382
```

3.往redis加入10条数据

```java
@GetMapping(value = "/init")
public void refreshData(){
	for(int i-8;1<10;1++){
		String key="user:"+i:
		this.redisTemplate.opsForvalue().set(key,i);
		log.debug("set key={}, value={}",key, i); .
	}
}
```

4.测试一
   所有的数据会被分成3份，存到3个master里面，进入6381与6382查看里面的数据是否不一样

```bash
docker exec -it redis-node-1 /bin/bash

redis-cli -h 192.168.1.138 -p 6381 -c
keys *

redis-cli -h 192.168.1.138 -p 6382 -c
keys *

redis-cli -h 192.168.1.138 -p 6383 -c
keys *
```

{% include figure.liquid loading="eager" path="assets/img/2021/04/image-2a2c7a4c8a2145fbb6b3f9b873a9e717.png" class="img-fluid rounded z-depth-1" zoomable=true width="70%"%}


5.测试二
   验证主从的数据是否一致

查看192.168.1.138:6386和1192.168.1.138:6381的数据是否一样

```bash
redis-cli -h 192.168.1.138 -p 6381 -c
keys *

redis-cli -h 192.168.1.138 -p 6386 -c
keys *
```
{% include figure.liquid loading="eager" path="assets/img/2021/04/image-6df851e8084348eeacc67a7fed804b0b.png" class="img-fluid rounded z-depth-1" zoomable=true width="70%"%}


**数据一致，只是顺序不同**

> 结论<br>
> 1. redis的集群读写操作,数据都会被切成3份,放在不同的master里面
> 2. redis的主从,里面的数据一定是一样的

<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Pytorch VS Tensorflow | Xingxing Zheng </title> <meta name="author" content="Xingxing Zheng"> <meta name="description" content="Welcome to Xingxing's blog, where I share my thoughts and experiences on various topics. "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://zhengstar94.github.io//blog/2018/pytorch-vs-tensorflow/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Xingxing</span> Zheng </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">archive </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/archive_by_year">by year</a> <a class="dropdown-item " href="/archive_by_tag">by category</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Pytorch VS Tensorflow</h1> <p class="post-meta"> Created in July 31, 2018 </p> <p class="post-tags"> <a href="/blog/2018"> <i class="fa-solid fa-calendar fa-sm"></i> 2018 </a>   ·   <a href="/blog/category/dl-ml-python"> <i class="fa-solid fa-tag fa-sm"></i> dl-ml-python</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Right now Pytorch and Tensorflow are the extremely popular AI frameworks , but AI researchers may find it a little bit tangled when it comes to the question that which framework to use. So rather than choose one of them to learn, why not use both of them since they will come in handy later on. So I’m going to introduce both of them from the perspective of  vanilla structure and API.</p> <h3 id="pytorch"><strong>Pytorch</strong></h3> <p>A PyTorch Tensor is conceptionally similar to a numpy array: it is an n-dimensional grid of numbers, and like numpy PyTorch provides many functions to efficiently operate on Tensors.</p> <p>all of the packages we import in this blog for pytorch part:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mi">1</span> <span class="kn">import</span> <span class="n">torch</span>
 <span class="mi">2</span> <span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
 <span class="mi">3</span> <span class="kn">import</span> <span class="n">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>
 <span class="mi">4</span> <span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
 <span class="mi">5</span> <span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">sampler</span>
 <span class="mi">6</span> 
 <span class="mi">7</span> <span class="kn">import</span> <span class="n">torchvision.datasets</span> <span class="k">as</span> <span class="n">dset</span>
 <span class="mi">8</span> <span class="kn">import</span> <span class="n">torchvision.transforms</span> <span class="k">as</span> <span class="n">T</span>
 <span class="mi">9</span> 
<span class="mi">10</span> <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
</code></pre></div></div> <p>Image data is typically stored in a Tensor shape x = N * C * H * W</p> <ul> <li>N is the number of datapoints</li> <li>C is the number of channels</li> <li>H is the height of the intermediate feature map in pixels</li> <li>W is the height of the intermediate feature map in pixels</li> </ul> <p>When we process the fully connected layer, we need to flatten the C * H *W values into a single vector per image</p> <p>1 def flatten(x): 2 N = x.shape[0] # read in N, C, H, W 3 return x.view(N, -1) # “flatten” the C * H * W values into a single vector per image</p> <p><strong>Three-layer network </strong> Implement a vanilla structure of three-layer netwok, and the architecture will be as follows:</p> <ol> <li>A convolutional layer (with bias) with <code class="language-plaintext highlighter-rouge">channel_1</code> filters, each with shape <code class="language-plaintext highlighter-rouge">KW1 x KH1</code>, and zero-padding of two</li> <li>ReLU nonlinearity</li> <li>A convolutional layer (with bias) with <code class="language-plaintext highlighter-rouge">channel_2</code> filters, each with shape <code class="language-plaintext highlighter-rouge">KW2 x KH2</code>, and zero-padding of one</li> <li>ReLU nonlinearity</li> <li>Fully-connected layer with bias, producing scores for C classes.</li> </ol> <p>Nomally, the function contains 2 parameters, which are input x and params, and the params are specified based on how many layers and what type of architecture you’re using.</p> <p>Notice that this architecture includes 2 convolutional layer, we need the conv2d function from torch.nn.functional.conv2d<img src="https://zhengliangliang.files.wordpress.com/2018/07/2018-07-31_123127.jpg" alt="2018-07-31_123127.jpg"></p> <p>And the core functions are conv2d,relu and mm</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mi">1</span> <span class="k">def</span> <span class="nf">three_layer_convnet</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
 <span class="mi">2</span>    <span class="sh">"""</span><span class="s">
 3    Performs the forward pass of a three-layer convolutional network with the
 4    architecture defined above.
 5 
 6    Inputs:
 7    - x: A PyTorch Tensor of shape (N, 3, H, W) giving a minibatch of images
 8    - params: A list of PyTorch Tensors giving the weights and biases for the
 9      network; should contain the following:
10       - conv_w1: PyTorch Tensor of shape (channel_1, 3, KH1, KW1) giving weights
11         for the first convolutional layer
12       - conv_b1: PyTorch Tensor of shape (channel_1,) giving biases for the first
13         convolutional layer
14       - conv_w2: PyTorch Tensor of shape (channel_2, channel_1, KH2, KW2) giving
15         weights for the second convolutional layer
16       - conv_b2: PyTorch Tensor of shape (channel_2,) giving biases for the second
17         convolutional layer
18       - fc_w: PyTorch Tensor giving weights for the fully-connected layer. Can you
19         figure out what the shape should be?
20       - fc_b: PyTorch Tensor giving biases for the fully-connected layer. Can you
21         figure out what the shape should be?
22     
23     Returns:
24     - scores: PyTorch Tensor of shape (N, C) giving classification scores for x
25     </span><span class="sh">"""</span>
<span class="mi">26</span>     <span class="n">conv_w1</span><span class="p">,</span> <span class="n">conv_b1</span><span class="p">,</span> <span class="n">conv_w2</span><span class="p">,</span> <span class="n">conv_b2</span><span class="p">,</span> <span class="n">fc_w</span><span class="p">,</span> <span class="n">fc_b</span> <span class="o">=</span> <span class="n">params</span>
<span class="mi">27</span>     <span class="n">scores</span> <span class="o">=</span> <span class="bp">None</span>
<span class="mi">28</span>     <span class="c1">################################################################################
</span><span class="mi">29</span>     <span class="c1"># TODO: Implement the forward pass for the three-layer ConvNet.                #
</span><span class="mi">30</span>     <span class="c1">################################################################################
</span><span class="mi">31</span>     <span class="n">conv1</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">conv2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">conv_w1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">conv_b1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="mi">32</span>     <span class="n">relu1</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">conv1</span><span class="p">)</span>
<span class="mi">33</span>     <span class="n">conv2</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">conv2d</span><span class="p">(</span><span class="n">relu1</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">conv_w2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">conv_b2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="mi">34</span>     <span class="n">relu2</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">conv2</span><span class="p">)</span>
<span class="mi">35</span>     <span class="n">relu2_flat</span> <span class="o">=</span> <span class="nf">flatten</span><span class="p">(</span><span class="n">relu2</span><span class="p">)</span>
<span class="mi">36</span>     <span class="n">scores</span> <span class="o">=</span> <span class="n">relu2_flat</span><span class="p">.</span><span class="nf">mm</span><span class="p">(</span><span class="n">fc_w</span><span class="p">)</span> <span class="o">+</span> <span class="n">fc_b</span>
<span class="mi">37</span>     <span class="c1">#pass
</span><span class="mi">38</span>     <span class="c1">################################################################################
</span><span class="mi">39</span>     <span class="c1">#                                 END OF YOUR CODE                             #
</span><span class="mi">40</span>     <span class="c1">################################################################################
</span><span class="mi">41</span>     <span class="k">return</span> <span class="n">scores</span>
</code></pre></div></div> <p><strong>Pytorch Initialization :</strong></p> <ul> <li> <code class="language-plaintext highlighter-rouge">random_weight(shape)</code> initializes a weight tensor with the Kaiming normalization method.(normally do it with weights)</li> <li> <code class="language-plaintext highlighter-rouge">zero_weight(shape)</code> initializes a weight tensor with all zeros. Useful for instantiating bias parameters.(normally do it with biases) <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code> <span class="mi">1</span> <span class="k">def</span> <span class="nf">random_weight</span><span class="p">(</span><span class="n">shape</span><span class="p">):</span>
 <span class="mi">2</span>    <span class="sh">"""</span><span class="s">
 3    Create random Tensors for weights; setting requires_grad=True means that we
 4    want to compute gradients for these Tensors during the backward pass.
 5    We use Kaiming normalization: sqrt(2 / fan_in)
 6    </span><span class="sh">"""</span>
 <span class="mi">7</span>    <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>  <span class="c1"># FC weight
</span> <span class="mi">8</span>        <span class="n">fan_in</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
 <span class="mi">9</span>    <span class="k">else</span><span class="p">:</span>
<span class="mi">10</span>         <span class="n">fan_in</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">prod</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span> <span class="c1"># conv weight [out_channel, in_channel, kH, kW]
</span><span class="mi">11</span>     <span class="c1"># randn is standard normal distribution generator. 
</span><span class="mi">12</span>     <span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mf">2.</span> <span class="o">/</span> <span class="n">fan_in</span><span class="p">)</span>
<span class="mi">13</span>     <span class="n">w</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>
<span class="mi">14</span>     <span class="k">return</span> <span class="n">w</span>
<span class="mi">15</span> 
<span class="mi">16</span> <span class="k">def</span> <span class="nf">zero_weight</span><span class="p">(</span><span class="n">shape</span><span class="p">):</span>
<span class="mi">17</span>     <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="mi">18</span> 
<span class="mi">19</span> <span class="c1"># create a weight of shape [3 x 5]
</span><span class="mi">20</span> <span class="c1"># you should see the type `torch.cuda.FloatTensor` if you use GPU. 
</span><span class="mi">21</span> <span class="c1"># Otherwise it should be `torch.FloatTensor`
</span><span class="mi">22</span> <span class="nf">random_weight</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
</code></pre></div> </div> <p><strong>PyTorch: Check Accuracy</strong></p> </li> </ul> <p>When checking accuracy we don’t need to compute any gradients; as a result we don’t need PyTorch to build a computational graph for us when we compute scores. To prevent a graph from being built we scope our computation under a <code class="language-plaintext highlighter-rouge">torch.no_grad()</code> context manager.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
 <span class="mi">1</span> <span class="k">def</span> <span class="nf">check_accuracy_part2</span><span class="p">(</span><span class="n">loader</span><span class="p">,</span> <span class="n">model_fn</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
 <span class="mi">2</span>    <span class="sh">"""</span><span class="s">
 3    Check the accuracy of a classification model.
 4    
 5    Inputs:
 6    - loader: A DataLoader for the data split we want to check
 7    - model_fn: A function that performs the forward pass of the model,
 8      with the signature scores = model_fn(x, params)
 9    - params: List of PyTorch Tensors giving parameters of the model
10     
11     Returns: Nothing, but prints the accuracy of the model
12     </span><span class="sh">"""</span>
<span class="mi">13</span>     <span class="n">split</span> <span class="o">=</span> <span class="sh">'</span><span class="s">val</span><span class="sh">'</span> <span class="k">if</span> <span class="n">loader</span><span class="p">.</span><span class="n">dataset</span><span class="p">.</span><span class="n">train</span> <span class="k">else</span> <span class="sh">'</span><span class="s">test</span><span class="sh">'</span>
<span class="mi">14</span>     <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Checking accuracy on the %s set</span><span class="sh">'</span> <span class="o">%</span> <span class="n">split</span><span class="p">)</span>
<span class="mi">15</span>     <span class="n">num_correct</span><span class="p">,</span> <span class="n">num_samples</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
<span class="mi">16</span>     <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
<span class="mi">17</span>         <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
<span class="mi">18</span>             <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>  <span class="c1"># move to device, e.g. GPU
</span><span class="mi">19</span>             <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">int64</span><span class="p">)</span>
<span class="mi">20</span>             <span class="n">scores</span> <span class="o">=</span> <span class="nf">model_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
<span class="mi">21</span>             <span class="n">_</span><span class="p">,</span> <span class="n">preds</span> <span class="o">=</span> <span class="n">scores</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="mi">22</span>             <span class="n">num_correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">preds</span> <span class="o">==</span> <span class="n">y</span><span class="p">).</span><span class="nf">sum</span><span class="p">()</span>
<span class="mi">23</span>             <span class="n">num_samples</span> <span class="o">+=</span> <span class="n">preds</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="mi">24</span>         <span class="n">acc</span> <span class="o">=</span> <span class="nf">float</span><span class="p">(</span><span class="n">num_correct</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_samples</span>
<span class="mi">25</span>         <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Got %d / %d correct (%.2f%%)</span><span class="sh">'</span> <span class="o">%</span> <span class="p">(</span><span class="n">num_correct</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">,</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">acc</span><span class="p">))</span>
</code></pre></div></div> <p><strong>PyTorch: Training Loop</strong></p> <p>The final step is to train the model , firstly move the data to proper device and then compute the loss , then using SGD to compute the gradients. then call the check accuracy function to print out the accuracy</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mi">1</span> <span class="k">def</span> <span class="nf">train_part2</span><span class="p">(</span><span class="n">model_fn</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
 <span class="mi">2</span>    <span class="sh">"""</span><span class="s">
 3    Train a model on CIFAR-10.
 4    
 5    Inputs:
 6    - model_fn: A Python function that performs the forward pass of the model.
 7      It should have the signature scores = model_fn(x, params) where x is a
 8      PyTorch Tensor of image data, params is a list of PyTorch Tensors giving
 9      model weights, and scores is a PyTorch Tensor of shape (N, C) giving
10       scores for the elements in x.
11     - params: List of PyTorch Tensors giving weights for the model
12     - learning_rate: Python scalar giving the learning rate to use for SGD
13     
14     Returns: Nothing
15     </span><span class="sh">"""</span>
<span class="mi">16</span>     <span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">loader_train</span><span class="p">):</span>
<span class="mi">17</span>         <span class="c1"># Move the data to the proper device (GPU or CPU)
</span><span class="mi">18</span>         <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
<span class="mi">19</span>         <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">)</span>
<span class="mi">20</span> 
<span class="mi">21</span>         <span class="c1"># Forward pass: compute scores and loss
</span><span class="mi">22</span>         <span class="n">scores</span> <span class="o">=</span> <span class="nf">model_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
<span class="mi">23</span>         <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">cross_entropy</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="mi">24</span> 
<span class="mi">25</span>         <span class="c1"># Backward pass: PyTorch figures out which Tensors in the computational
</span><span class="mi">26</span>         <span class="c1"># graph has requires_grad=True and uses backpropagation to compute the
</span><span class="mi">27</span>         <span class="c1"># gradient of the loss with respect to these Tensors, and stores the
</span><span class="mi">28</span>         <span class="c1"># gradients in the .grad attribute of each Tensor.
</span><span class="mi">29</span>         <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
<span class="mi">30</span> 
<span class="mi">31</span>         <span class="c1"># Update parameters. We don't want to backpropagate through the
</span><span class="mi">32</span>         <span class="c1"># parameter updates, so we scope the updates under a torch.no_grad()
</span><span class="mi">33</span>         <span class="c1"># context manager to prevent a computational graph from being built.
</span><span class="mi">34</span>         <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
<span class="mi">35</span>             <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>
<span class="mi">36</span>                 <span class="n">w</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">w</span><span class="p">.</span><span class="n">grad</span>
<span class="mi">37</span> 
<span class="mi">38</span>                 <span class="c1"># Manually zero the gradients after running the backward pass
</span><span class="mi">39</span>                 <span class="n">w</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="nf">zero_</span><span class="p">()</span>
<span class="mi">40</span> 
<span class="mi">41</span>         <span class="k">if</span> <span class="n">t</span> <span class="o">%</span> <span class="n">print_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span class="mi">42</span>             <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Iteration %d, loss = %.4f</span><span class="sh">'</span> <span class="o">%</span> <span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()))</span>
<span class="mi">43</span>             <span class="nf">check_accuracy_part2</span><span class="p">(</span><span class="n">loader_val</span><span class="p">,</span> <span class="n">model_fn</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
<span class="mi">44</span>             <span class="nf">print</span><span class="p">()</span>
</code></pre></div></div> <p>To sum up, the whole process will be 1. Initialize hidden layer size and learning rate,weights 2. Passing data and params( in train function) to three_layer_convnet 3. After computing the scores,then calculate the cross entropy loss and start backward part and upgrating weights(SGD) 4. finally print out the accuracy</p> <h3 id="module-api-2-layer-network"><strong>Module API: 2-layer network:</strong></h3> <p>Barebone PyTorch requires that we track all the parameter tensors by hand. This is fine for small networks with a few tensors, but it would be extremely inconvenient and error-prone to track tens or hundreds of tensors in larger networks.</p> <p>To use the Module API, follow the steps below:</p> <ol> <li>Subclass <code class="language-plaintext highlighter-rouge">nn.Module</code>. Give your network class an intuitive name like <code class="language-plaintext highlighter-rouge">TwoLayerFC</code>.</li> <li>In the constructor <code class="language-plaintext highlighter-rouge">__init__()</code>, define all the layers you need as class attributes. Layer objects like <code class="language-plaintext highlighter-rouge">nn.Linear</code> and <code class="language-plaintext highlighter-rouge">nn.Conv2d</code> are themselves <code class="language-plaintext highlighter-rouge">nn.Module</code> subclasses and contain learnable parameters, so that you don’t have to instantiate the raw tensors yourself. <code class="language-plaintext highlighter-rouge">nn.Module</code> will track these internal parameters for you. Refer to the <a href="http://pytorch.org/docs/master/nn.html" rel="external nofollow noopener" target="_blank">doc</a> to learn more about the dozens of builtin layers. <strong>Warning</strong>: don’t forget to call the <code class="language-plaintext highlighter-rouge">super().__init__()</code> first!</li> <li>In the <code class="language-plaintext highlighter-rouge">forward()</code> method, define the <em>connectivity</em> of your network. You should use the attributes defined in <code class="language-plaintext highlighter-rouge">__init__</code> as function calls that take tensor as input and output the “transformed” tensor. Do <em>not</em> create any new layers with learnable parameters in <code class="language-plaintext highlighter-rouge">forward()</code>! All of them must be declared upfront in <code class="language-plaintext highlighter-rouge">__init__</code>.</li> </ol> <p>Example for following architecture:</p> <ol> <li>Convolutional layer with <code class="language-plaintext highlighter-rouge">channel_1</code> 5x5 filters with zero-padding of 2</li> <li>ReLU</li> <li>Convolutional layer with <code class="language-plaintext highlighter-rouge">channel_2</code> 3x3 filters with zero-padding of 1</li> <li>ReLU</li> <li>Fully-connected layer to <code class="language-plaintext highlighter-rouge">num_classes</code> classes</li> </ol> <p>and all of the functions are from nn.Module, in the init funcution , we setup the layers information, and there are kaiming_normal and constant initilization function in the nn.Module</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mi">1</span> <span class="k">class</span> <span class="nc">ThreeLayerConvNet</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
 <span class="mi">2</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channel</span><span class="p">,</span> <span class="n">channel_1</span><span class="p">,</span> <span class="n">channel_2</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">):</span>
 <span class="mi">3</span>        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
 <span class="mi">4</span>        <span class="c1">########################################################################
</span> <span class="mi">5</span>        <span class="c1"># TODO: Set up the layers you need for a three-layer ConvNet with the  #
</span> <span class="mi">6</span>        <span class="c1"># architecture defined above.                                          #
</span> <span class="mi">7</span>        <span class="c1">########################################################################
</span> <span class="mi">8</span>        <span class="n">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channel</span><span class="p">,</span><span class="n">channel_1</span><span class="p">,</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span><span class="n">padding</span> <span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
 <span class="mi">9</span>        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">kaiming_normal_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">conv1</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span>
<span class="mi">10</span>         <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">constant_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">conv1</span><span class="p">.</span><span class="n">bias</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="mi">11</span>         
<span class="mi">12</span>         <span class="n">self</span><span class="p">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">channel_1</span><span class="p">,</span><span class="n">channel_2</span><span class="p">,</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span><span class="n">padding</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span><span class="n">bias</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="mi">13</span>         <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">kaiming_normal_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">conv1</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span>
<span class="mi">14</span>         <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">constant_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">conv1</span><span class="p">.</span><span class="n">bias</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="mi">15</span>         
<span class="mi">16</span>         <span class="n">self</span><span class="p">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">channel_2</span><span class="o">*</span><span class="mi">32</span><span class="o">*</span><span class="mi">32</span><span class="p">,</span><span class="n">num_classes</span><span class="p">)</span>
<span class="mi">17</span>         <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">kaiming_normal_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">fc</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span>
<span class="mi">18</span>         <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">constant_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">fc</span><span class="p">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="mi">19</span>         
<span class="mi">20</span>         <span class="c1">#pass
</span><span class="mi">21</span>         <span class="c1">########################################################################
</span><span class="mi">22</span>         <span class="c1">#                          END OF YOUR CODE                            # 
</span><span class="mi">23</span>         <span class="c1">########################################################################
</span><span class="mi">24</span> 
<span class="mi">25</span>     <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="mi">26</span>         <span class="n">scores</span> <span class="o">=</span> <span class="bp">None</span>
<span class="mi">27</span>         <span class="c1">########################################################################
</span><span class="mi">28</span>         <span class="c1"># TODO: Implement the forward function for a 3-layer ConvNet. you      #
</span><span class="mi">29</span>         <span class="c1"># should use the layers you defined in __init__ and specify the        #
</span><span class="mi">30</span>         <span class="c1"># connectivity of those layers in forward()                            #
</span><span class="mi">31</span>         <span class="c1">########################################################################
</span><span class="mi">32</span>         <span class="n">relu1</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="mi">33</span>         <span class="n">relu2</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">conv2</span><span class="p">(</span><span class="n">relu1</span><span class="p">))</span>
<span class="mi">34</span>         <span class="n">scores</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc</span><span class="p">(</span><span class="nf">flatten</span><span class="p">(</span><span class="n">relu2</span><span class="p">))</span>
<span class="mi">35</span>         <span class="c1">#pass
</span><span class="mi">36</span>         <span class="c1">########################################################################
</span><span class="mi">37</span>         <span class="c1">#                             END OF YOUR CODE                         #
</span><span class="mi">38</span>         <span class="c1">########################################################################
</span><span class="mi">39</span>         <span class="k">return</span> <span class="n">scores</span>
</code></pre></div></div> <p><strong>Module API: Check Accuracy</strong> This version is slightly different from the one in part II. You don’t manually pass in the parameters anymore.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mi">1</span> <span class="k">def</span> <span class="nf">check_accuracy_part34</span><span class="p">(</span><span class="n">loader</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
 <span class="mi">2</span>    <span class="k">if</span> <span class="n">loader</span><span class="p">.</span><span class="n">dataset</span><span class="p">.</span><span class="n">train</span><span class="p">:</span>
 <span class="mi">3</span>        <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Checking accuracy on validation set</span><span class="sh">'</span><span class="p">)</span>
 <span class="mi">4</span>    <span class="k">else</span><span class="p">:</span>
 <span class="mi">5</span>        <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Checking accuracy on test set</span><span class="sh">'</span><span class="p">)</span>   
 <span class="mi">6</span>    <span class="n">num_correct</span> <span class="o">=</span> <span class="mi">0</span>
 <span class="mi">7</span>    <span class="n">num_samples</span> <span class="o">=</span> <span class="mi">0</span>
 <span class="mi">8</span>    <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>  <span class="c1"># set model to evaluation mode
</span> <span class="mi">9</span>    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
<span class="mi">10</span>         <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
<span class="mi">11</span>             <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>  <span class="c1"># move to device, e.g. GPU
</span><span class="mi">12</span>             <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">)</span>
<span class="mi">13</span>             <span class="n">scores</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="mi">14</span>             <span class="n">_</span><span class="p">,</span> <span class="n">preds</span> <span class="o">=</span> <span class="n">scores</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="mi">15</span>             <span class="n">num_correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">preds</span> <span class="o">==</span> <span class="n">y</span><span class="p">).</span><span class="nf">sum</span><span class="p">()</span>
<span class="mi">16</span>             <span class="n">num_samples</span> <span class="o">+=</span> <span class="n">preds</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="mi">17</span>         <span class="n">acc</span> <span class="o">=</span> <span class="nf">float</span><span class="p">(</span><span class="n">num_correct</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_samples</span>
<span class="mi">18</span>         <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Got %d / %d correct (%.2f)</span><span class="sh">'</span> <span class="o">%</span> <span class="p">(</span><span class="n">num_correct</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">,</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">acc</span><span class="p">))</span>
</code></pre></div></div> <p><strong>Module API : Training Loop</strong></p> <p>We also use a slightly different training loop. Rather than updating the values of the weights ourselves, we use an Optimizer object from the <code class="language-plaintext highlighter-rouge">torch.optim</code> package, which abstract the notion of an optimization algorithm and provides implementations of most of the algorithms commonly used to optimize neural networks.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mi">1</span> <span class="k">def</span> <span class="nf">train_part34</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
 <span class="mi">2</span>    <span class="sh">"""</span><span class="s">
 3    Train a model on CIFAR-10 using the PyTorch Module API.
 4    
 5    Inputs:
 6    - model: A PyTorch Module giving the model to train.
 7    - optimizer: An Optimizer object we will use to train the model
 8    - epochs: (Optional) A Python integer giving the number of epochs to train for
 9    
10     Returns: Nothing, but prints model accuracies during training.
11     </span><span class="sh">"""</span>
<span class="mi">12</span>     <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># move the model parameters to CPU/GPU
</span><span class="mi">13</span>     <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
<span class="mi">14</span>         <span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">loader_train</span><span class="p">):</span>
<span class="mi">15</span>             <span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>  <span class="c1"># put model to training mode
</span><span class="mi">16</span>             <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>  <span class="c1"># move to device, e.g. GPU
</span><span class="mi">17</span>             <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">)</span>
<span class="mi">18</span> 
<span class="mi">19</span>             <span class="n">scores</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="mi">20</span>             <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">cross_entropy</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="mi">21</span> 
<span class="mi">22</span>             <span class="c1"># Zero out all of the gradients for the variables which the optimizer
</span><span class="mi">23</span>             <span class="c1"># will update.
</span><span class="mi">24</span>             <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
<span class="mi">25</span> 
<span class="mi">26</span>             <span class="c1"># This is the backwards pass: compute the gradient of the loss with
</span><span class="mi">27</span>             <span class="c1"># respect to each  parameter of the model.
</span><span class="mi">28</span>             <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
<span class="mi">29</span> 
<span class="mi">30</span>             <span class="c1"># Actually update the parameters of the model using the gradients
</span><span class="mi">31</span>             <span class="c1"># computed by the backwards pass.
</span><span class="mi">32</span>             <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
<span class="mi">33</span> 
<span class="mi">34</span>             <span class="k">if</span> <span class="n">t</span> <span class="o">%</span> <span class="n">print_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span class="mi">35</span>                 <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Iteration %d, loss = %.4f</span><span class="sh">'</span> <span class="o">%</span> <span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()))</span>
<span class="mi">36</span>                 <span class="nf">check_accuracy_part34</span><span class="p">(</span><span class="n">loader_val</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
<span class="mi">37</span>                 <span class="nf">print</span><span class="p">()</span>
</code></pre></div></div> <p>Sum up the Module API: 1.initialize learning rate and chennel_1,passing then through model and initilize weights and declare the architecture 2. passing values to optim.SGD, 3. training them</p> <h3 id="pytorch-sequential-api"><strong>Pytorch Sequential API</strong></h3> <p>Part III introduced the PyTorch Module API, which allows you to define arbitrary learnable layers and their connectivity.</p> <p>For simple models like a stack of feed forward layers, you still need to go through 3 steps: subclass <code class="language-plaintext highlighter-rouge">nn.Module</code>, assign layers to class attributes in <code class="language-plaintext highlighter-rouge">__init__</code>, and call each layer one by one in <code class="language-plaintext highlighter-rouge">forward()</code>. Is there a more convenient way?</p> <p>Fortunately, PyTorch provides a container Module called <code class="language-plaintext highlighter-rouge">nn.Sequential</code>, which merges the above steps into one. It is not as flexible as <code class="language-plaintext highlighter-rouge">nn.Module</code>, because you cannot specify more complex topology than a feed-forward stack, but it’s good enough for many use cases.</p> <p>Three Layers: Using Sequential API</p> <ol> <li>Convolutional layer (with bias) with 32 5x5 filters, with zero-padding of 2</li> <li>ReLU</li> <li>Convolutional layer (with bias) with 16 3x3 filters, with zero-padding of 1</li> <li>ReLU</li> <li>Fully-connected layer (with bias) to compute scores for 10 classes</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mi">1</span> <span class="n">channel_1</span> <span class="o">=</span> <span class="mi">32</span>
 <span class="mi">2</span> <span class="n">channel_2</span> <span class="o">=</span> <span class="mi">16</span>
 <span class="mi">3</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-2</span>
 <span class="mi">4</span> 
 <span class="mi">5</span> <span class="n">model</span> <span class="o">=</span> <span class="bp">None</span>
 <span class="mi">6</span> <span class="n">optimizer</span> <span class="o">=</span> <span class="bp">None</span>
 <span class="mi">7</span> 
 <span class="mi">8</span> <span class="c1">################################################################################
</span> <span class="mi">9</span> <span class="c1"># TODO: Rewrite the 2-layer ConvNet with bias from Part III with the           #
</span><span class="mi">10</span> <span class="c1"># Sequential API.                                                              #
</span><span class="mi">11</span> <span class="c1">################################################################################
</span><span class="mi">12</span> <span class="c1">#pass
</span><span class="mi">13</span> <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
<span class="mi">14</span>     <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="n">channel_1</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="mi">15</span>     <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
<span class="mi">16</span>     <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">channel_1</span><span class="p">,</span><span class="n">channel_2</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
<span class="mi">17</span>     <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
<span class="mi">18</span>     <span class="nc">Flatten</span><span class="p">(),</span>
<span class="mi">19</span>     <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">channel_2</span><span class="o">*</span><span class="mi">32</span><span class="o">*</span><span class="mi">32</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="mi">20</span> <span class="p">)</span>
<span class="mi">21</span> 
<span class="mi">22</span> <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">SGD</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span><span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
<span class="mi">23</span>                      <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span><span class="n">nesterov</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="mi">24</span> <span class="c1">################################################################################
</span><span class="mi">25</span> <span class="c1">#                                 END OF YOUR CODE 
</span><span class="mi">26</span> <span class="c1">################################################################################
</span><span class="mi">27</span> 
<span class="mi">28</span> <span class="nf">train_part34</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
</code></pre></div></div> <p>Using training_part34 and Sequential API, it’s super easy to set to the layers and transported the data to be trained: Finally the accuracy result will be:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> 1 Iteration 0, loss = 2.2939
 2 Checking accuracy on validation set
 3 Got 140 / 1000 correct (14.00)
 4 
 5 Iteration 100, loss = 1.4576
 6 Checking accuracy on validation set
 7 Got 471 / 1000 correct (47.10)
 8 
 9 Iteration 200, loss = 1.3825
10 Checking accuracy on validation set
11 Got 466 / 1000 correct (46.60)
12 
13 Iteration 300, loss = 1.5948
14 Checking accuracy on validation set
15 Got 524 / 1000 correct (52.40)
16 
17 Iteration 400, loss = 1.2816
18 Checking accuracy on validation set
19 Got 513 / 1000 correct (51.30)
20 
21 Iteration 500, loss = 1.3663
22 Checking accuracy on validation set
23 Got 530 / 1000 correct (53.00)
24 
25 Iteration 600, loss = 1.1300
26 Checking accuracy on validation set
27 Got 545 / 1000 correct (54.50)
28 
29 Iteration 700, loss = 1.2276
30 Checking accuracy on validation set
31 Got 542 / 1000 correct (54.20)

* * *
</code></pre></div></div> <h3 id="tensorflow"><strong>Tensorflow</strong></h3> <p>In this Tensorflow introduction, we gonna do the same structure as we do in the introduction of Pytorch</p> <p><img src="https://zhengliangliang.files.wordpress.com/2018/07/2018-07-31_135946.jpg" alt="2018-07-31_135946.jpg"></p> <p>All of the packages we imported:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mi">1</span> <span class="kn">import</span> <span class="n">os</span>
 <span class="mi">2</span> <span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
 <span class="mi">3</span> <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
 <span class="mi">4</span> <span class="kn">import</span> <span class="n">math</span>
 <span class="mi">5</span> <span class="kn">import</span> <span class="n">timeit</span>
 <span class="mi">6</span> <span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
 <span class="mi">7</span> 
 <span class="mi">8</span> <span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</code></pre></div></div> <p><strong>Barebone Tensorflow:</strong></p> <p>We can see this in action by defining a simple <code class="language-plaintext highlighter-rouge">flatten</code> function that will reshape image data for use in a fully-connected network.</p> <p>In TensorFlow, data for convolutional feature maps is typically stored in a Tensor of shape N x H x W x C where:</p> <ul> <li>N is the number of datapoints (minibatch size)</li> <li>H is the height of the feature map</li> <li>W is the width of the feature map</li> <li>C is the number of channels in the feature map</li> </ul> <p>Notice that this is a little different from pytorch.</p> <p><strong>Three_layer_convnet</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mi">1</span> <span class="k">def</span> <span class="nf">three_layer_convnet</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
 <span class="mi">2</span>    <span class="sh">"""</span><span class="s">
 3    A three-layer convolutional network with the architecture described above.
 4    
 5    Inputs:
 6    - x: A TensorFlow Tensor of shape (N, H, W, 3) giving a minibatch of images
 7    - params: A list of TensorFlow Tensors giving the weights and biases for the
 8      network; should contain the following:
 9      - conv_w1: TensorFlow Tensor of shape (KH1, KW1, 3, channel_1) giving
10         weights for the first convolutional layer.
11       - conv_b1: TensorFlow Tensor of shape (channel_1,) giving biases for the
12         first convolutional layer.
13       - conv_w2: TensorFlow Tensor of shape (KH2, KW2, channel_1, channel_2)
14         giving weights for the second convolutional layer
15       - conv_b2: TensorFlow Tensor of shape (channel_2,) giving biases for the
16         second convolutional layer.
17       - fc_w: TensorFlow Tensor giving weights for the fully-connected layer.
18         Can you figure out what the shape should be?
19       - fc_b: TensorFlow Tensor giving biases for the fully-connected layer.
20         Can you figure out what the shape should be?
21     </span><span class="sh">"""</span>
<span class="mi">22</span>     <span class="n">conv_w1</span><span class="p">,</span> <span class="n">conv_b1</span><span class="p">,</span> <span class="n">conv_w2</span><span class="p">,</span> <span class="n">conv_b2</span><span class="p">,</span> <span class="n">fc_w</span><span class="p">,</span> <span class="n">fc_b</span> <span class="o">=</span> <span class="n">params</span>
<span class="mi">23</span>     <span class="n">scores</span> <span class="o">=</span> <span class="bp">None</span>
<span class="mi">24</span>     <span class="c1">############################################################################
</span><span class="mi">25</span>     <span class="c1"># TODO: Implement the forward pass for the three-layer ConvNet.            #
</span><span class="mi">26</span>     <span class="c1">############################################################################
</span><span class="mi">27</span>     <span class="n">x_padded</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">pad</span><span class="p">(</span><span class="n">x</span><span class="p">,[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">],[</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]],</span><span class="sh">'</span><span class="s">CONSTANT</span><span class="sh">'</span><span class="p">)</span>
<span class="mi">28</span>     <span class="n">conv1</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">conv2d</span><span class="p">(</span><span class="n">x_padded</span><span class="p">,</span><span class="n">conv_w1</span><span class="p">,[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">VALID</span><span class="sh">'</span><span class="p">)</span><span class="o">+</span><span class="n">conv_b1</span>
<span class="mi">29</span>     <span class="n">relu1</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">conv1</span><span class="p">)</span>
<span class="mi">30</span>     <span class="n">x_padded_1</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">pad</span><span class="p">(</span><span class="n">relu1</span><span class="p">,[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]],</span><span class="sh">'</span><span class="s">CONSTANT</span><span class="sh">'</span><span class="p">)</span>
<span class="mi">31</span>     <span class="n">conv2</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">conv2d</span><span class="p">(</span><span class="n">x_padded_1</span><span class="p">,</span><span class="n">conv_w2</span><span class="p">,[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">VALID</span><span class="sh">'</span><span class="p">)</span><span class="o">+</span><span class="n">conv_b2</span>
<span class="mi">32</span>     <span class="n">relu2</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">conv2</span><span class="p">)</span>
<span class="mi">33</span>     <span class="n">fc_x</span> <span class="o">=</span> <span class="nf">flatten</span><span class="p">(</span><span class="n">relu2</span><span class="p">)</span>
<span class="mi">34</span>     <span class="n">h</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">fc_x</span><span class="p">,</span> <span class="n">fc_w</span><span class="p">)</span> <span class="o">+</span> <span class="n">fc_b</span>
<span class="mi">35</span>     <span class="n">scores</span> <span class="o">=</span> <span class="n">h</span>
<span class="mi">36</span>     <span class="c1">#pass
</span><span class="mi">37</span>     <span class="c1">############################################################################
</span><span class="mi">38</span>     <span class="c1">#                              END OF YOUR CODE                            #
</span><span class="mi">39</span>     <span class="c1">############################################################################
</span><span class="mi">40</span>     <span class="k">return</span> <span class="n">scores</span>
</code></pre></div></div> <p>All of the functions are from tf.nn . From the above code you may find it very similar to pytorch, but we need to declear the padded form in tf.pad then pass them in tf.nn.conv2d function, and the stride parameter would be like [1,1,1,1]</p> <p>Training step:</p> <ol> <li>Compute the loss</li> <li>Compute the gradient of the loss with respect to all network weights</li> <li>Make a weight update step using (stochastic) gradient descent.</li> </ol> <p>Note that the step of updating the weights is itself an operation in the computational graph - the calls to <code class="language-plaintext highlighter-rouge">tf.assign_sub</code> in <code class="language-plaintext highlighter-rouge">training_step</code> return TensorFlow operations that mutate the weights when they are executed. There is an important bit of subtlety here - when we call <code class="language-plaintext highlighter-rouge">sess.run</code>, TensorFlow does not execute all operations in the computational graph; it only executes the minimal subset of the graph necessary to compute the outputs that we ask TensorFlow to produce. As a result, naively computing the loss would not cause the weight update operations to execute, <strong>since the operations needed to compute the loss do not depend on the output of the weight update</strong>. To fix this problem, we insert a <strong>control dependency</strong> into the graph, adding a duplicate <code class="language-plaintext highlighter-rouge">loss</code> node to the graph that does depend on the outputs of the weight update operations; this is the object that we actually return from the <code class="language-plaintext highlighter-rouge">training_step</code> function. As a result, asking TensorFlow to evaluate the value of the <code class="language-plaintext highlighter-rouge">loss</code>returned from <code class="language-plaintext highlighter-rouge">training_step</code> will also implicitly update the weights of the network using that minibatch of data.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mi">1</span> <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
 <span class="mi">2</span>    <span class="sh">"""</span><span class="s">
 3    Set up the part of the computational graph which makes a training step.
 4 
 5    Inputs:
 6    - scores: TensorFlow Tensor of shape (N, C) giving classification scores for
 7      the model.
 8    - y: TensorFlow Tensor of shape (N,) giving ground-truth labels for scores;
 9      y[i] == c means that c is the correct class for scores[i].
10     - params: List of TensorFlow Tensors giving the weights of the model
11     - learning_rate: Python scalar giving the learning rate to use for gradient
12       descent step.
13       
14     Returns:
15     - loss: A TensorFlow Tensor of shape () (scalar) giving the loss for this
16       batch of data; evaluating the loss also performs a gradient descent step
17       on params (see above).
18     </span><span class="sh">"""</span>
<span class="mi">19</span>     <span class="c1"># First compute the loss; the first line gives losses for each example in
</span><span class="mi">20</span>     <span class="c1"># the minibatch, and the second averages the losses acros the batch
</span><span class="mi">21</span>     <span class="n">losses</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">scores</span><span class="p">)</span>
<span class="mi">22</span>     <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_mean</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
<span class="mi">23</span> 
<span class="mi">24</span>     <span class="c1"># Compute the gradient of the loss with respect to each parameter of the the
</span><span class="mi">25</span>     <span class="c1"># network. This is a very magical function call: TensorFlow internally
</span><span class="mi">26</span>     <span class="c1"># traverses the computational graph starting at loss backward to each element
</span><span class="mi">27</span>     <span class="c1"># of params, and uses backpropagation to figure out how to compute gradients;
</span><span class="mi">28</span>     <span class="c1"># it then adds new operations to the computational graph which compute the
</span><span class="mi">29</span>     <span class="c1"># requested gradients, and returns a list of TensorFlow Tensors that will
</span><span class="mi">30</span>     <span class="c1"># contain the requested gradients when evaluated.
</span><span class="mi">31</span>     <span class="n">grad_params</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">gradients</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
<span class="mi">32</span>     
<span class="mi">33</span>     <span class="c1"># Make a gradient descent step on all of the model parameters.
</span><span class="mi">34</span>     <span class="n">new_weights</span> <span class="o">=</span> <span class="p">[]</span>   
<span class="mi">35</span>     <span class="k">for</span> <span class="n">w</span><span class="p">,</span> <span class="n">grad_w</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">grad_params</span><span class="p">):</span>
<span class="mi">36</span>         <span class="n">new_w</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">assign_sub</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_w</span><span class="p">)</span>
<span class="mi">37</span>         <span class="n">new_weights</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">new_w</span><span class="p">)</span>
<span class="mi">38</span> 
<span class="mi">39</span>     <span class="c1"># Insert a control dependency so that evaluting the loss causes a weight
</span><span class="mi">40</span>     <span class="c1"># update to happen; see the discussion above.
</span><span class="mi">41</span>     <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nf">control_dependencies</span><span class="p">(</span><span class="n">new_weights</span><span class="p">):</span>
<span class="mi">42</span>         <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="nf">identity</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</code></pre></div></div> <p>you need to be familiar with the function tf.nn.sparse_softmax_cross_entropy_with_logits <strong>Tensorflow : Trainning Loop</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mi">1</span> <span class="k">def</span> <span class="nf">train_part2</span><span class="p">(</span><span class="n">model_fn</span><span class="p">,</span> <span class="n">init_fn</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
 <span class="mi">2</span>    <span class="sh">"""</span><span class="s">
 3    Train a model on CIFAR-10.
 4    
 5    Inputs:
 6    - model_fn: A Python function that performs the forward pass of the model
 7      using TensorFlow; it should have the following signature:
 8      scores = model_fn(x, params) where x is a TensorFlow Tensor giving a
 9      minibatch of image data, params is a list of TensorFlow Tensors holding
10       the model weights, and scores is a TensorFlow Tensor of shape (N, C)
11       giving scores for all elements of x.
12     - init_fn: A Python function that initializes the parameters of the model.
13       It should have the signature params = init_fn() where params is a list
14       of TensorFlow Tensors holding the (randomly initialized) weights of the
15       model.
16     - learning_rate: Python float giving the learning rate to use for SGD.
17     </span><span class="sh">"""</span>
<span class="mi">18</span>     <span class="c1"># First clear the default graph
</span><span class="mi">19</span>     <span class="n">tf</span><span class="p">.</span><span class="nf">reset_default_graph</span><span class="p">()</span>
<span class="mi">20</span>     <span class="n">is_training</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nb">bool</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">is_training</span><span class="sh">'</span><span class="p">)</span>
<span class="mi">21</span>     <span class="c1"># Set up the computational graph for performing forward and backward passes,
</span><span class="mi">22</span>     <span class="c1"># and weight updates.
</span><span class="mi">23</span>     <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="n">device</span><span class="p">):</span>
<span class="mi">24</span>         <span class="c1"># Set up placeholders for the data and labels
</span><span class="mi">25</span>         <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="mi">26</span>         <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">])</span>
<span class="mi">27</span>         <span class="n">params</span> <span class="o">=</span> <span class="nf">init_fn</span><span class="p">()</span>           <span class="c1"># Initialize the model parameters
</span><span class="mi">28</span>         <span class="n">scores</span> <span class="o">=</span> <span class="nf">model_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span> <span class="c1"># Forward pass of the model
</span><span class="mi">29</span>         <span class="n">loss</span> <span class="o">=</span> <span class="nf">training_step</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>
<span class="mi">30</span> 
<span class="mi">31</span>     <span class="c1"># Now we actually run the graph many times using the training data
</span><span class="mi">32</span>     <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nc">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
<span class="mi">33</span>         <span class="c1"># Initialize variables that will live in the graph
</span><span class="mi">34</span>         <span class="n">sess</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">global_variables_initializer</span><span class="p">())</span>
<span class="mi">35</span>         <span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="p">(</span><span class="n">x_np</span><span class="p">,</span> <span class="n">y_np</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">train_dset</span><span class="p">):</span>
<span class="mi">36</span>             <span class="c1"># Run the graph on a batch of training data; recall that asking
</span><span class="mi">37</span>             <span class="c1"># TensorFlow to evaluate loss will cause an SGD step to happen.
</span><span class="mi">38</span>             <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">x_np</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_np</span><span class="p">}</span>
<span class="mi">39</span>             <span class="n">loss_np</span> <span class="o">=</span> <span class="n">sess</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="n">feed_dict</span><span class="p">)</span>
<span class="mi">40</span>             
<span class="mi">41</span>             <span class="c1"># Periodically print the loss and check accuracy on the val set
</span><span class="mi">42</span>             <span class="k">if</span> <span class="n">t</span> <span class="o">%</span> <span class="n">print_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span class="mi">43</span>                 <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Iteration %d, loss = %.4f</span><span class="sh">'</span> <span class="o">%</span> <span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">loss_np</span><span class="p">))</span>
<span class="mi">44</span>                 <span class="nf">check_accuracy</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">val_dset</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">is_training</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Barebones TensorFlow: Check Accuracy</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mi">1</span> <span class="k">def</span> <span class="nf">check_accuracy</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">dset</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">is_training</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
 <span class="mi">2</span>    <span class="sh">"""</span><span class="s">
 3    Check accuracy on a classification model.
 4    
 5    Inputs:
 6    - sess: A TensorFlow Session that will be used to run the graph
 7    - dset: A Dataset object on which to check accuracy
 8    - x: A TensorFlow placeholder Tensor where input images should be fed
 9    - scores: A TensorFlow Tensor representing the scores output from the
10       model; this is the Tensor we will ask TensorFlow to evaluate.
11       
12     Returns: Nothing, but prints the accuracy of the model
13     </span><span class="sh">"""</span>
<span class="mi">14</span>     <span class="n">num_correct</span><span class="p">,</span> <span class="n">num_samples</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
<span class="mi">15</span>     <span class="k">for</span> <span class="n">x_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="ow">in</span> <span class="n">dset</span><span class="p">:</span>
<span class="mi">16</span>         <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">x_batch</span><span class="p">,</span> <span class="n">is_training</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>
<span class="mi">17</span>         <span class="n">scores_np</span> <span class="o">=</span> <span class="n">sess</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="n">feed_dict</span><span class="p">)</span>
<span class="mi">18</span>         <span class="n">y_pred</span> <span class="o">=</span> <span class="n">scores_np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="mi">19</span>         <span class="n">num_samples</span> <span class="o">+=</span> <span class="n">x_batch</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="mi">20</span>         <span class="n">num_correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">==</span> <span class="n">y_batch</span><span class="p">).</span><span class="nf">sum</span><span class="p">()</span>
<span class="mi">21</span>     <span class="n">acc</span> <span class="o">=</span> <span class="nf">float</span><span class="p">(</span><span class="n">num_correct</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_samples</span>
<span class="mi">22</span>     <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Got %d / %d correct (%.2f%%)</span><span class="sh">'</span> <span class="o">%</span> <span class="p">(</span><span class="n">num_correct</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">,</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">acc</span><span class="p">))</span>
</code></pre></div></div> <p>I will omit the initilization part because it’s similar to the pytorch part. To sum up, the process of training the passing values are the same as they do in the pytorch, but in the tensorflow we need to use placeholder and sess.run to make it work,and tbh, tensorflow it’s a little bit difficult to get started at the very begining comparing to pytorch.</p> <h3 id="keras-model-api">Keras Model API</h3> <p>Implementing a neural network using the low-level TensorFlow API is a good way to understand how TensorFlow works, but it’s a little inconvenient - we had to manually keep track of all Tensors holding learnable parameters, and we had to use a control dependency to implement the gradient descent update step. This was fine for a small network, but could quickly become unweildy for a large complex model.</p> <p>Fortunately TensorFlow provides higher-level packages such as <code class="language-plaintext highlighter-rouge">tf.keras</code> and <code class="language-plaintext highlighter-rouge">tf.layers</code> which make it easy to build models out of modular, object-oriented layers; <code class="language-plaintext highlighter-rouge">tf.train</code> allows you to easily train these models using a variety of different optimization algorithms.</p> <h3 id="keras-model-api-three-layer-convnet">Keras Model API: Three-Layer ConvNet</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="mf">1.</span> <span class="n">Convolutional</span> <span class="n">layer</span> <span class="k">with</span> <span class="mi">5</span> <span class="n">x</span> <span class="mi">5</span> <span class="n">kernels</span><span class="p">,</span> <span class="k">with</span> <span class="n">zero</span><span class="o">-</span><span class="n">padding</span> <span class="n">of</span> <span class="mi">2</span>
<span class="mf">2.</span> <span class="n">ReLU</span> <span class="n">nonlinearity</span>
<span class="mf">3.</span> <span class="n">Convolutional</span> <span class="n">layer</span> <span class="k">with</span> <span class="mi">3</span> <span class="n">x</span> <span class="mi">3</span> <span class="n">kernels</span><span class="p">,</span> <span class="k">with</span> <span class="n">zero</span><span class="o">-</span><span class="n">padding</span> <span class="n">of</span> <span class="mi">1</span>
<span class="mf">4.</span> <span class="n">ReLU</span> <span class="n">nonlinearity</span>
<span class="mf">5.</span> <span class="n">Fully</span><span class="o">-</span><span class="n">connected</span> <span class="n">layer</span> <span class="n">to</span> <span class="n">give</span> <span class="k">class</span> <span class="nc">scores</span>

 <span class="mi">1</span> <span class="k">class</span> <span class="nc">ThreeLayerConvNet</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Model</span><span class="p">):</span>
 <span class="mi">2</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">channel_1</span><span class="p">,</span> <span class="n">channel_2</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">):</span>
 <span class="mi">3</span>        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
 <span class="mi">4</span>        <span class="c1">########################################################################
</span> <span class="mi">5</span>        <span class="c1"># TODO: Implement the __init__ method for a three-layer ConvNet. You   #
</span> <span class="mi">6</span>        <span class="c1"># should instantiate layer objects to be used in the forward pass.     #
</span> <span class="mi">7</span>        <span class="c1">########################################################################
</span> <span class="mi">8</span>        <span class="n">initializer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">variance_scaling_initializer</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span>
 <span class="mi">9</span>        <span class="n">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="n">channel_1</span><span class="p">,[</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span><span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
<span class="mi">10</span>                                 <span class="n">padding</span><span class="o">=</span><span class="sh">"</span><span class="s">valid</span><span class="sh">"</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">relu</span><span class="p">,</span>
<span class="mi">11</span>                                 <span class="n">kernel_initializer</span> <span class="o">=</span> <span class="n">initializer</span><span class="p">)</span>
<span class="mi">12</span>         <span class="n">self</span><span class="p">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="n">channel_2</span><span class="p">,[</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span><span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
<span class="mi">13</span>                                 <span class="n">padding</span><span class="o">=</span><span class="sh">"</span><span class="s">valid</span><span class="sh">"</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">relu</span><span class="p">,</span>
<span class="mi">14</span>                                 <span class="n">kernel_initializer</span> <span class="o">=</span> <span class="n">initializer</span><span class="p">)</span>
<span class="mi">15</span>         <span class="n">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span><span class="n">kernel_initializer</span><span class="o">=</span><span class="n">initializer</span><span class="p">)</span>
<span class="mi">16</span>         <span class="c1">#pass
</span><span class="mi">17</span>         <span class="c1">########################################################################
</span><span class="mi">18</span>         <span class="c1">#                           END OF YOUR CODE                           #
</span><span class="mi">19</span>         <span class="c1">########################################################################
</span></code></pre></div></div> <p>Training Loop:</p> <p>We need to implement a slightly different training loop when using the <code class="language-plaintext highlighter-rouge">tf.keras.Model</code> API. Instead of computing gradients and updating the weights of the model manually, we use an <code class="language-plaintext highlighter-rouge">Optimizer</code> object from the <code class="language-plaintext highlighter-rouge">tf.train</code> package which takes care of these details for us. You can read more about</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mi">1</span> <span class="k">def</span> <span class="nf">train_part34</span><span class="p">(</span><span class="n">model_init_fn</span><span class="p">,</span> <span class="n">optimizer_init_fn</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
 <span class="mi">2</span>    <span class="sh">"""</span><span class="s">
 3    Simple training loop for use with models defined using tf.keras. It trains
 4    a model for one epoch on the CIFAR-10 training set and periodically checks
 5    accuracy on the CIFAR-10 validation set.
 6    
 7    Inputs:
 8    - model_init_fn: A function that takes no parameters; when called it
 9      constructs the model we want to train: model = model_init_fn()
10     - optimizer_init_fn: A function which takes no parameters; when called it
11       constructs the Optimizer object we will use to optimize the model:
12       optimizer = optimizer_init_fn()
13     - num_epochs: The number of epochs to train for
14     
15     Returns: Nothing, but prints progress during trainingn
16     </span><span class="sh">"""</span>
<span class="mi">17</span>     <span class="n">tf</span><span class="p">.</span><span class="nf">reset_default_graph</span><span class="p">()</span>    
<span class="mi">18</span>     <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="n">device</span><span class="p">):</span>
<span class="mi">19</span>         <span class="c1"># Construct the computational graph we will use to train the model. We
</span><span class="mi">20</span>         <span class="c1"># use the model_init_fn to construct the model, declare placeholders for
</span><span class="mi">21</span>         <span class="c1"># the data and labels
</span><span class="mi">22</span>         <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="mi">23</span>         <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">])</span>
<span class="mi">24</span>         
<span class="mi">25</span>         <span class="c1"># We need a place holder to explicitly specify if the model is in the training
</span><span class="mi">26</span>         <span class="c1"># phase or not. This is because a number of layers behaves differently in
</span><span class="mi">27</span>         <span class="c1"># training and in testing, e.g., dropout and batch normalization.
</span><span class="mi">28</span>         <span class="c1"># We pass this variable to the computation graph through feed_dict as shown below.
</span><span class="mi">29</span>         <span class="n">is_training</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nb">bool</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">is_training</span><span class="sh">'</span><span class="p">)</span>
<span class="mi">30</span>         
<span class="mi">31</span>         <span class="c1"># Use the model function to build the forward pass.
</span><span class="mi">32</span>         <span class="n">scores</span> <span class="o">=</span> <span class="nf">model_init_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">is_training</span><span class="p">)</span>
<span class="mi">33</span> 
<span class="mi">34</span>         <span class="c1"># Compute the loss like we did in Part II
</span><span class="mi">35</span>         <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">scores</span><span class="p">)</span>
<span class="mi">36</span>         <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_mean</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
<span class="mi">37</span> 
<span class="mi">38</span>         <span class="c1"># Use the optimizer_fn to construct an Optimizer, then use the optimizer
</span><span class="mi">39</span>         <span class="c1"># to set up the training step. Asking TensorFlow to evaluate the
</span><span class="mi">40</span>         <span class="c1"># train_op returned by optimizer.minimize(loss) will cause us to make a
</span><span class="mi">41</span>         <span class="c1"># single update step using the current minibatch of data.
</span><span class="mi">42</span>         
<span class="mi">43</span>         <span class="c1"># Note that we use tf.control_dependencies to force the model to run
</span><span class="mi">44</span>         <span class="c1"># the tf.GraphKeys.UPDATE_OPS at each training step. tf.GraphKeys.UPDATE_OPS
</span><span class="mi">45</span>         <span class="c1"># holds the operators that update the states of the network.
</span><span class="mi">46</span>         <span class="c1"># For example, the tf.layers.batch_normalization function adds the running mean
</span><span class="mi">47</span>         <span class="c1"># and variance update operators to tf.GraphKeys.UPDATE_OPS.
</span><span class="mi">48</span>         <span class="n">optimizer</span> <span class="o">=</span> <span class="nf">optimizer_init_fn</span><span class="p">()</span>
<span class="mi">49</span>         <span class="n">update_ops</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">GraphKeys</span><span class="p">.</span><span class="n">UPDATE_OPS</span><span class="p">)</span>
<span class="mi">50</span>         <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nf">control_dependencies</span><span class="p">(</span><span class="n">update_ops</span><span class="p">):</span>
<span class="mi">51</span>             <span class="n">train_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">.</span><span class="nf">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
<span class="mi">52</span> 
<span class="mi">53</span>     <span class="c1"># Now we can run the computational graph many times to train the model.
</span><span class="mi">54</span>     <span class="c1"># When we call sess.run we ask it to evaluate train_op, which causes the
</span><span class="mi">55</span>     <span class="c1"># model to update.
</span><span class="mi">56</span>     <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nc">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
<span class="mi">57</span>         <span class="n">sess</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">global_variables_initializer</span><span class="p">())</span>
<span class="mi">58</span>         <span class="n">t</span> <span class="o">=</span> <span class="mi">0</span>
<span class="mi">59</span>         <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
<span class="mi">60</span>             <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Starting epoch %d</span><span class="sh">'</span> <span class="o">%</span> <span class="n">epoch</span><span class="p">)</span>
<span class="mi">61</span>             <span class="k">for</span> <span class="n">x_np</span><span class="p">,</span> <span class="n">y_np</span> <span class="ow">in</span> <span class="n">train_dset</span><span class="p">:</span>
<span class="mi">62</span>                 <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">x_np</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_np</span><span class="p">,</span> <span class="n">is_training</span><span class="p">:</span><span class="mi">1</span><span class="p">}</span>
<span class="mi">63</span>                 <span class="n">loss_np</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sess</span><span class="p">.</span><span class="nf">run</span><span class="p">([</span><span class="n">loss</span><span class="p">,</span> <span class="n">train_op</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="n">feed_dict</span><span class="p">)</span>
<span class="mi">64</span>                 <span class="k">if</span> <span class="n">t</span> <span class="o">%</span> <span class="n">print_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span class="mi">65</span>                     <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Iteration %d, loss = %.4f</span><span class="sh">'</span> <span class="o">%</span> <span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">loss_np</span><span class="p">))</span>
<span class="mi">66</span>                     <span class="nf">check_accuracy</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">val_dset</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">is_training</span><span class="o">=</span><span class="n">is_training</span><span class="p">)</span>
<span class="mi">67</span>                     <span class="nf">print</span><span class="p">()</span>
<span class="mi">68</span>                 <span class="n">t</span> <span class="o">+=</span> <span class="mi">1</span>
</code></pre></div></div> <p>Finally :</p> <h3 id="keras-sequential-api">Keras Sequential API</h3> <p>Here you should use <code class="language-plaintext highlighter-rouge">tf.keras.Sequential</code> to reimplement the same three-layer ConvNet architecture used in Part II and Part III. As a reminder, your model should have the following architecture:</p> <ol> <li>Convolutional layer with 16 5x5 kernels, using zero padding of 2</li> <li>ReLU nonlinearity</li> <li>Convolutional layer with 32 3x3 kernels, using zero padding of 1</li> <li>ReLU nonlinearity</li> <li>Fully-connected layer giving class scores</li> </ol> <p>You should initialize the weights of the model using a <code class="language-plaintext highlighter-rouge">tf.variance_scaling_initializer</code> as above.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mi">1</span> <span class="k">def</span> <span class="nf">model_init_fn</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">is_training</span><span class="p">):</span>
 <span class="mi">2</span>    <span class="n">model</span> <span class="o">=</span> <span class="bp">None</span>
 <span class="mi">3</span>    <span class="c1">############################################################################
</span> <span class="mi">4</span>    <span class="c1"># TODO: Construct a three-layer ConvNet using tf.keras.Sequential.         #
</span> <span class="mi">5</span>    <span class="c1">############################################################################
</span> <span class="mi">6</span>    <span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
 <span class="mi">7</span>    <span class="n">channel_1</span><span class="p">,</span> <span class="n">channel_2</span><span class="p">,</span> <span class="n">num_classes</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span>
 <span class="mi">8</span>    <span class="n">initializer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">variance_scaling_initializer</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span>
 <span class="mi">9</span>    <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span>
<span class="mi">10</span>         <span class="c1"># 'Same' padding acts similar to zero padding of 2 for this input
</span><span class="mi">11</span>         <span class="n">tf</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="n">channel_1</span><span class="p">,[</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span><span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
<span class="mi">12</span>                                 <span class="n">padding</span><span class="o">=</span><span class="sh">"</span><span class="s">same</span><span class="sh">"</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">relu</span><span class="p">,</span>
<span class="mi">13</span>                                 <span class="n">kernel_initializer</span> <span class="o">=</span> <span class="n">initializer</span><span class="p">,</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span><span class="mi">3</span><span class="p">)),</span>
<span class="mi">14</span>         <span class="n">tf</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="n">channel_2</span><span class="p">,[</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span><span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
<span class="mi">15</span>                                 <span class="n">padding</span><span class="o">=</span><span class="sh">"</span><span class="s">same</span><span class="sh">"</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">relu</span><span class="p">,</span>
<span class="mi">16</span>                                 <span class="n">kernel_initializer</span> <span class="o">=</span> <span class="n">initializer</span><span class="p">),</span>
<span class="mi">17</span>         <span class="n">tf</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Flatten</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">),</span>
<span class="mi">18</span>         <span class="n">tf</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">initializer</span><span class="p">),</span>
<span class="mi">19</span>     <span class="p">]</span>
<span class="mi">20</span>     <span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>
<span class="mi">21</span>     <span class="c1">#pass
</span><span class="mi">22</span>     <span class="c1">############################################################################
</span><span class="mi">23</span>     <span class="c1">#                            END OF YOUR CODE                              #
</span><span class="mi">24</span>     <span class="c1">############################################################################
</span><span class="mi">25</span>     <span class="k">return</span> <span class="nf">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="mi">26</span> 
<span class="mi">27</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">5e-4</span>
<span class="mi">28</span> <span class="k">def</span> <span class="nf">optimizer_init_fn</span><span class="p">():</span>
<span class="mi">29</span>     <span class="n">optimizer</span> <span class="o">=</span> <span class="bp">None</span>
<span class="mi">30</span>     <span class="c1">############################################################################
</span><span class="mi">31</span>     <span class="c1"># TODO: Complete the implementation of model_fn.                           #
</span><span class="mi">32</span>     <span class="c1">############################################################################
</span><span class="mi">33</span>     <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="nc">MomentumOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="mi">34</span> 
<span class="mi">35</span>     <span class="c1">############################################################################
</span><span class="mi">36</span>     <span class="c1">#                           END OF YOUR CODE                               #
</span><span class="mi">37</span>     <span class="c1">############################################################################
</span><span class="mi">38</span>     <span class="k">return</span> <span class="n">optimizer</span>
<span class="mi">39</span> 
<span class="mi">40</span> <span class="nf">train_part34</span><span class="p">(</span><span class="n">model_init_fn</span><span class="p">,</span> <span class="n">optimizer_init_fn</span><span class="p">)</span>
</code></pre></div></div> <p> </p> <hr> <p>To be honest, I personally prefer pytorch because it is more succinct and simple in syntax. In contrast, tensorflow is very grammatically complex and needs to be written repeatedly to write such as sess.run and placeholder to run the whole code. <strong>In tensorflow’s Sequential API, dropout and batchnorm are not available,</strong> but those API is very simple and available in pytorch.</p> <p>Objectively speaking, the advantage of tensorflow is that TF has the perfect community and documentation which are supported by GOOGLE, which is a great benefit for industrial developers. So in the future, although tensorflow has some shortcomings, I will still use it anyway.</p> <p>(The following content and introduction are based on the assignment of CS231n)</p> <p>liangliangzheng</p> <p>July,31,2018</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/displaying-external-posts-on-your-al-folio-blog/">Displaying External Posts on Your al-folio Blog</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/small-guide-to-supplements-what-you-need-to-know/">A small guide to supplements: What you need to know</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/%E6%B7%B7%E4%B9%B1%E4%B8%8E%E7%A7%A9%E5%BA%8F/">混乱与秩序</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/podcast-notes-huberman-lab-dopomine/">Podcast Notes: Huberman Lab Dopomine</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/erc-721/">ERC 721</a> </li> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"zhengstar94/zhengstar94.github.io","data-repo-id":"R_kgDOMKQWcA","data-category":"Comments","data-category-id":"DIC_kwDOMKQWcM4CgKT_","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Xingxing Zheng. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?06cae41083477f121be8cd9797ad8e2f"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-repositories",title:"repositories",description:"",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-cv",title:"CV",description:"Personal CV, updated on 18 Jun, 2024",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"dropdown-by-year",title:"by year",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-by-category",title:"by category",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"post-a-small-guide-to-supplements-what-you-need-to-know",title:"A small guide to supplements: What you need to know",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/small-guide-to-supplements-what-you-need-to-know/"}},{id:"post-\u6df7\u4e71\u4e0e\u79e9\u5e8f",title:"\u6df7\u4e71\u4e0e\u79e9\u5e8f",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/%E6%B7%B7%E4%B9%B1%E4%B8%8E%E7%A7%A9%E5%BA%8F/"}},{id:"post-podcast-notes-huberman-lab-dopomine",title:"Podcast Notes: Huberman Lab Dopomine",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/podcast-notes-huberman-lab-dopomine/"}},{id:"post-displaying-external-posts-on-your-al-folio-blog",title:"Displaying External Posts on Your al-folio Blog",description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2","_blank")}},{id:"post-erc-721",title:"ERC 721",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/erc-721/"}},{id:"post-cost-functions-and-its-properties-in-deep-learning-tbc",title:"Cost Functions and its properties in Deep Learning (TBC)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/cost-functions-and-its-properties-in-deep-learning-tbc/"}},{id:"post-cv-modifying",title:"CV: Modifying",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/cv-modifying/"}},{id:"post-final",title:"Final",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/Final/"}},{id:"post-design-pattern",title:"Design Pattern",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/Design-Pattern/"}},{id:"post-distributed",title:"Distributed",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/Distributed/"}},{id:"post-mq",title:"MQ",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/MQ/"}},{id:"post-spring",title:"Spring",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/Spring/"}},{id:"post-computer",title:"Computer",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/Computer/"}},{id:"post-redis",title:"Redis",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/Redis/"}},{id:"post-jvm",title:"Jvm",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/JVM/"}},{id:"post-thread",title:"Thread",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/Thread/"}},{id:"post-collection",title:"Collection",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/Collection/"}},{id:"post-base",title:"Base",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/Base/"}},{id:"post-mysql",title:"MYSQL",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/MYSQL/"}},{id:"post-interview",title:"Interview",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/Interview/"}},{id:"post-concurrenthashmap",title:"ConcurrentHashMap",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/ConcurrentHashMap/"}},{id:"post-hashmap\u6b7b\u5faa\u73af\u539f\u56e0",title:"HashMap\u6b7b\u5faa\u73af\u539f\u56e0",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/HashMap%E6%AD%BB%E5%BE%AA%E7%8E%AF%E5%8E%9F%E5%9B%A0/"}},{id:"post-hashmap",title:"HashMap",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/HashMap/"}},{id:"post-arraylist\u4e0elinkedlist",title:"ArrayList\u4e0eLinkedList",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/ArrayList%E4%B8%8ELinkedList/"}},{id:"post-java8stream\u6d41\u9012\u5f52\u6811\u904d\u5386",title:"Java8Stream\u6d41\u9012\u5f52\u6811\u904d\u5386",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/Java8Stream%E6%B5%81%E9%80%92%E5%BD%92%E6%A0%91%E9%81%8D%E5%8E%86/"}},{id:"post-\u5e8f\u5217\u5316\u4e0e\u53cd\u5e8f\u5217\u5316",title:"\u5e8f\u5217\u5316\u4e0e\u53cd\u5e8f\u5217\u5316",description:"an example of a blog post with some code",section:"Posts",handler:()=>{window.location.href="/blog/2021/%E5%BA%8F%E5%88%97%E5%8C%96%E4%B8%8E%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96/"}},{id:"post-\u6ce8\u89e3\u4e0e\u53cd\u5c04",title:"\u6ce8\u89e3\u4e0e\u53cd\u5c04",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/%E6%B3%A8%E8%A7%A3%E4%B8%8E%E5%8F%8D%E5%B0%84/"}},{id:"post-\u522b\u867d\u7136\u4f46\u662f\u4e86-2020\u8fc7\u53bb\u4e86",title:"\u522b\u867d\u7136\u4f46\u662f\u4e86\uff0c2020\u8fc7\u53bb\u4e86\uff0e",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2020/%E5%88%AB%E8%99%BD%E7%84%B6%E4%BD%86%E6%98%AF%E4%BA%86-2020%E8%BF%87%E5%8E%BB%E4%BA%86/"}},{id:"post-sql\u89c4\u8303\u4e0e\u6280\u5de7-\u6301\u7eed\u66f4\u65b0",title:"SQL\u89c4\u8303\u4e0e\u6280\u5de7(\u6301\u7eed\u66f4\u65b0)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2020/sql%E8%A7%84%E8%8C%83%E4%B8%8E%E6%8A%80%E5%B7%A7/"}},{id:"post-\u91d1\u878d\u8bfb\u4e66\u7b14\u8bb0-3",title:"\u91d1\u878d\u8bfb\u4e66\u7b14\u8bb0(3)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2020/%E9%87%91%E8%9E%8D%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B03/"}},{id:"post-\u91d1\u878d\u8bfb\u4e66\u7b14\u8bb0-2",title:"\u91d1\u878d\u8bfb\u4e66\u7b14\u8bb0(2)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2020/%E9%87%91%E8%9E%8D%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B02/"}},{id:"post-\u91d1\u878d\u8bfb\u4e66\u7b14\u8bb0-1",title:"\u91d1\u878d\u8bfb\u4e66\u7b14\u8bb0(1)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2020/%E9%87%91%E8%9E%8D%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B01/"}},{id:"post-gpt-3\u662f\u4ec0\u4e48",title:"GPT-3\u662f\u4ec0\u4e48?",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2020/gpt-3/"}},{id:"post-\u524d\u8111\u767d\u8d28\u5207\u9664\u672f",title:"\u524d\u8111\u767d\u8d28\u5207\u9664\u672f",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2020/%E5%89%8D%E8%84%91%E7%99%BD%E8%B4%A8%E5%88%87%E9%99%A4%E6%9C%AF/"}},{id:"post-personal-github",title:"Personal Github",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2020/person-github/"}},{id:"post-\u6c38\u522b\u4e86\u54402019",title:"\u6c38\u522b\u4e86\u54402019",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2019/%E6%B0%B8%E5%88%AB%E4%BA%86%E5%91%802019/"}},{id:"post-emacs-\u5165\u95e8-\u5360\u5751",title:"Emacs \u5165\u95e8(\u5360\u5751)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2019/emacs-%E5%85%A5%E9%97%A8%E5%8D%A0%E5%9D%91/"}},{id:"post-\u6211\u5728\u5907\u5fd8\u5f55\u90fd\u5199\u4e9b\u4ec0\u4e48",title:"\u6211\u5728\u5907\u5fd8\u5f55\u90fd\u5199\u4e9b\u4ec0\u4e48\uff1f",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2019/%E6%8C%96%E5%9D%91%E8%BF%87%E5%8E%BB%E7%9A%84%E4%B8%80%E5%B9%B4/"}},{id:"post-adam-vs-radam",title:"Adam vs. RAdam",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2019/%E5%8D%A0%E5%9D%91-adam-vs-radam/"}},{id:"post-\u7855\u58eb\u5f00\u5b66\u7b2c\u4e00\u5b66\u671f-\u5173\u4e8e\u7410\u4e8b\u548c\u5b66\u4e60\u53ca\u65b9\u6cd5",title:"\u7855\u58eb\u5f00\u5b66\u7b2c\u4e00\u5b66\u671f\u2014\u2014\u5173\u4e8e\u7410\u4e8b\u548c\u5b66\u4e60\u53ca\u65b9\u6cd5",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2018/%E7%A1%95%E5%A3%AB%E5%BC%80%E5%AD%A6%E7%AC%AC%E4%B8%80%E5%AD%A6%E6%9C%9F-%E5%85%B3%E4%BA%8E%E7%90%90%E4%BA%8B%E5%92%8C%E5%AD%A6%E4%B9%A0%E5%8F%8A%E6%96%B9%E6%B3%95/"}},{id:"post-emotion-recognition-based-on-tensorflow",title:"Emotion Recognition Based on Tensorflow",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2018/emotion-recognition-based-on-tensorflow/"}},{id:"post-python-review-amp-amp-some-simple-algorithms",title:"Python_review &amp; Some simple algorithms",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2018/python_review-some-simple-algorithms/"}},{id:"post-nlp-word2vec-skip-gram-cs224n-implemented-in-raw-way-and-in-tensorflow",title:"NLP: Word2Vec Skip-Gram(CS224n) implemented in raw way and in Tensorflow",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2018/nlp-word2vec-skip-gramcs224n/"}},{id:"post-cs231n-assignment3-image-captioning-with-rnns",title:"CS231n Assignment3_Image Captioning with RNNs",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2018/cs231n-assignment3_image-captioning-with-rnns/"}},{id:"post-pytorch-vs-tensorflow",title:"Pytorch VS Tensorflow",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2018/pytorch-vs-tensorflow/"}},{id:"post-cs231n-cnn-notes-amp-amp-assignment2",title:"CS231N_CNN_Notes&amp;Assignment2",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2018/cs231n_cnn_notesassignment2/"}},{id:"post-web-crawler\u722ctop100\u7535\u5f71\u4fe1\u606f",title:"Web_Crawler\u722ctop100\u7535\u5f71\u4fe1\u606f",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2018/web_crawler%E7%88%ACtop100%E7%94%B5%E5%BD%B1%E4%BF%A1%E6%81%AF/"}},{id:"post-\u725b\u5ba2\u7b97\u6cd5\u76f4\u64ad\u9898\u76ee\u603b\u7ed3day2t1234",title:"\u725b\u5ba2\u7b97\u6cd5\u76f4\u64ad\u9898\u76ee\u603b\u7ed3Day2T1234",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2017/%E7%89%9B%E5%AE%A2%E7%AE%97%E6%B3%95%E7%9B%B4%E6%92%AD%E9%A2%98%E7%9B%AE%E6%80%BB%E7%BB%93day2t1234/"}},{id:"post-\u725b\u5ba2\u7b97\u6cd5\u76f4\u64ad\u9898\u76ee\u603b\u7ed3day1t234",title:"\u725b\u5ba2\u7b97\u6cd5\u76f4\u64ad\u9898\u76ee\u603b\u7ed3Day1T234",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2017/%E7%89%9B%E5%AE%A2%E7%AE%97%E6%B3%95%E7%9B%B4%E6%92%AD%E9%A2%98%E7%9B%AE%E6%80%BB%E7%BB%93day1t234%E9%9B%B6%E5%92%8C%E5%8D%9A%E5%BC%88/"}},{id:"post-\u725b\u5ba2\u7b97\u6cd5\u76f4\u64ad\u9898\u76ee\u603b\u7ed3day1t1-\u96f6\u548c\u535a\u5f08",title:"\u725b\u5ba2\u7b97\u6cd5\u76f4\u64ad\u9898\u76ee\u603b\u7ed3Day1T1(\u96f6\u548c\u535a\u5f08)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2017/%E7%89%9B%E5%AE%A2%E7%AE%97%E6%B3%95%E7%9B%B4%E6%92%AD%E9%A2%98%E7%9B%AE%E6%80%BB%E7%BB%93/"}},{id:"post-java\u5b66\u4e60\u7b14\u8bb0-\u516d-\u88c5\u9970-\u89c2\u5bdf\u8005\u6a21\u5f0f\u53ca\u76d1\u542c\u5668",title:"Java\u5b66\u4e60\u7b14\u8bb0(\u516d):\u88c5\u9970\u3001\u89c2\u5bdf\u8005\u6a21\u5f0f\u53ca\u76d1\u542c\u5668",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2017/java%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E5%85%AD%E8%A3%85%E9%A5%B0-%E8%A7%82%E5%AF%9F%E8%80%85%E6%A8%A1%E5%BC%8F%E5%8F%8A%E7%9B%91%E5%90%AC%E5%99%A8/"}},{id:"post-java\u5b66\u4e60\u7b14\u8bb0-\u56db-filter",title:"Java\u5b66\u4e60\u7b14\u8bb0(\u56db):Filter",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2017/java%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E5%9B%9Bfilter/"}},{id:"post-java\u5b66\u4e60\u7b14\u8bb0-\u4e94-\u6587\u4ef6\u7684\u4e0a\u4f20\u548c\u4e0b\u8f7d",title:"Java\u5b66\u4e60\u7b14\u8bb0(\u4e94):\u6587\u4ef6\u7684\u4e0a\u4f20\u548c\u4e0b\u8f7d",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2017/java%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%BA%94%E6%96%87%E4%BB%B6%E7%9A%84%E4%B8%8A%E4%BC%A0%E5%92%8C%E4%B8%8B%E8%BD%BD/"}},{id:"post-jdbc\u5b66\u4e60\u7b14\u8bb0-\u4e09",title:"JDBC\u5b66\u4e60\u7b14\u8bb0(\u4e09)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2017/jdbc%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%89/"}},{id:"post-jdbc\u5b66\u4e60\u7b14\u8bb0-\u4e8c",title:"JDBC\u5b66\u4e60\u7b14\u8bb0(\u4e8c)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2017/jdbc%E7%AC%94%E8%AE%B0%E4%BA%8C/"}},{id:"post-jdbc\u5b66\u4e60\u7b14\u8bb0-\u4e00",title:"JDBC\u5b66\u4e60\u7b14\u8bb0(\u4e00)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2017/jdbc%E7%AC%94%E8%AE%B0%E5%8F%8A%E5%85%B6%E4%BB%96/"}},{id:"post-j2ee-cookie\u4e0esession\u77e5\u8bc6\u70b9\u56de\u987e",title:"J2EE_Cookie\u4e0eSession\u77e5\u8bc6\u70b9\u56de\u987e",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2017/j2ee_cookie%E4%B8%8Esession%E7%9F%A5%E8%AF%86%E7%82%B9%E5%9B%9E%E9%A1%BE/"}},{id:"post-bug-\u89e3\u51b3-nginx-502-bad-gateway",title:"Bug\uff1a\u89e3\u51b3 nginx 502 Bad GateWay",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2017/bug-%E8%A7%A3%E5%86%B3-nginx-502-bad-gateway/"}},{id:"post-windows-laravel\u73af\u5883\u90e8\u7f72",title:"Windows Laravel\u73af\u5883\u90e8\u7f72",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/windows-laravel%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/"}},{id:"post-git\u57fa\u7840\u8bb0\u5f55",title:"Git\u57fa\u7840\u8bb0\u5f55",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/git%E5%9F%BA%E7%A1%80%E8%AE%B0%E5%BD%95/"}},{id:"post-\u7b97\u6cd5\u5bfc\u8bba\u8bfb\u4e66\u7b14\u8bb0-6",title:"\u7b97\u6cd5\u5bfc\u8bba\u8bfb\u4e66\u7b14\u8bb0(6)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B06/"}},{id:"post-qt\u5b9e\u73b0\u7535\u5b50\u8bcd\u5178gui-project",title:"QT\u5b9e\u73b0\u7535\u5b50\u8bcd\u5178GUI\uff08project\uff09",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/qt%E5%AE%9E%E7%8E%B0%E7%94%B5%E5%AD%90%E8%AF%8D%E5%85%B8gui-project/"}},{id:"post-\u7b97\u6cd5\u5bfc\u8bba\u8bfb\u4e66\u7b14\u8bb0-5",title:"\u7b97\u6cd5\u5bfc\u8bba\u8bfb\u4e66\u7b14\u8bb0(5)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B05/"}},{id:"post-\u7b97\u6cd5\u5bfc\u8bba\u8bfb\u4e66\u7b14\u8bb0-4",title:"\u7b97\u6cd5\u5bfc\u8bba\u8bfb\u4e66\u7b14\u8bb0(4)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B04/"}},{id:"post-\u7b97\u6cd5\u5bfc\u8bba\u8bfb\u4e66\u7b14\u8bb0-3",title:"\u7b97\u6cd5\u5bfc\u8bba\u8bfb\u4e66\u7b14\u8bb0(3)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B03/"}},{id:"post-\u7b97\u6cd5\u5bfc\u8bba\u8bfb\u4e66\u7b14\u8bb0-2",title:"\u7b97\u6cd5\u5bfc\u8bba\u8bfb\u4e66\u7b14\u8bb0(2)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B02/"}},{id:"post-aws\u514d\u8d39\u670d\u52a1\u5668\u7533\u8bf7\u548c\u4e2d\u7ee7\u5668\u642d\u5efa",title:"AWS\u514d\u8d39\u670d\u52a1\u5668\u7533\u8bf7\u548c\u4e2d\u7ee7\u5668\u642d\u5efa",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/aws%E5%85%8D%E8%B4%B9%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%94%B3%E8%AF%B7%E5%92%8C%E4%B8%AD%E7%BB%A7%E5%99%A8%E6%90%AD%E5%BB%BA/"}},{id:"post-\u7b97\u6cd5\u5bfc\u8bba\u8bfb\u4e66\u7b14\u8bb0-1",title:"\u7b97\u6cd5\u5bfc\u8bba\u8bfb\u4e66\u7b14\u8bb0\uff081\uff09",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-1/"}},{id:"post-\u6570\u636e\u7ed3\u6784\u4e4b-\u6392\u5e8f",title:"\u6570\u636e\u7ed3\u6784\u4e4b\u2014\u2014\u6392\u5e8f",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2015/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B-%E6%8E%92%E5%BA%8F/"}},{id:"post-\u6570\u636e\u7ed3\u6784\u4e4b-\u56fe",title:"\u6570\u636e\u7ed3\u6784\u4e4b\u2014\u2014\u56fe",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2015/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B-%E5%9B%BE/"}},{id:"post-stl\u4e4bvector\u7528\u6cd5",title:"STL\u4e4bVector\u7528\u6cd5",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2015/stl%E4%B9%8Bvector%E7%94%A8%E6%B3%95/"}},{id:"post-\u6570\u636e\u7ed3\u6784\u4e4b-\u6811",title:"\u6570\u636e\u7ed3\u6784\u4e4b\u2014\u2014\u6811",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2015/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B-%E6%A0%91/"}},{id:"post-\u56de\u6eaf\u6cd5\u7ecf\u5178\u5e94\u7528-n\u7687\u540e-amp-amp-\u8ff7\u5bab\u95ee\u9898",title:"\u56de\u6eaf\u6cd5\u7ecf\u5178\u5e94\u7528\u2014\u2014N\u7687\u540e&amp;\u8ff7\u5bab\u95ee\u9898",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2015/%E5%9B%9E%E6%BA%AF%E6%B3%95%E7%BB%8F%E5%85%B8%E5%BA%94%E7%94%A8-n%E7%9A%87%E5%90%8E%E8%BF%B7%E5%AE%AB%E9%97%AE%E9%A2%98/"}},{id:"post-\u6570\u636e\u7ed3\u6784\u4e4b-\u961f\u5217",title:"\u6570\u636e\u7ed3\u6784\u4e4b\u2014\u2014\u961f\u5217",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2015/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B-%E9%98%9F%E5%88%97/"}},{id:"post-\u6570\u636e\u7ed3\u6784\u4e4b-\u6808",title:"\u6570\u636e\u7ed3\u6784\u4e4b\u2014\u2014\u6808",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2015/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B-%E6%A0%88/"}},{id:"post-\u5927\u4e8c\u7b2c\u4e00\u5b66\u671f\u8ba1\u5212-\u5907\u8868\u53ca\u5176\u4ed6",title:"\u5927\u4e8c\u7b2c\u4e00\u5b66\u671f\u8ba1\u5212/\u5907\u8868\u53ca\u5176\u4ed6",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2015/%E5%A4%A7%E4%BA%8C%E7%AC%AC%E4%B8%80%E5%AD%A6%E6%9C%9F%E8%AE%A1%E5%88%92%E5%A4%87%E8%A1%A8%E5%8F%8A%E5%85%B6%E4%BB%96/"}},{id:"post-\u5229\u7528\u9753\u6c64beautifulsoup4\u5199\u4e00\u4e2a\u7b80\u6613\u722c\u866b",title:"\u5229\u7528\u9753\u6c64BeautifulSoup4\u5199\u4e00\u4e2a\u7b80\u6613\u722c\u866b",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2015/%E5%88%A9%E7%94%A8%E9%9D%93%E6%B1%A4beautifulsoup4%E5%86%99%E4%B8%80%E4%B8%AA%E7%AE%80%E6%98%93%E7%88%AC%E8%99%AB/"}},{id:"post-\u6570\u636e\u7ed3\u6784\u4e4b-\u94fe\u8868",title:"\u6570\u636e\u7ed3\u6784\u4e4b\u2014\u2014\u94fe\u8868",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2015/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B-%E9%93%BE%E8%A1%A8/"}},{id:"post-\u6570\u636e\u7ed3\u6784\u4e4b-\u5411\u91cf",title:"\u6570\u636e\u7ed3\u6784\u4e4b\u2014\u2014\u5411\u91cf",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2015/vector/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%7A%68%65%6E%67%73%74%61%72%73@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/zhengstar94","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/zhengxingxing","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>
<!DOCTYPE html>
<html lang="en">
  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <!-- Metadata, OpenGraph and Schema.org -->




<!-- Standard metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>
  
  
    
      CS231N_CNN_Notes&amp;Assignment2 | Xingxing Zheng
    
  
</title>
<meta name="author" content="Xingxing Zheng">
<meta name="description" content="Welcome to Xingxing's blog, where I share my thoughts and experiences on various topics.
">










<!-- Bootstrap & MDB -->
<link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

<!-- Bootstrap Table -->


<!-- Fonts & Icons -->
<link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5">
<link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap">

<!-- Code Syntax Highlighting -->
<link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light">


  <!-- Sidebar Table of Contents -->
  <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet">


<!-- Styles -->

<!-- pseudocode -->



  <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">

<link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
<link rel="canonical" href="http://0.0.0.0:8080/blog/2018/cs231n_cnn_notesassignment2/">

<!-- Dark Mode -->
<script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script>

  <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark">
  <script>
    initTheme();
  </script>


<!-- GeoJSON support via Leaflet -->


<!-- diff2html -->






  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">
    <!-- Header -->
    <header>
  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation">
    <div class="container">
      
        <a class="navbar-brand title font-weight-lighter" href="/">
          
            
              <span class="font-weight-bold">Xingxing</span>
            
            
            Zheng
          
        </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">about
              
            </a>
          </li>

          <!-- Other pages -->
          
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
              
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/blog/">blog
                    
                  </a>
                </li>
              
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
              
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/repositories/">repositories
                    
                  </a>
                </li>
              
            
          
            
              
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/cv/">CV
                    
                  </a>
                </li>
              
            
          
            
              
                
                
                <li class="nav-item dropdown ">
                  <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">archive
                    
                  </a>
                  <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
                    
                      
                        <a class="dropdown-item " href="/archive_by_year">by year</a>
                      
                    
                      
                        <a class="dropdown-item " href="/archive_by_tag">by category</a>
                      
                    
                  </div>
                </li>
              
            
          
          
            <!-- Search -->
            <li class="nav-item">
              <button id="search-toggle" title="Search" onclick="openSearchModal()">
                <span class="nav-link">ctrl k <i class="ti ti-search"></i></span>
              </button>
            </li>
          
          
            <!-- Toogle theme mode -->
            <li class="toggle-container">
              <button id="light-toggle" title="Change theme">
                <i class="ti ti-sun-moon" id="light-toggle-system"></i>
                <i class="ti ti-moon-filled" id="light-toggle-dark"></i>
                <i class="ti ti-sun-filled" id="light-toggle-light"></i>
              </button>
            </li>
          
        </ul>
      </div>
    </div>
  </nav>
  
    <!-- Scrolling Progress Bar -->
    <progress id="progress" value="0">
      <div class="progress-container">
        <span class="progress-bar"></span>
      </div>
    </progress>
  
</header>


    <!-- Content -->
    <div class="container mt-5" role="main">
      
        
          <div class="row">
            <!-- sidebar, which will move to the top on a small screen -->
            <div class="col-sm-3">
              <nav id="toc-sidebar" class="sticky-top"></nav>
            </div>
            <!-- main content area -->
            <div class="col-sm-9">







<div class="post">
  <header class="post-header">
    <h1 class="post-title">CS231N_CNN_Notes&amp;Assignment2</h1>
    <p class="post-meta">
      Created in June 08, 2018
      
      
      
    </p>
    <p class="post-tags">
      
        <a href="/blog/2018"> <i class="fa-solid fa-calendar fa-sm"></i> 2018 </a>
      
      
          ·  
        
          
            <a href="/blog/tag/cs231n"> <i class="fa-solid fa-hashtag fa-sm"></i> cs231n</a>
          
          
        
      

      
          ·  
        
          
            <a href="/blog/category/dl-ml-python">
              <i class="fa-solid fa-tag fa-sm"></i> dl-ml-python</a>
          
          
        
      
    </p>
  </header>

  <article class="post-content">
    
    <div id="markdown-content">
      <p>The corresponding contents and materials of this blog are from Stanford Online Course CS231N and other Internet Resources.</p>

<hr>

<p><strong>Training Neural Networks:<img src="https://zhengliangliang.files.wordpress.com/2018/06/2018-06-07_140234.png" alt="2018-06-07_140234.png"></strong></p>

<p>This is a fundamental one layer network, and the following one is 2-layer Neural Network</p>

<p><img src="https://zhengliangliang.files.wordpress.com/2018/06/2018-06-07_140557.png" alt="2018-06-07_140557.png"></p>

<p>Convolutional Neural Networks, I think the best way to imagine CNN is that: say the task is to let computer know if a picture with a mouse(The input) is mouse(mouse)</p>

<p><img src="https://zhengliangliang.files.wordpress.com/2018/06/2018-06-07_140933.png" alt="2018-06-07_140933.png"></p>

<p>In the computer’s vision, we need to use a small rectangle to scan the whole picture like this:</p>

<p><img src="https://zhengliangliang.files.wordpress.com/2018/06/2018-06-07_141045.png" alt="2018-06-07_141045.png"></p>

<p>Knowing that a picture is composed with hundreds of thousands of pixels, in every step where the small rectangle scanned, it can get the original pixel matrix, then we put the original matrix to a filter( which is get the pixel representation), after filtering, we can get a filtering matrix,then we apply the multiplication and summation operation:</p>

<p><img src="https://zhengliangliang.files.wordpress.com/2018/06/2018-06-07_141523.png" alt="2018-06-07_141523.png"></p>

<p><img src="https://zhengliangliang.files.wordpress.com/2018/06/2018-06-07_141721.png" alt="2018-06-07_141721.png"></p>

<p>After scanning, we can get a activation map, if we have 6 filters, then the image will be transferred into a “new image” of size 28 * 28 * 6</p>

<p>and how do we know the size of the new image?  It depends on the size of the filters and the size of the original images, you can find some of the calculation in cs231n slides(Lecture 05)</p>

<hr>

<p><strong>Training Neural Networks:</strong></p>

<ul>
  <li>Activation Function</li>
</ul>

<p>Neural Science is a very complex field for humans nowadays , so it’s still a mystery for neural scientist to know what’s the detail functions and mechanisms of when one neural activates another, so in this maze, AI scientists use Activation function to simulate it.</p>

<p>some activation functions and there advantages and disadvantages:</p>

<p><img src="https://zhengliangliang.files.wordpress.com/2018/06/2018-06-07_142942.png" alt="2018-06-07_142942.png"></p>

<p>Problems with Sigmod : 1 . Saturated neurons “kill” the gradients 2 . Sigmoid outputs are not zero-centered(if the input to a neuron is always positive, then the hypothetical optimal w vector will be zig-zag path) 3 . exp() is a bit compute expensive</p>

<p>Problems with tanh(x) 1 . Saturated neurons “kill” the gradients But zero centered(nice!)</p>

<p>well, in ReLU, things are better :</p>

<p><img src="https://zhengliangliang.files.wordpress.com/2018/06/2018-06-07_144248.png" alt="2018-06-07_144248.png"></p>

<hr>

<p><strong>Preprocess the data</strong></p>

<p><img src="https://zhengliangliang.files.wordpress.com/2018/06/2018-06-07_144430.png" alt="2018-06-07_144430.png"></p>

<p><strong>Weight Initialization</strong></p>

<p>What would happen if the weight is constant, then all activations would become zero!</p>

<p>then we should use a resonable initialization, which is Xavier initialization:</p>

<p><img src="https://zhengliangliang.files.wordpress.com/2018/06/2018-06-07_145004.png" alt="2018-06-07_145004.png"></p>

<p>Remember: Nice distribution of activations at all layers will make the learning proceeds nicely.</p>

<hr>

<p><strong>Batch Normalization</strong></p>

<p>We want zero-mean unit-variance activations</p>

<p><img src="https://zhengliangliang.files.wordpress.com/2018/06/2018-06-07_145709.png" alt="2018-06-07_145709.png"></p>

<p>You may ask, why batch normalization: The introduction of Batch Normalization is to overcome the difficulty of training big and deep NNs, bn can also prevent the gradient dispersion,in this way the gradient can flow through the network.</p>

<p><strong>Babysitting the Learning Process:</strong></p>

<ol>
  <li>double check that the loss is reasonable  (Sanity Check)</li>
  <li>Tricks(Make sure that your model can overfit very small protion of training data)</li>
</ol>

<p><strong>Hyperparameter Optimization:</strong></p>

<p>Cross-Validation Strategy:</p>

<p><img src="https://zhengliangliang.files.wordpress.com/2018/06/2018-06-08_092710.png" alt="2018-06-08_092710.png"></p>

<p>It’s best to optimize in log space</p>

<p><img src="https://zhengliangliang.files.wordpress.com/2018/06/2018-06-08_092838.png" alt="2018-06-08_092838.png"></p>

<p>Monitoring and visualizing the loss curve will make it easy for programmers to debug.</p>

<p>Some normal problems occurs when visualizing the loss curve:</p>

<p><img src="https://zhengliangliang.files.wordpress.com/2018/06/2018-06-08_093255.png" alt="2018-06-08_093255.png"></p>

<p><img src="https://zhengliangliang.files.wordpress.com/2018/06/2018-06-08_093333.png" alt="2018-06-08_093333.png"></p>

<hr>

<p><strong>Batch Normalization at Test time:</strong></p>

<p>We can’t use the formal batch normalization for calculating mean and variance in the test time, so instead of using it, we replace it as the following:</p>

<p><img src="https://zhengliangliang.files.wordpress.com/2018/06/2018-06-08_094121.png" alt="2018-06-08_094121.png"></p>

<p>There are different kinds of normalization in training partG:</p>

<p><img src="https://zhengliangliang.files.wordpress.com/2018/06/2018-06-08_094301.png" alt="2018-06-08_094301.png"></p>

<p><img src="https://zhengliangliang.files.wordpress.com/2018/06/2018-06-08_094313.png" alt="2018-06-08_094313.png"></p>

<p><img src="https://zhengliangliang.files.wordpress.com/2018/06/2018-06-08_094332.png" alt="2018-06-08_094332.png"></p>

<p><img src="https://zhengliangliang.files.wordpress.com/2018/06/2018-06-08_094341.png" alt="2018-06-08_094341.png"></p>

<p><strong>Optimization:</strong></p>

<p>Previous classes we’ve learnt the SGD optimization, but there are problems with SGD:</p>

<p><img src="https://zhengliangliang.files.wordpress.com/2018/06/2018-06-08_094542.png" alt="2018-06-08_094542.png"></p>

<p><img src="https://zhengliangliang.files.wordpress.com/2018/06/2018-06-08_094636.png" alt="2018-06-08_094636.png"></p>

<p>So scientist come up with SGD + Momontum Optimization”</p>

<p><img src="https://zhengliangliang.files.wordpress.com/2018/06/2018-06-08_094751.png" alt="2018-06-08_094751.png"></p>

<p>Let’s get into the most important 2 optimization ways:</p>

<p><img src="https://zhengliangliang.files.wordpress.com/2018/06/2018-06-08_095001.png" alt="2018-06-08_095001.png"></p>

<p><img src="https://zhengliangliang.files.wordpress.com/2018/06/2018-06-08_095117.png" alt="2018-06-08_095117.png"></p>

<p><strong>Regularization : Dropout</strong></p>

<p>randomly set some neurons to zero Probability of dropping is a hyperparameter, and 0.5 is common</p>

<p><img src="https://zhengliangliang.files.wordpress.com/2018/06/2018-06-08_095505.png" alt="2018-06-08_095505.png"></p>

<p>More common is the “Inverted Dropout”</p>

<p><img src="https://zhengliangliang.files.wordpress.com/2018/06/2018-06-08_095740.png" alt="2018-06-08_095740.png"></p>

<hr>

<p>Transfer Learning:</p>

<p><img src="https://zhengliangliang.files.wordpress.com/2018/06/2018-06-08_120359.png" alt="2018-06-08_120359.png"></p>

<p>and I will write some introductions about this in later blogs ,stay tuned.</p>

<hr>

<p><strong>Assignment:</strong></p>

<p>First of all, we need to implement a FullyConnectedNets</p>

<p>Affine Layer in forward pass:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mi">1</span> <span class="k">def</span> <span class="nf">affine_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
 <span class="mi">2</span>  <span class="sh">"""</span><span class="s">
 3  Computes the forward pass for an affine (fully-connected) layer.
 4 
 5  The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N
 6  examples, where each example x[i] has shape (d_1, ..., d_k). We will
 7  reshape each input into a vector of dimension D = d_1 * ... * d_k, and
 8  then transform it to an output vector of dimension M.
 9 
10   Inputs:
11   - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)
12   - w: A numpy array of weights, of shape (D, M)
13   - b: A numpy array of biases, of shape (M,)
14   
15   Returns a tuple of:
16   - out: output, of shape (N, M)
17   - cache: (x, w, b)
18   </span><span class="sh">"""</span>
<span class="mi">19</span>   <span class="n">out</span> <span class="o">=</span> <span class="bp">None</span>
<span class="mi">20</span>   <span class="c1">#############################################################################
</span><span class="mi">21</span>   <span class="c1"># TODO: Implement the affine forward pass. Store the result in out. You     #
</span><span class="mi">22</span>   <span class="c1"># will need to reshape the input into rows.                                 #
</span><span class="mi">23</span>   <span class="c1">#############################################################################
</span><span class="mi">24</span>   <span class="n">N</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="mi">25</span>   <span class="n">x_rsp</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="mi">26</span>   <span class="n">out</span> <span class="o">=</span> <span class="n">x_rsp</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
<span class="mi">27</span>   <span class="c1">#pass
</span><span class="mi">28</span>   <span class="c1">#############################################################################
</span><span class="mi">29</span>   <span class="c1">#                             END OF YOUR CODE                              #
</span><span class="mi">30</span>   <span class="c1">#############################################################################
</span><span class="mi">31</span>   <span class="n">cache</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="mi">32</span>   <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">cache</span>

<span class="n">Turn</span> <span class="n">the</span> <span class="n">x</span> <span class="n">to</span> <span class="n">a</span> <span class="n">vector</span> <span class="ow">and</span> <span class="n">multiply</span> <span class="n">w</span><span class="p">.</span>

 <span class="mi">1</span> <span class="k">def</span> <span class="nf">affine_backward</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
 <span class="mi">2</span>  <span class="sh">"""</span><span class="s">
 3  Computes the backward pass for an affine layer.
 4 
 5  Inputs:
 6  - dout: Upstream derivative, of shape (N, M)
 7  - cache: Tuple of:
 8    - x: Input data, of shape (N, d_1, ... d_k)
 9    - w: Weights, of shape (D, M)
10 
11   Returns a tuple of:
12   - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)
13   - dw: Gradient with respect to w, of shape (D, M)
14   - db: Gradient with respect to b, of shape (M,)
15   </span><span class="sh">"""</span>
<span class="mi">16</span>   <span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">cache</span>
<span class="mi">17</span>   <span class="n">dx</span><span class="p">,</span> <span class="n">dw</span><span class="p">,</span> <span class="n">db</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span>
<span class="mi">18</span>   <span class="c1">#############################################################################
</span><span class="mi">19</span>   <span class="c1"># TODO: Implement the affine backward pass.                                 #
</span><span class="mi">20</span>   <span class="c1">#############################################################################
</span><span class="mi">21</span>   <span class="n">N</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  
<span class="mi">22</span>   <span class="n">x_rsp</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">N</span> <span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  
<span class="mi">23</span>   <span class="n">dx</span> <span class="o">=</span> <span class="n">dout</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">w</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
<span class="mi">24</span>   <span class="n">dx</span> <span class="o">=</span> <span class="n">dx</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="mi">25</span>   <span class="n">dw</span> <span class="o">=</span> <span class="n">x_rsp</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">dout</span><span class="p">)</span>
<span class="mi">26</span>   <span class="n">db</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="mi">27</span>   <span class="c1">#pass
</span><span class="mi">28</span>   <span class="c1">#############################################################################
</span><span class="mi">29</span>   <span class="c1">#                             END OF YOUR CODE                              #
</span><span class="mi">30</span>   <span class="c1">#############################################################################
</span><span class="mi">31</span>   <span class="k">return</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dw</span><span class="p">,</span> <span class="n">db</span>
</code></pre></div></div>
<p>And the affine_backward function will be the simple gradient process above.</p>

<p>“Sandwich” layers:  There are some common patterns of layers that are frequently used in neural nets. For example, affine layers are frequently followed by a ReLU nonlinearity.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mi">1</span> <span class="k">def</span> <span class="nf">affine_relu_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
 <span class="mi">2</span>    <span class="sh">"""</span><span class="s">
 3    Convenience layer that perorms an affine transform followed by a ReLU
 4 
 5    Inputs:
 6    - x: Input to the affine layer
 7    - w, b: Weights for the affine layer
 8 
 9    Returns a tuple of:
10     - out: Output from the ReLU
11     - cache: Object to give to the backward pass
12     </span><span class="sh">"""</span>
<span class="mi">13</span>     <span class="n">a</span><span class="p">,</span> <span class="n">fc_cache</span> <span class="o">=</span> <span class="nf">affine_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="mi">14</span>     <span class="n">out</span><span class="p">,</span> <span class="n">relu_cache</span> <span class="o">=</span> <span class="nf">relu_forward</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="mi">15</span>     <span class="n">cache</span> <span class="o">=</span> <span class="p">(</span><span class="n">fc_cache</span><span class="p">,</span> <span class="n">relu_cache</span><span class="p">)</span>
<span class="mi">16</span>     <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">cache</span>
</code></pre></div></div>
<p>and the backward is as easy as the forward:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mi">1</span> <span class="k">def</span> <span class="nf">affine_relu_backward</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
 <span class="mi">2</span>    <span class="sh">"""</span><span class="s">
 3    Backward pass for the affine-relu convenience layer
 4    </span><span class="sh">"""</span>
 <span class="mi">5</span>    <span class="n">fc_cache</span><span class="p">,</span> <span class="n">relu_cache</span> <span class="o">=</span> <span class="n">cache</span>
 <span class="mi">6</span>    <span class="n">da</span> <span class="o">=</span> <span class="nf">relu_backward</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">relu_cache</span><span class="p">)</span>
 <span class="mi">7</span>    <span class="n">dx</span><span class="p">,</span> <span class="n">dw</span><span class="p">,</span> <span class="n">db</span> <span class="o">=</span> <span class="nf">affine_backward</span><span class="p">(</span><span class="n">da</span><span class="p">,</span> <span class="n">fc_cache</span><span class="p">)</span>
 <span class="mi">8</span>    <span class="k">return</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dw</span><span class="p">,</span> <span class="n">db</span>
</code></pre></div></div>
<p>And the loss Layers (Loss Function)to compute the  loss, i will omit the detailed algorithm here but you can find more information and algorithms in this <a href="https://blog.csdn.net/u012767526/article/details/51396196" rel="external nofollow noopener" target="_blank">link</a></p>

<p><strong>Two- Layer network: A two-layer fully-connected neural network with ReLU nonlinearity and softmax loss that uses a modular layer design. We assume an input dimension of D, a hidden dimension of H, and perform classification over C classes.</strong></p>

<p>and the architecture should be affine - relu - affine - softmax</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mi">1</span>        <span class="c1">############################################################################
</span> <span class="mi">2</span>        <span class="c1"># TODO: Initialize the weights and biases of the two-layer net. Weights    #
</span> <span class="mi">3</span>        <span class="c1"># should be initialized from a Gaussian centered at 0.0 with               #
</span> <span class="mi">4</span>        <span class="c1"># standard deviation equal to weight_scale, and biases should be           #
</span> <span class="mi">5</span>        <span class="c1"># initialized to zero. All weights and biases should be stored in the      #
</span> <span class="mi">6</span>        <span class="c1"># dictionary self.params, with first layer weights                         #
</span> <span class="mi">7</span>        <span class="c1"># and biases using the keys 'W1' and 'b1' and second layer                 #
</span> <span class="mi">8</span>        <span class="c1"># weights and biases using the keys 'W2' and 'b2'.                         #
</span> <span class="mi">9</span>        <span class="c1">############################################################################
</span><span class="mi">10</span>         <span class="c1"># initialization 
</span><span class="mi">11</span>         <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">W1</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight_scale</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span><span class="n">hidden_dim</span><span class="p">)</span>
<span class="mi">12</span>         <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">b1</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">)</span>
<span class="mi">13</span>         <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">W2</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight_scale</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span><span class="n">num_classes</span><span class="p">)</span>
<span class="mi">14</span>         <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">b2</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">num_classes</span><span class="p">)</span>
</code></pre></div></div>
<p>Then is the loss function,we use the softmax,after forward passing , we use softmax to calculate the loss and backward passing to count the grad. and in the following code, notice that if y is none, then it is in the test, otherwise it is in the training stage.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mi">1</span>   <span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
 <span class="mi">2</span>        <span class="sh">"""</span><span class="s">
 3        Compute loss and gradient for a minibatch of data.
 4 
 5        Inputs:
 6        - X: Array of input data of shape (N, d_1, ..., d_k)
 7        - y: Array of labels, of shape (N,). y[i] gives the label for X[i].
 8 
 9        Returns:
10         If y is None, then run a test-time forward pass of the model and return:
11         - scores: Array of shape (N, C) giving classification scores, where
12           scores[i, c] is the classification score for X[i] and class c.
13 
14         If y is not None, then run a training-time forward and backward pass and
15         return a tuple of:
16         - loss: Scalar value giving the loss
17         - grads: Dictionary with the same keys as self.params, mapping parameter
18           names to gradients of the loss with respect to those parameters.
19         </span><span class="sh">"""</span>
<span class="mi">20</span>         <span class="n">scores</span> <span class="o">=</span> <span class="bp">None</span>
<span class="mi">21</span>         <span class="c1">############################################################################
</span><span class="mi">22</span>         <span class="c1"># TODO: Implement the forward pass for the two-layer net, computing the    #
</span><span class="mi">23</span>         <span class="c1"># class scores for X and storing them in the scores variable.              #
</span><span class="mi">24</span>         <span class="c1">############################################################################
</span><span class="mi">25</span>         <span class="n">ar1_out</span><span class="p">,</span> <span class="n">ar1_cache</span> <span class="o">=</span> <span class="nf">affine_relu_forward</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">W1</span><span class="sh">'</span><span class="p">],</span><span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">b1</span><span class="sh">'</span><span class="p">])</span>
<span class="mi">26</span>         <span class="n">a2_out</span><span class="p">,</span> <span class="n">a2_cache</span><span class="o">=</span> <span class="nf">affine_forward</span><span class="p">(</span><span class="n">ar1_out</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">W2</span><span class="sh">'</span><span class="p">],</span><span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">b2</span><span class="sh">'</span><span class="p">])</span>
<span class="mi">27</span>         <span class="n">scores</span> <span class="o">=</span> <span class="n">a2_out</span>
<span class="mi">28</span>         <span class="c1">#pass
</span><span class="mi">29</span>         <span class="c1">############################################################################
</span><span class="mi">30</span>         <span class="c1">#                             END OF YOUR CODE                             #
</span><span class="mi">31</span>         <span class="c1">############################################################################
</span><span class="mi">32</span> 
<span class="mi">33</span>         <span class="c1"># If y is None then we are in test mode so just return scores
</span><span class="mi">34</span>         <span class="k">if</span> <span class="n">y</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
<span class="mi">35</span>             <span class="k">return</span> <span class="n">scores</span>
<span class="mi">36</span> 
<span class="mi">37</span>         <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="p">{}</span>
<span class="mi">38</span>         <span class="c1">############################################################################
</span><span class="mi">39</span>         <span class="c1"># TODO: Implement the backward pass for the two-layer net. Store the loss  #
</span><span class="mi">40</span>         <span class="c1"># in the loss variable and gradients in the grads dictionary. Compute data #
</span><span class="mi">41</span>         <span class="c1"># loss using softmax, and make sure that grads[k] holds the gradients for  #
</span><span class="mi">42</span>         <span class="c1"># self.params[k]. Don't forget to add L2 regularization!                   #
</span><span class="mi">43</span>         <span class="c1">#                                                                          #
</span><span class="mi">44</span>         <span class="c1"># NOTE: To ensure that your implementation matches ours and you pass the   #
</span><span class="mi">45</span>         <span class="c1"># automated tests, make sure that your L2 regularization includes a factor #
</span><span class="mi">46</span>         <span class="c1"># of 0.5 to simplify the expression for the gradient.                      #
</span><span class="mi">47</span>         <span class="c1">############################################################################
</span><span class="mi">48</span>         <span class="n">loss</span><span class="p">,</span><span class="n">dscores</span> <span class="o">=</span> <span class="nf">softmax_loss</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="mi">49</span>         <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">reg</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">W1</span><span class="sh">'</span><span class="p">]</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">W1</span><span class="sh">'</span><span class="p">])</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">reg</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">W2</span><span class="sh">'</span><span class="p">]</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">W2</span><span class="sh">'</span><span class="p">])</span>
<span class="mi">50</span>         <span class="n">dx2</span><span class="p">,</span> <span class="n">dw2</span><span class="p">,</span> <span class="n">db2</span> <span class="o">=</span> <span class="nf">affine_backward</span><span class="p">(</span><span class="n">dscores</span><span class="p">,</span> <span class="n">a2_cache</span><span class="p">)</span>
<span class="mi">51</span>         <span class="n">grads</span><span class="p">[</span><span class="sh">'</span><span class="s">W2</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">dw2</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">reg</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">W2</span><span class="sh">'</span><span class="p">]</span>
<span class="mi">52</span>         <span class="n">grads</span><span class="p">[</span><span class="sh">'</span><span class="s">b2</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">db2</span>
<span class="mi">53</span>         <span class="c1">#dx2_relu = relu_backward(dx2, r1_cache)
</span><span class="mi">54</span>         <span class="c1">#dx1, dw1, db1 = affine_backward(dx2_relu, a1_cache)
</span><span class="mi">55</span>         <span class="n">dx1</span> <span class="p">,</span> <span class="n">dw1</span><span class="p">,</span> <span class="n">db1</span> <span class="o">=</span> <span class="nf">affine_relu_backward</span><span class="p">(</span><span class="n">dx2</span><span class="p">,</span> <span class="n">ar1_cache</span><span class="p">)</span>
<span class="mi">56</span>         <span class="n">grads</span><span class="p">[</span><span class="sh">'</span><span class="s">W1</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">dw1</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">reg</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">W1</span><span class="sh">'</span><span class="p">]</span>
<span class="mi">57</span>         <span class="n">grads</span><span class="p">[</span><span class="sh">'</span><span class="s">b1</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">db1</span>
<span class="mi">58</span>         <span class="c1">#pass
</span><span class="mi">59</span>         <span class="c1">############################################################################
</span><span class="mi">60</span>         <span class="c1">#                             END OF YOUR CODE                             #
</span><span class="mi">61</span>         <span class="c1">############################################################################
</span><span class="mi">62</span> 
<span class="mi">63</span>         <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span>
</code></pre></div></div>
<p>Solver part: pre-define all the parameters such as update_rule or optimization configuration (Learning rate),lr decay ,number of epochs and so on, and usually the usage of the solve will be like:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mi">1</span> <span class="n">Example</span> <span class="n">usage</span> <span class="n">might</span> <span class="n">look</span> <span class="n">something</span> <span class="n">like</span> <span class="n">this</span><span class="p">:</span>
 <span class="mi">2</span> 
 <span class="mi">3</span>    <span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
 <span class="mi">4</span>      <span class="sh">'</span><span class="s">X_train</span><span class="sh">'</span><span class="p">:</span> <span class="c1"># training data
</span> <span class="mi">5</span>      <span class="sh">'</span><span class="s">y_train</span><span class="sh">'</span><span class="p">:</span> <span class="c1"># training labels
</span> <span class="mi">6</span>      <span class="sh">'</span><span class="s">X_val</span><span class="sh">'</span><span class="p">:</span> <span class="c1"># validation data
</span> <span class="mi">7</span>      <span class="sh">'</span><span class="s">y_val</span><span class="sh">'</span><span class="p">:</span> <span class="c1"># validation labels
</span> <span class="mi">8</span>    <span class="p">}</span>
 <span class="mi">9</span>    <span class="n">model</span> <span class="o">=</span> <span class="nc">MyAwesomeModel</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">reg</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="mi">10</span>     <span class="n">solver</span> <span class="o">=</span> <span class="nc">Solver</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span>
<span class="mi">11</span>                     <span class="n">update_rule</span><span class="o">=</span><span class="sh">'</span><span class="s">sgd</span><span class="sh">'</span><span class="p">,</span>
<span class="mi">12</span>                     <span class="n">optim_config</span><span class="o">=</span><span class="p">{</span>
<span class="mi">13</span>                       <span class="sh">'</span><span class="s">learning_rate</span><span class="sh">'</span><span class="p">:</span> <span class="mf">1e-3</span><span class="p">,</span>
<span class="mi">14</span>                     <span class="p">},</span>
<span class="mi">15</span>                     <span class="n">lr_decay</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>
<span class="mi">16</span>                     <span class="n">num_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
<span class="mi">17</span>                     <span class="n">print_every</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="mi">18</span>     <span class="n">solver</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>

<span class="o">-----------------------------------------------------------------------------------------------------------------------------</span>
</code></pre></div></div>
<p><strong>Multilayer Network: Inplement a fully-connected network with an arbitrary number of hidden layers</strong></p>

<p>And the architecture will be :</p>

<p>{affine - [batch/layer norm] - relu - [dropout]} x (L - 1) - affine - softmax</p>

<p>Initialize a new FullyConnectedNet.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mi">1</span> <span class="k">class</span> <span class="nc">FullyConnectedNet</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
 <span class="mi">2</span>    <span class="sh">"""</span><span class="s">
 3    A fully-connected neural network with an arbitrary number of hidden layers,
 4    ReLU nonlinearities, and a softmax loss function. This will also implement
 5    dropout and batch/layer normalization as options. For a network with L layers,
 6    the architecture will be
 7 
 8    {affine - [batch/layer norm] - relu - [dropout]} x (L - 1) - affine - softmax
 9 
10     where batch/layer normalization and dropout are optional, and the {...} block is
11     repeated L - 1 times.
12 
13     Similar to the TwoLayerNet above, learnable parameters are stored in the
14     self.params dictionary and will be learned using the Solver class.
15     </span><span class="sh">"""</span>
<span class="mi">16</span> 
<span class="mi">17</span>     <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_dims</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">3</span><span class="o">*</span><span class="mi">32</span><span class="o">*</span><span class="mi">32</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="mi">18</span>                  <span class="n">dropout</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">normalization</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">reg</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
<span class="mi">19</span>                  <span class="n">weight_scale</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
<span class="mi">20</span>         <span class="sh">"""</span><span class="s">
21         Initialize a new FullyConnectedNet.
22 
23         Inputs:
24         - hidden_dims: A list of integers giving the size of each hidden layer.
25         - input_dim: An integer giving the size of the input.
26         - num_classes: An integer giving the number of classes to classify.
27         - dropout: Scalar between 0 and 1 giving dropout strength. If dropout=1 then
28           the network should not use dropout at all.
29         - normalization: What type of normalization the network should use. Valid values
30           are </span><span class="sh">"</span><span class="s">batchnorm</span><span class="sh">"</span><span class="s">, </span><span class="sh">"</span><span class="s">layernorm</span><span class="sh">"</span><span class="s">, or None for no normalization (the default).
31         - reg: Scalar giving L2 regularization strength.
32         - weight_scale: Scalar giving the standard deviation for random
33           initialization of the weights.
34         - dtype: A numpy datatype object; all computations will be performed using
35           this datatype. float32 is faster but less accurate, so you should use
36           float64 for numeric gradient checking.
37         - seed: If not None, then pass this random seed to the dropout layers. This
38           will make the dropout layers deteriminstic so we can gradient check the
39           model.
40         </span><span class="sh">"""</span>
<span class="mi">41</span>         <span class="n">self</span><span class="p">.</span><span class="n">normalization</span> <span class="o">=</span> <span class="n">normalization</span>
<span class="mi">42</span>         <span class="n">self</span><span class="p">.</span><span class="n">use_dropout</span> <span class="o">=</span> <span class="n">dropout</span> <span class="o">!=</span> <span class="mi">1</span>
<span class="mi">43</span>         <span class="n">self</span><span class="p">.</span><span class="n">reg</span> <span class="o">=</span> <span class="n">reg</span>
<span class="mi">44</span>         <span class="n">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="nf">len</span><span class="p">(</span><span class="n">hidden_dims</span><span class="p">)</span>
<span class="mi">45</span>         <span class="n">self</span><span class="p">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span>
<span class="mi">46</span>         <span class="n">self</span><span class="p">.</span><span class="n">params</span> <span class="o">=</span> <span class="p">{}</span>
<span class="mi">47</span> 
<span class="mi">48</span>         <span class="c1">############################################################################
</span><span class="mi">49</span>         <span class="c1"># TODO: Initialize the parameters of the network, storing all values in    #
</span><span class="mi">50</span>         <span class="c1"># the self.params dictionary. Store weights and biases for the first layer #
</span><span class="mi">51</span>         <span class="c1"># in W1 and b1; for the second layer use W2 and b2, etc. Weights should be #
</span><span class="mi">52</span>         <span class="c1"># initialized from a normal distribution centered at 0 with standard       #
</span><span class="mi">53</span>         <span class="c1"># deviation equal to weight_scale. Biases should be initialized to zero.   #
</span><span class="mi">54</span>         <span class="c1">#                                                                          #
</span><span class="mi">55</span>         <span class="c1"># When using batch normalization, store scale and shift parameters for the #
</span><span class="mi">56</span>         <span class="c1"># first layer in gamma1 and beta1; for the second layer use gamma2 and     #
</span><span class="mi">57</span>         <span class="c1"># beta2, etc. Scale parameters should be initialized to ones and shift     #
</span><span class="mi">58</span>         <span class="c1"># parameters should be initialized to zeros.                               #
</span><span class="mi">59</span>         <span class="c1">############################################################################
</span><span class="mi">60</span>         <span class="c1"># initialization
</span><span class="mi">61</span>         <span class="n">layer_input_dim</span> <span class="o">=</span> <span class="n">input_dim</span>
<span class="mi">62</span>         <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">hd</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">hidden_dims</span><span class="p">):</span>
<span class="mi">63</span>             <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">W%d</span><span class="sh">'</span><span class="o">%</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">weight_scale</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">layer_input_dim</span><span class="p">,</span><span class="n">hd</span><span class="p">)</span>
<span class="mi">64</span>             <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">b%d</span><span class="sh">'</span><span class="o">%</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">weight_scale</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">hd</span><span class="p">)</span>
<span class="mi">65</span>             <span class="c1">#batch normalization
</span><span class="mi">66</span>             <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">normalization</span><span class="p">:</span>
<span class="mi">67</span>                 <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">gamma%d</span><span class="sh">'</span><span class="o">%</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">hd</span><span class="p">)</span>
<span class="mi">68</span>                 <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">beta%d</span><span class="sh">'</span><span class="o">%</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">hd</span><span class="p">)</span>
<span class="mi">69</span>             <span class="n">layer_input_dim</span> <span class="o">=</span> <span class="n">hd</span>
<span class="mi">70</span>         <span class="c1">#final output layer
</span><span class="mi">71</span>         <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">W%d</span><span class="sh">'</span><span class="o">%</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">)]</span> <span class="o">=</span> <span class="n">weight_scale</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">layer_input_dim</span><span class="p">,</span><span class="n">num_classes</span><span class="p">)</span>
<span class="mi">72</span>         <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">b%d</span><span class="sh">'</span><span class="o">%</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">)]</span> <span class="o">=</span> <span class="n">weight_scale</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">num_classes</span><span class="p">)</span>        
<span class="mi">73</span>         <span class="c1">#pass
</span><span class="mi">74</span>         <span class="c1">############################################################################
</span><span class="mi">75</span>         <span class="c1">#                             END OF YOUR CODE                             #
</span><span class="mi">76</span>         <span class="c1">############################################################################
</span><span class="mi">77</span> 
<span class="mi">78</span>         <span class="c1"># When using dropout we need to pass a dropout_param dictionary to each
</span><span class="mi">79</span>         <span class="c1"># dropout layer so that the layer knows the dropout probability and the mode
</span><span class="mi">80</span>         <span class="c1"># (train / test). You can pass the same dropout_param to each dropout layer.
</span><span class="mi">81</span>         <span class="n">self</span><span class="p">.</span><span class="n">dropout_param</span> <span class="o">=</span> <span class="p">{}</span>
<span class="mi">82</span>         <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">use_dropout</span><span class="p">:</span>
<span class="mi">83</span>             <span class="n">self</span><span class="p">.</span><span class="n">dropout_param</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">mode</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">p</span><span class="sh">'</span><span class="p">:</span> <span class="n">dropout</span><span class="p">}</span>
<span class="mi">84</span>             <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
<span class="mi">85</span>                 <span class="n">self</span><span class="p">.</span><span class="n">dropout_param</span><span class="p">[</span><span class="sh">'</span><span class="s">seed</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">seed</span>
<span class="mi">86</span> 
<span class="mi">87</span>         <span class="c1"># With batch normalization we need to keep track of running means and
</span><span class="mi">88</span>         <span class="c1"># variances, so we need to pass a special bn_param object to each batch
</span><span class="mi">89</span>         <span class="c1"># normalization layer. You should pass self.bn_params[0] to the forward pass
</span><span class="mi">90</span>         <span class="c1"># of the first batch normalization layer, self.bn_params[1] to the forward
</span><span class="mi">91</span>         <span class="c1"># pass of the second batch normalization layer, etc.
</span><span class="mi">92</span>         <span class="n">self</span><span class="p">.</span><span class="n">bn_params</span> <span class="o">=</span> <span class="p">[]</span>
<span class="mi">93</span>         <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">normalization</span><span class="o">==</span><span class="sh">'</span><span class="s">batchnorm</span><span class="sh">'</span><span class="p">:</span>
<span class="mi">94</span>             <span class="n">self</span><span class="p">.</span><span class="n">bn_params</span> <span class="o">=</span> <span class="p">[{</span><span class="sh">'</span><span class="s">mode</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">}</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>
<span class="mi">95</span>         <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">normalization</span><span class="o">==</span><span class="sh">'</span><span class="s">layernorm</span><span class="sh">'</span><span class="p">:</span>
<span class="mi">96</span>             <span class="n">self</span><span class="p">.</span><span class="n">bn_params</span> <span class="o">=</span> <span class="p">[{}</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>
<span class="mi">97</span> 
<span class="mi">98</span>         <span class="c1"># Cast all parameters to the correct datatype
</span><span class="mi">99</span>         <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
<span class="mi">100</span>             <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></div>
<p>Same as the two-layers net, the loss function :</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mi">1</span>    <span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
 <span class="mi">2</span>        <span class="sh">"""</span><span class="s">
 3        Compute loss and gradient for the fully-connected net.
 4 
 5        Input / output: Same as TwoLayerNet above.
 6        </span><span class="sh">"""</span>
 <span class="mi">7</span>        <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">dtype</span><span class="p">)</span>
 <span class="mi">8</span>        <span class="n">mode</span> <span class="o">=</span> <span class="sh">'</span><span class="s">test</span><span class="sh">'</span> <span class="k">if</span> <span class="n">y</span> <span class="ow">is</span> <span class="bp">None</span> <span class="k">else</span> <span class="sh">'</span><span class="s">train</span><span class="sh">'</span>
 <span class="mi">9</span> 
<span class="mi">10</span>         <span class="c1"># Set train/test mode for batchnorm params and dropout param since they
</span><span class="mi">11</span>         <span class="c1"># behave differently during training and testing.
</span><span class="mi">12</span>         <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">use_dropout</span><span class="p">:</span>
<span class="mi">13</span>             <span class="n">self</span><span class="p">.</span><span class="n">dropout_param</span><span class="p">[</span><span class="sh">'</span><span class="s">mode</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">mode</span>
<span class="mi">14</span>         <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">normalization</span><span class="o">==</span><span class="sh">'</span><span class="s">batchnorm</span><span class="sh">'</span><span class="p">:</span>
<span class="mi">15</span>             <span class="k">for</span> <span class="n">bn_param</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">bn_params</span><span class="p">:</span>
<span class="mi">16</span>                 <span class="n">bn_param</span><span class="p">[</span><span class="sh">'</span><span class="s">mode</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">mode</span>
<span class="mi">17</span>         <span class="n">scores</span> <span class="o">=</span> <span class="bp">None</span>
<span class="mi">18</span>         <span class="c1">############################################################################
</span><span class="mi">19</span>         <span class="c1"># TODO: Implement the forward pass for the fully-connected net, computing  #
</span><span class="mi">20</span>         <span class="c1"># the class scores for X and storing them in the scores variable.          #
</span><span class="mi">21</span>         <span class="c1">#                                                                          #
</span><span class="mi">22</span>         <span class="c1"># When using dropout, you'll need to pass self.dropout_param to each       #
</span><span class="mi">23</span>         <span class="c1"># dropout forward pass.                                                     #
</span><span class="mi">24</span>         <span class="c1">#                                                                          #
</span><span class="mi">25</span>         <span class="c1"># When using batch normalization, you'll need to pass self.bn_params[0] to #
</span><span class="mi">26</span>         <span class="c1"># the forward pass for the first batch normalization layer, pass           #
</span><span class="mi">27</span>         <span class="c1"># self.bn_params[1] to the forward pass for the second batch normalization #
</span><span class="mi">28</span>         <span class="c1"># layer, etc.                                                           #
</span><span class="mi">29</span>         <span class="c1">############################################################################
</span><span class="mi">30</span>         <span class="n">layer_input</span> <span class="o">=</span> <span class="n">X</span>
<span class="mi">31</span>         <span class="n">ar_cache</span> <span class="o">=</span> <span class="p">{}</span> <span class="c1"># Affine ReLU Cache
</span><span class="mi">32</span>         <span class="n">dp_cache</span> <span class="o">=</span> <span class="p">{}</span> <span class="c1"># DropOut Cache
</span><span class="mi">33</span>         
<span class="mi">34</span>         <span class="k">for</span> <span class="n">lay</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">num_layers</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
<span class="mi">35</span>             <span class="c1">#if using the batch normalization
</span><span class="mi">36</span>             <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">normalization</span><span class="p">:</span>
<span class="mi">37</span>                 <span class="n">layer_input</span><span class="p">,</span> <span class="n">ar_cache</span><span class="p">[</span><span class="n">lay</span><span class="p">]</span> <span class="o">=</span> <span class="nf">affine_bn_relu_forward</span><span class="p">(</span><span class="n">layer_input</span><span class="p">,</span>
<span class="mi">38</span>                                   <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">W%d</span><span class="sh">'</span><span class="o">%</span><span class="p">(</span><span class="n">lay</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span><span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">b%d</span><span class="sh">'</span><span class="o">%</span><span class="p">(</span><span class="n">lay</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span>
<span class="mi">39</span>                                   <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">gamma%d</span><span class="sh">'</span><span class="o">%</span><span class="p">(</span><span class="n">lay</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span><span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">beta%d</span><span class="sh">'</span><span class="o">%</span><span class="p">(</span><span class="n">lay</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span>
<span class="mi">40</span>                                   <span class="n">self</span><span class="p">.</span><span class="n">bn_params</span><span class="p">[</span><span class="n">lay</span><span class="p">])</span>
<span class="mi">41</span>             <span class="k">else</span><span class="p">:</span>
<span class="mi">42</span>                 <span class="n">layer_input</span><span class="p">,</span> <span class="n">ar_cache</span><span class="p">[</span><span class="n">lay</span><span class="p">]</span> <span class="o">=</span> <span class="nf">affine_relu_forward</span><span class="p">(</span><span class="n">layer_input</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">W%d</span><span class="sh">'</span><span class="o">%</span><span class="p">(</span><span class="n">lay</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span><span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">b%d</span><span class="sh">'</span><span class="o">%</span><span class="p">(</span><span class="n">lay</span><span class="o">+</span><span class="mi">1</span><span class="p">)])</span>
<span class="mi">43</span>             <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">use_dropout</span><span class="p">:</span>
<span class="mi">44</span>                 <span class="n">lay_input</span><span class="p">,</span><span class="n">dp_cache</span><span class="p">[</span><span class="n">lay</span><span class="p">]</span> <span class="o">=</span> <span class="nf">dropout_forward</span><span class="p">(</span><span class="n">layer_input</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">dropout_param</span><span class="p">)</span>
<span class="mi">45</span>                 
<span class="mi">46</span>         <span class="c1">#The Last layer 
</span><span class="mi">47</span>         <span class="n">ar_out</span><span class="p">,</span><span class="n">ar_cache</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">]</span> <span class="o">=</span> <span class="nf">affine_forward</span><span class="p">(</span><span class="n">layer_input</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">W%d</span><span class="sh">'</span><span class="o">%</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">)],</span><span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">b%d</span><span class="sh">'</span><span class="o">%</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">)])</span>
<span class="mi">48</span>         <span class="n">scores</span> <span class="o">=</span> <span class="n">ar_out</span>
<span class="mi">49</span>         <span class="c1">#pass
</span><span class="mi">50</span>         <span class="c1">############################################################################
</span><span class="mi">51</span>         <span class="c1">#                             END OF YOUR CODE                             #
</span><span class="mi">52</span>         <span class="c1">############################################################################
</span><span class="mi">53</span> 
<span class="mi">54</span>         <span class="c1"># If test mode return early
</span><span class="mi">55</span>         <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="sh">'</span><span class="s">test</span><span class="sh">'</span><span class="p">:</span>
<span class="mi">56</span>             <span class="k">return</span> <span class="n">scores</span>
<span class="mi">57</span> 
<span class="mi">58</span>         <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="p">{}</span>
<span class="mi">59</span>         <span class="c1">############################################################################
</span><span class="mi">60</span>         <span class="c1"># TODO: Implement the backward pass for the fully-connected net. Store the #
</span><span class="mi">61</span>         <span class="c1"># loss in the loss variable and gradients in the grads dictionary. Compute #
</span><span class="mi">62</span>         <span class="c1"># data loss using softmax, and make sure that grads[k] holds the gradients #
</span><span class="mi">63</span>         <span class="c1"># for self.params[k]. Don't forget to add L2 regularization!               #
</span><span class="mi">64</span>         <span class="c1">#                                                                          #
</span><span class="mi">65</span>         <span class="c1"># When using batch/layer normalization, you don't need to regularize the scale   #
</span><span class="mi">66</span>         <span class="c1"># and shift parameters.                                                    #
</span><span class="mi">67</span>         <span class="c1">#                                                                          #
</span><span class="mi">68</span>         <span class="c1"># NOTE: To ensure that your implementation matches ours and you pass the   #
</span><span class="mi">69</span>         <span class="c1"># automated tests, make sure that your L2 regularization includes a factor #
</span><span class="mi">70</span>         <span class="c1"># of 0.5 to simplify the expression for the gradient.                      #
</span><span class="mi">71</span>         <span class="c1">############################################################################
</span><span class="mi">72</span>         <span class="c1">#first calculate the loss using softmax_loss
</span><span class="mi">73</span>         <span class="n">loss</span><span class="p">,</span><span class="n">dscores</span> <span class="o">=</span> <span class="nf">softmax_loss</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="mi">74</span>         <span class="n">dhout</span> <span class="o">=</span> <span class="n">dscores</span> <span class="c1"># upstream derivatives
</span><span class="mi">75</span>         <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">reg</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">W%d</span><span class="sh">'</span><span class="o">%</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">)]</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">W%d</span><span class="sh">'</span><span class="o">%</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">)])</span>
<span class="mi">76</span>         <span class="c1"># From the last layer to the input layer
</span><span class="mi">77</span>         <span class="n">dx</span><span class="p">,</span><span class="n">dw</span><span class="p">,</span><span class="n">db</span> <span class="o">=</span> <span class="nf">affine_backward</span><span class="p">(</span><span class="n">dhout</span><span class="p">,</span><span class="n">ar_cache</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">])</span>
<span class="mi">78</span>         <span class="n">grads</span><span class="p">[</span><span class="sh">'</span><span class="s">W%d</span><span class="sh">'</span><span class="o">%</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">)]</span> <span class="o">=</span> <span class="n">dw</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">reg</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">W%d</span><span class="sh">'</span><span class="o">%</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">)]</span>
<span class="mi">79</span>         <span class="n">grads</span><span class="p">[</span><span class="sh">'</span><span class="s">b%d</span><span class="sh">'</span><span class="o">%</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">)]</span> <span class="o">=</span> <span class="n">db</span>
<span class="mi">80</span>         <span class="n">dhout</span> <span class="o">=</span> <span class="n">dx</span> <span class="c1">#upstream gradient
</span><span class="mi">81</span>         <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
<span class="mi">82</span>             <span class="n">lay</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">idx</span> <span class="o">-</span> <span class="mi">1</span>
<span class="mi">83</span>             <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span>  <span class="n">self</span><span class="p">.</span><span class="n">reg</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">W%d</span><span class="sh">'</span><span class="o">%</span><span class="p">(</span><span class="n">lay</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">W%d</span><span class="sh">'</span><span class="o">%</span><span class="p">(</span><span class="n">lay</span><span class="o">+</span><span class="mi">1</span><span class="p">)])</span>
<span class="mi">84</span>             <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">use_dropout</span><span class="p">:</span>
<span class="mi">85</span>                 <span class="n">dhout</span> <span class="o">=</span> <span class="nf">dropout_backward</span><span class="p">(</span><span class="n">dhout</span><span class="p">,</span><span class="n">dp_cache</span><span class="p">[</span><span class="n">lay</span><span class="p">])</span>
<span class="mi">86</span>             <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">normalization</span><span class="p">:</span>
<span class="mi">87</span>                 <span class="n">dx</span><span class="p">,</span> <span class="n">dw</span><span class="p">,</span> <span class="n">db</span><span class="p">,</span> <span class="n">dgamma</span><span class="p">,</span> <span class="n">dbeta</span> <span class="o">=</span> <span class="nf">affine_bn_relu_backward</span><span class="p">(</span><span class="n">dhout</span><span class="p">,</span><span class="n">ar_cache</span><span class="p">[</span><span class="n">lay</span><span class="p">])</span>
<span class="mi">88</span>             <span class="k">else</span><span class="p">:</span>
<span class="mi">89</span>                 <span class="n">dx</span><span class="p">,</span> <span class="n">dw</span><span class="p">,</span> <span class="n">db</span> <span class="o">=</span> <span class="nf">affine_relu_backward</span><span class="p">(</span><span class="n">dhout</span><span class="p">,</span><span class="n">ar_cache</span><span class="p">[</span><span class="n">lay</span><span class="p">])</span>
<span class="mi">90</span>             <span class="n">grads</span><span class="p">[</span><span class="sh">'</span><span class="s">W%d</span><span class="sh">'</span><span class="o">%</span><span class="p">(</span><span class="n">lay</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">dw</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">reg</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">W%d</span><span class="sh">'</span><span class="o">%</span><span class="p">(</span><span class="n">lay</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span>
<span class="mi">91</span>             <span class="n">grads</span><span class="p">[</span><span class="sh">'</span><span class="s">b%d</span><span class="sh">'</span><span class="o">%</span><span class="p">(</span><span class="n">lay</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">db</span>
<span class="mi">92</span>             <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">normalization</span><span class="p">:</span>
<span class="mi">93</span>                <span class="n">grads</span><span class="p">[</span><span class="sh">'</span><span class="s">gamma%d</span><span class="sh">'</span><span class="o">%</span><span class="p">(</span><span class="n">lay</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">dgamma</span>
<span class="mi">94</span>                <span class="n">grads</span><span class="p">[</span><span class="sh">'</span><span class="s">beta%d</span><span class="sh">'</span><span class="o">%</span><span class="p">(</span><span class="n">lay</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">dbeta</span>
<span class="mi">95</span>             <span class="n">dhout</span> <span class="o">=</span> <span class="n">dx</span>
<span class="mi">96</span>         <span class="c1">#pass
</span><span class="mi">97</span>         <span class="c1">############################################################################
</span><span class="mi">98</span>         <span class="c1">#                             END OF YOUR CODE                             #
</span><span class="mi">99</span>         <span class="c1">############################################################################
</span><span class="mi">100</span> 
<span class="mi">101</span>         <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span>
</code></pre></div></div>

    </div>
  </article>

  

  

  
    
      

  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2>
    <p class="mb-2">Here are some more articles you might like to read next:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/displaying-external-posts-on-your-al-folio-blog/">Displaying External Posts on Your al-folio Blog</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/small-guide-to-supplements-what-you-need-to-know/">A small guide to supplements: What you need to know</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/%E6%B7%B7%E4%B9%B1%E4%B8%8E%E7%A7%A9%E5%BA%8F/">混乱与秩序</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/podcast-notes-huberman-lab-dopomine/">Podcast Notes: Huberman Lab Dopomine</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/erc-721/">ERC 721</a>
  </li>


    
  

  
  
    <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;">
  
    <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'zhengstar94/zhengstar94.github.io',
        'data-repo-id': 'R_kgDOMKQWcA',
        'data-category': 'Comments',
        'data-category-id': 'DIC_kwDOMKQWcM4CgKT_',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a>
</noscript>
  
</div>

  
</div>
</div>
          </div>
        
      
    </div>

    <!-- Footer -->
    
  <footer class="fixed-bottom" role="contentinfo">
    <div class="container mt-0">
      © Copyright 2024
      Xingxing
      
      Zheng. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      
      
    </div>
  </footer>



    <!-- JavaScripts -->
    <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
<script src="/assets/js/bootstrap.bundle.min.js"></script>
<!-- <script src="/assets/js/mdb.min.js"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
  <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>


    

    

    

    

    

    

    

    

    

  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script>


  <!-- Sidebar Table of Contents -->
  <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script>


<!-- Bootstrap Table -->


<!-- Load Common JS -->
<script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script>
<script defer src="/assets/js/common.js?06cae41083477f121be8cd9797ad8e2f"></script>
<script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script>

<!-- Jupyter Open External Links New Tab -->
<script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script>



    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>


  <script async src="https://badge.dimensions.ai/badge.js"></script>


    
  
    <!-- MathJax -->
    <script type="text/javascript">
      window.MathJax = {
        tex: {
          tags: 'ams',
        },
      };
    </script>
    <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script>
    <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script>
  


    

    


    
  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    /*
     * This JavaScript code has been adapted from the article
     * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar,
     * published on the website https://css-tricks.com on the 7th of May, 2014.
     * Couple of changes were made to the original code to make it compatible
     * with the `al-foio` theme.
     */
    const progressBar = $('#progress');
    /*
     * We set up the bar after all elements are done loading.
     * In some cases, if the images in the page are larger than the intended
     * size they'll have on the page, they'll be resized via CSS to accomodate
     * the desired size. This mistake, however, breaks the computations as the
     * scroll size is computed as soon as the elements finish loading.
     * To account for this, a minimal delay was introduced before computing the
     * values.
     */
    window.onload = function () {
      setTimeout(progressBarSetup, 50);
    };
    /*
     * We set up the bar according to the browser.
     * If the browser supports the progress element we use that.
     * Otherwise, we resize the bar thru CSS styling
     */
    function progressBarSetup() {
      if ('max' in document.createElement('progress')) {
        initializeProgressElement();
        $(document).on('scroll', function () {
          progressBar.attr({ value: getCurrentScrollPosition() });
        });
        $(window).on('resize', initializeProgressElement);
      } else {
        resizeProgressBar();
        $(document).on('scroll', resizeProgressBar);
        $(window).on('resize', resizeProgressBar);
      }
    }
    /*
     * The vertical scroll position is the same as the number of pixels that
     * are hidden from view above the scrollable area. Thus, a value > 0 is
     * how much the user has scrolled from the top
     */
    function getCurrentScrollPosition() {
      return $(window).scrollTop();
    }

    function initializeProgressElement() {
      let navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      progressBar.css({ top: navbarHeight });
      progressBar.attr({
        max: getDistanceToScroll(),
        value: getCurrentScrollPosition(),
      });
    }
    /*
     * The offset between the html document height and the browser viewport
     * height will be greater than zero if vertical scroll is possible.
     * This is the distance the user can scroll
     */
    function getDistanceToScroll() {
      return $(document).height() - $(window).height();
    }

    function resizeProgressBar() {
      progressBar.css({ width: getWidthPercentage() + '%' });
    }
    // The scroll ratio equals the percentage to resize the bar
    function getWidthPercentage() {
      return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
    }
  </script>


    

    

    

    
  <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script>
  <script>
    addBackToTop();
  </script>


    
  <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script>
  <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys>
  <script>
    let searchTheme = determineComputedTheme();
    const ninjaKeys = document.querySelector('ninja-keys');

    if (searchTheme === 'dark') {
      ninjaKeys.classList.add('dark');
    } else {
      ninjaKeys.classList.remove('dark');
    }

    const openSearchModal = () => {
      // collapse navbarNav if expanded on mobile
      const $navbarNav = $('#navbarNav');
      if ($navbarNav.hasClass('show')) {
        $navbarNav.collapse('hide');
      }
      ninjaKeys.open();
    };
  </script>
  <script>
    // get the ninja-keys element
    const ninja = document.querySelector('ninja-keys');

    // add the home and posts menu items
    ninja.data = [{
        id: "nav-about",
        title: "about",
        section: "Navigation",
        handler: () => {
          window.location.href = "/";
        },
      },{id: "nav-blog",
              title: "blog",
              description: "",
              section: "Navigation",
              handler: () => {
                window.location.href = "/blog/";
              },
            },{id: "nav-repositories",
              title: "repositories",
              description: "",
              section: "Navigation",
              handler: () => {
                window.location.href = "/repositories/";
              },
            },{id: "nav-cv",
              title: "CV",
              description: "Personal CV, updated on 18 Jun, 2024",
              section: "Navigation",
              handler: () => {
                window.location.href = "/cv/";
              },
            },{id: "dropdown-by-year",
                  title: "by year",
                  description: "",
                  section: "Dropdown",
                  handler: () => {
                    window.location.href = "";
                  },
                },{id: "dropdown-by-category",
                  title: "by category",
                  description: "",
                  section: "Dropdown",
                  handler: () => {
                    window.location.href = "";
                  },
                },{id: "post-a-small-guide-to-supplements-what-you-need-to-know",
          
            title: "A small guide to supplements: What you need to know",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2023/small-guide-to-supplements-what-you-need-to-know/";
            
          },
        },{id: "post-混乱与秩序",
          
            title: "混乱与秩序",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2022/%E6%B7%B7%E4%B9%B1%E4%B8%8E%E7%A7%A9%E5%BA%8F/";
            
          },
        },{id: "post-podcast-notes-huberman-lab-dopomine",
          
            title: "Podcast Notes: Huberman Lab Dopomine",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2022/podcast-notes-huberman-lab-dopomine/";
            
          },
        },{id: "post-displaying-external-posts-on-your-al-folio-blog",
          
            // TODO: fix the svg icon position for external posts
            // title: 'Displaying External Posts on Your al-folio Blog <svg width="2rem" height="2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',
            title: "Displaying External Posts on Your al-folio Blog",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.open("https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2", "_blank");
            
          },
        },{id: "post-erc-721",
          
            title: "ERC 721",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2022/erc-721/";
            
          },
        },{id: "post-cost-functions-and-its-properties-in-deep-learning-tbc",
          
            title: "Cost Functions and its properties in Deep Learning (TBC)",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2022/cost-functions-and-its-properties-in-deep-learning-tbc/";
            
          },
        },{id: "post-cv-modifying",
          
            title: "CV: Modifying",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2022/cv-modifying/";
            
          },
        },{id: "post-final",
          
            title: "Final",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/Final/";
            
          },
        },{id: "post-design-pattern",
          
            title: "Design Pattern",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/Design-Pattern/";
            
          },
        },{id: "post-distributed",
          
            title: "Distributed",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/Distributed/";
            
          },
        },{id: "post-mq",
          
            title: "MQ",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/MQ/";
            
          },
        },{id: "post-spring",
          
            title: "Spring",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/Spring/";
            
          },
        },{id: "post-computer",
          
            title: "Computer",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/Computer/";
            
          },
        },{id: "post-redis",
          
            title: "Redis",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/Redis/";
            
          },
        },{id: "post-jvm",
          
            title: "Jvm",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/JVM/";
            
          },
        },{id: "post-thread",
          
            title: "Thread",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/Thread/";
            
          },
        },{id: "post-collection",
          
            title: "Collection",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/Collection/";
            
          },
        },{id: "post-base",
          
            title: "Base",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/Base/";
            
          },
        },{id: "post-mysql",
          
            title: "MYSQL",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/MYSQL/";
            
          },
        },{id: "post-interview",
          
            title: "Interview",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/Interview/";
            
          },
        },{id: "post-分库分表做到永不迁移数据和避免热点",
          
            title: "分库分表做到永不迁移数据和避免热点",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%81%9A%E5%88%B0%E6%B0%B8%E4%B8%8D%E8%BF%81%E7%A7%BB%E6%95%B0%E6%8D%AE%E5%92%8C%E9%81%BF%E5%85%8D%E7%83%AD%E7%82%B9/";
            
          },
        },{id: "post-分库分表实战",
          
            title: "分库分表实战",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%AE%9E%E6%88%98/";
            
          },
        },{id: "post-分库分表",
          
            title: "分库分表",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/%E5%88%86%E8%A1%A8%E5%88%86%E5%BA%93/";
            
          },
        },{id: "post-mysql的主从复制",
          
            title: "MYSQL的主从复制",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/MYSQL%E7%9A%84%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/";
            
          },
        },{id: "post-mvcc多版本并发控制",
          
            title: "MVCC多版本并发控制",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/MVCC%E5%A4%9A%E7%89%88%E6%9C%AC%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6/";
            
          },
        },{id: "post-mysql优化建议",
          
            title: "MySQL优化建议",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/MySQL%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%AE/";
            
          },
        },{id: "post-mysql查出重复的记录",
          
            title: "MySQL查出重复的记录",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/MySQL%E6%9F%A5%E5%87%BA%E9%87%8D%E5%A4%8D%E7%9A%84%E8%AE%B0%E5%BD%95/";
            
          },
        },{id: "post-mysql的七种join",
          
            title: "MySQL的七种JOIN",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/MySQL%E7%9A%84%E4%B8%83%E7%A7%8DJOIN/";
            
          },
        },{id: "post-mysql如何被执行的",
          
            title: "MySQL如何被执行的",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/MySQL%E5%A6%82%E4%BD%95%E8%A2%AB%E6%89%A7%E8%A1%8C%E7%9A%84/";
            
          },
        },{id: "post-mysql常见问题",
          
            title: "MYSQL常见问题",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/MYSQL%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/";
            
          },
        },{id: "post-mysql",
          
            title: "MYSQL",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/MYSQL/";
            
          },
        },{id: "post-concurrenthashmap",
          
            title: "ConcurrentHashMap",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/ConcurrentHashMap/";
            
          },
        },{id: "post-hashmap死循环原因",
          
            title: "HashMap死循环原因",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/HashMap%E6%AD%BB%E5%BE%AA%E7%8E%AF%E5%8E%9F%E5%9B%A0/";
            
          },
        },{id: "post-hashmap",
          
            title: "HashMap",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/HashMap/";
            
          },
        },{id: "post-arraylist与linkedlist",
          
            title: "ArrayList与LinkedList",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/ArrayList%E4%B8%8ELinkedList/";
            
          },
        },{id: "post-java8stream流递归树遍历",
          
            title: "Java8Stream流递归树遍历",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/Java8Stream%E6%B5%81%E9%80%92%E5%BD%92%E6%A0%91%E9%81%8D%E5%8E%86/";
            
          },
        },{id: "post-序列化与反序列化",
          
            title: "序列化与反序列化",
          
          description: "an example of a blog post with some code",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/%E5%BA%8F%E5%88%97%E5%8C%96%E4%B8%8E%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96/";
            
          },
        },{id: "post-注解与反射",
          
            title: "注解与反射",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/%E6%B3%A8%E8%A7%A3%E4%B8%8E%E5%8F%8D%E5%B0%84/";
            
          },
        },{id: "post-别虽然但是了-2020过去了",
          
            title: "别虽然但是了，2020过去了．",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2020/%E5%88%AB%E8%99%BD%E7%84%B6%E4%BD%86%E6%98%AF%E4%BA%86-2020%E8%BF%87%E5%8E%BB%E4%BA%86/";
            
          },
        },{id: "post-sql规范与技巧-持续更新",
          
            title: "SQL规范与技巧(持续更新)",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2020/sql%E8%A7%84%E8%8C%83%E4%B8%8E%E6%8A%80%E5%B7%A7/";
            
          },
        },{id: "post-金融读书笔记-3",
          
            title: "金融读书笔记(3)",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2020/%E9%87%91%E8%9E%8D%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B03/";
            
          },
        },{id: "post-金融读书笔记-2",
          
            title: "金融读书笔记(2)",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2020/%E9%87%91%E8%9E%8D%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B02/";
            
          },
        },{id: "post-金融读书笔记-1",
          
            title: "金融读书笔记(1)",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2020/%E9%87%91%E8%9E%8D%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B01/";
            
          },
        },{id: "post-gpt-3是什么",
          
            title: "GPT-3是什么?",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2020/gpt-3/";
            
          },
        },{id: "post-前脑白质切除术",
          
            title: "前脑白质切除术",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2020/%E5%89%8D%E8%84%91%E7%99%BD%E8%B4%A8%E5%88%87%E9%99%A4%E6%9C%AF/";
            
          },
        },{id: "post-personal-github",
          
            title: "Personal Github",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2020/person-github/";
            
          },
        },{id: "post-永别了呀2019",
          
            title: "永别了呀2019",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2019/%E6%B0%B8%E5%88%AB%E4%BA%86%E5%91%802019/";
            
          },
        },{id: "post-emacs-入门-占坑",
          
            title: "Emacs 入门(占坑)",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2019/emacs-%E5%85%A5%E9%97%A8%E5%8D%A0%E5%9D%91/";
            
          },
        },{id: "post-我在备忘录都写些什么",
          
            title: "我在备忘录都写些什么？",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2019/%E6%8C%96%E5%9D%91%E8%BF%87%E5%8E%BB%E7%9A%84%E4%B8%80%E5%B9%B4/";
            
          },
        },{id: "post-adam-vs-radam",
          
            title: "Adam vs. RAdam",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2019/%E5%8D%A0%E5%9D%91-adam-vs-radam/";
            
          },
        },{id: "post-硕士开学第一学期-关于琐事和学习及方法",
          
            title: "硕士开学第一学期——关于琐事和学习及方法",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2018/%E7%A1%95%E5%A3%AB%E5%BC%80%E5%AD%A6%E7%AC%AC%E4%B8%80%E5%AD%A6%E6%9C%9F-%E5%85%B3%E4%BA%8E%E7%90%90%E4%BA%8B%E5%92%8C%E5%AD%A6%E4%B9%A0%E5%8F%8A%E6%96%B9%E6%B3%95/";
            
          },
        },{id: "post-emotion-recognition-based-on-tensorflow",
          
            title: "Emotion Recognition Based on Tensorflow",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2018/emotion-recognition-based-on-tensorflow/";
            
          },
        },{id: "post-python-review-amp-amp-some-simple-algorithms",
          
            title: "Python_review &amp; Some simple algorithms",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2018/python_review-some-simple-algorithms/";
            
          },
        },{id: "post-nlp-word2vec-skip-gram-cs224n-implemented-in-raw-way-and-in-tensorflow",
          
            title: "NLP: Word2Vec Skip-Gram(CS224n) implemented in raw way and in Tensorflow",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2018/nlp-word2vec-skip-gramcs224n/";
            
          },
        },{id: "post-cs231n-assignment3-image-captioning-with-rnns",
          
            title: "CS231n Assignment3_Image Captioning with RNNs",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2018/cs231n-assignment3_image-captioning-with-rnns/";
            
          },
        },{id: "post-pytorch-vs-tensorflow",
          
            title: "Pytorch VS Tensorflow",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2018/pytorch-vs-tensorflow/";
            
          },
        },{id: "post-cs231n-cnn-notes-amp-amp-assignment2",
          
            title: "CS231N_CNN_Notes&amp;Assignment2",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2018/cs231n_cnn_notesassignment2/";
            
          },
        },{id: "post-web-crawler爬top100电影信息",
          
            title: "Web_Crawler爬top100电影信息",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2018/web_crawler%E7%88%ACtop100%E7%94%B5%E5%BD%B1%E4%BF%A1%E6%81%AF/";
            
          },
        },{id: "post-牛客算法直播题目总结day2t1234",
          
            title: "牛客算法直播题目总结Day2T1234",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2017/%E7%89%9B%E5%AE%A2%E7%AE%97%E6%B3%95%E7%9B%B4%E6%92%AD%E9%A2%98%E7%9B%AE%E6%80%BB%E7%BB%93day2t1234/";
            
          },
        },{id: "post-牛客算法直播题目总结day1t234",
          
            title: "牛客算法直播题目总结Day1T234",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2017/%E7%89%9B%E5%AE%A2%E7%AE%97%E6%B3%95%E7%9B%B4%E6%92%AD%E9%A2%98%E7%9B%AE%E6%80%BB%E7%BB%93day1t234%E9%9B%B6%E5%92%8C%E5%8D%9A%E5%BC%88/";
            
          },
        },{id: "post-牛客算法直播题目总结day1t1-零和博弈",
          
            title: "牛客算法直播题目总结Day1T1(零和博弈)",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2017/%E7%89%9B%E5%AE%A2%E7%AE%97%E6%B3%95%E7%9B%B4%E6%92%AD%E9%A2%98%E7%9B%AE%E6%80%BB%E7%BB%93/";
            
          },
        },{id: "post-java学习笔记-六-装饰-观察者模式及监听器",
          
            title: "Java学习笔记(六):装饰、观察者模式及监听器",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2017/java%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E5%85%AD%E8%A3%85%E9%A5%B0-%E8%A7%82%E5%AF%9F%E8%80%85%E6%A8%A1%E5%BC%8F%E5%8F%8A%E7%9B%91%E5%90%AC%E5%99%A8/";
            
          },
        },{id: "post-java学习笔记-四-filter",
          
            title: "Java学习笔记(四):Filter",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2017/java%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E5%9B%9Bfilter/";
            
          },
        },{id: "post-java学习笔记-五-文件的上传和下载",
          
            title: "Java学习笔记(五):文件的上传和下载",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2017/java%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%BA%94%E6%96%87%E4%BB%B6%E7%9A%84%E4%B8%8A%E4%BC%A0%E5%92%8C%E4%B8%8B%E8%BD%BD/";
            
          },
        },{id: "post-jdbc学习笔记-三",
          
            title: "JDBC学习笔记(三)",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2017/jdbc%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%89/";
            
          },
        },{id: "post-jdbc学习笔记-二",
          
            title: "JDBC学习笔记(二)",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2017/jdbc%E7%AC%94%E8%AE%B0%E4%BA%8C/";
            
          },
        },{id: "post-jdbc学习笔记-一",
          
            title: "JDBC学习笔记(一)",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2017/jdbc%E7%AC%94%E8%AE%B0%E5%8F%8A%E5%85%B6%E4%BB%96/";
            
          },
        },{id: "post-j2ee-cookie与session知识点回顾",
          
            title: "J2EE_Cookie与Session知识点回顾",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2017/j2ee_cookie%E4%B8%8Esession%E7%9F%A5%E8%AF%86%E7%82%B9%E5%9B%9E%E9%A1%BE/";
            
          },
        },{id: "post-bug-解决-nginx-502-bad-gateway",
          
            title: "Bug：解决 nginx 502 Bad GateWay",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2017/bug-%E8%A7%A3%E5%86%B3-nginx-502-bad-gateway/";
            
          },
        },{id: "post-windows-laravel环境部署",
          
            title: "Windows Laravel环境部署",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2016/windows-laravel%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/";
            
          },
        },{id: "post-git基础记录",
          
            title: "Git基础记录",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2016/git%E5%9F%BA%E7%A1%80%E8%AE%B0%E5%BD%95/";
            
          },
        },{id: "post-算法导论读书笔记-6",
          
            title: "算法导论读书笔记(6)",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2016/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B06/";
            
          },
        },{id: "post-qt实现电子词典gui-project",
          
            title: "QT实现电子词典GUI（project）",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2016/qt%E5%AE%9E%E7%8E%B0%E7%94%B5%E5%AD%90%E8%AF%8D%E5%85%B8gui-project/";
            
          },
        },{id: "post-算法导论读书笔记-5",
          
            title: "算法导论读书笔记(5)",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2016/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B05/";
            
          },
        },{id: "post-算法导论读书笔记-4",
          
            title: "算法导论读书笔记(4)",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2016/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B04/";
            
          },
        },{id: "post-算法导论读书笔记-3",
          
            title: "算法导论读书笔记(3)",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2016/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B03/";
            
          },
        },{id: "post-算法导论读书笔记-2",
          
            title: "算法导论读书笔记(2)",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2016/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B02/";
            
          },
        },{id: "post-aws免费服务器申请和中继器搭建",
          
            title: "AWS免费服务器申请和中继器搭建",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2016/aws%E5%85%8D%E8%B4%B9%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%94%B3%E8%AF%B7%E5%92%8C%E4%B8%AD%E7%BB%A7%E5%99%A8%E6%90%AD%E5%BB%BA/";
            
          },
        },{id: "post-算法导论读书笔记-1",
          
            title: "算法导论读书笔记（1）",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2016/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-1/";
            
          },
        },{id: "post-数据结构之-排序",
          
            title: "数据结构之——排序",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2015/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B-%E6%8E%92%E5%BA%8F/";
            
          },
        },{id: "post-数据结构之-图",
          
            title: "数据结构之——图",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2015/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B-%E5%9B%BE/";
            
          },
        },{id: "post-stl之vector用法",
          
            title: "STL之Vector用法",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2015/stl%E4%B9%8Bvector%E7%94%A8%E6%B3%95/";
            
          },
        },{id: "post-数据结构之-树",
          
            title: "数据结构之——树",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2015/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B-%E6%A0%91/";
            
          },
        },{id: "post-回溯法经典应用-n皇后-amp-amp-迷宫问题",
          
            title: "回溯法经典应用——N皇后&amp;迷宫问题",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2015/%E5%9B%9E%E6%BA%AF%E6%B3%95%E7%BB%8F%E5%85%B8%E5%BA%94%E7%94%A8-n%E7%9A%87%E5%90%8E%E8%BF%B7%E5%AE%AB%E9%97%AE%E9%A2%98/";
            
          },
        },{id: "post-数据结构之-队列",
          
            title: "数据结构之——队列",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2015/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B-%E9%98%9F%E5%88%97/";
            
          },
        },{id: "post-数据结构之-栈",
          
            title: "数据结构之——栈",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2015/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B-%E6%A0%88/";
            
          },
        },{id: "post-大二第一学期计划-备表及其他",
          
            title: "大二第一学期计划/备表及其他",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2015/%E5%A4%A7%E4%BA%8C%E7%AC%AC%E4%B8%80%E5%AD%A6%E6%9C%9F%E8%AE%A1%E5%88%92%E5%A4%87%E8%A1%A8%E5%8F%8A%E5%85%B6%E4%BB%96/";
            
          },
        },{id: "post-利用靓汤beautifulsoup4写一个简易爬虫",
          
            title: "利用靓汤BeautifulSoup4写一个简易爬虫",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2015/%E5%88%A9%E7%94%A8%E9%9D%93%E6%B1%A4beautifulsoup4%E5%86%99%E4%B8%80%E4%B8%AA%E7%AE%80%E6%98%93%E7%88%AC%E8%99%AB/";
            
          },
        },{id: "post-数据结构之-链表",
          
            title: "数据结构之——链表",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2015/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B-%E9%93%BE%E8%A1%A8/";
            
          },
        },{id: "post-数据结构之-向量",
          
            title: "数据结构之——向量",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2015/vector/";
            
          },
        },{id: "news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",
          title: 'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',
          description: "",
          section: "News",},{id: "news-a-long-announcement-with-details",
          title: 'A long announcement with details',
          description: "",
          section: "News",handler: () => {
              window.location.href = "/news/announcement_2/";
            },},{id: "news-a-simple-inline-announcement",
          title: 'A simple inline announcement.',
          description: "",
          section: "News",},{id: "projects-project-1",
              title: 'project 1',
              description: "with background image",
              section: "Projects",handler: () => {
                  window.location.href = "/projects/1_project/";
                },},{id: "projects-project-2",
              title: 'project 2',
              description: "a project with a background image and giscus comments",
              section: "Projects",handler: () => {
                  window.location.href = "/projects/2_project/";
                },},{id: "projects-project-3-with-very-long-name",
              title: 'project 3 with very long name',
              description: "a project that redirects to another website",
              section: "Projects",handler: () => {
                  window.location.href = "/projects/3_project/";
                },},{id: "projects-project-4",
              title: 'project 4',
              description: "another without an image",
              section: "Projects",handler: () => {
                  window.location.href = "/projects/4_project/";
                },},{id: "projects-project-5",
              title: 'project 5',
              description: "a project with a background image",
              section: "Projects",handler: () => {
                  window.location.href = "/projects/5_project/";
                },},{id: "projects-project-6",
              title: 'project 6',
              description: "a project with no image",
              section: "Projects",handler: () => {
                  window.location.href = "/projects/6_project/";
                },},{id: "projects-project-7",
              title: 'project 7',
              description: "with background image",
              section: "Projects",handler: () => {
                  window.location.href = "/projects/7_project/";
                },},{id: "projects-project-8",
              title: 'project 8',
              description: "an other project with a background image and giscus comments",
              section: "Projects",handler: () => {
                  window.location.href = "/projects/8_project/";
                },},{id: "projects-project-9",
              title: 'project 9',
              description: "another project with an image 🎉",
              section: "Projects",handler: () => {
                  window.location.href = "/projects/9_project/";
                },},{
            id: 'socials-email',
            title: 'Send email',
            section: 'Socials',
            handler: () => {
              window.open("mailto:%7A%68%65%6E%67%73%74%61%72%73@%67%6D%61%69%6C.%63%6F%6D", "_blank");
            },
          },{
            id: 'socials-github',
            title: 'GitHub',
            section: 'Socials',
            handler: () => {
              window.open("https://github.com/zhengstar94", "_blank");
            },
          },{
            id: 'socials-linkedin',
            title: 'LinkedIn',
            section: 'Socials',
            handler: () => {
              window.open("https://www.linkedin.com/in/zhengxingxing", "_blank");
            },
          },{
            id: 'socials-rss',
            title: 'RSS Feed',
            section: 'Socials',
            handler: () => {
              window.open("/feed.xml", "_blank");
            },
          },{
          id: 'light-theme',
          title: 'Change theme to light',
          description: 'Change the theme of the site to Light',
          section: 'Theme',
          handler: () => {
            setThemeSetting("light");
          },
        },
        {
          id: 'dark-theme',
          title: 'Change theme to dark',
          description: 'Change the theme of the site to Dark',
          section: 'Theme',
          handler: () => {
            setThemeSetting("dark");
          },
        },
        {
          id: 'system-theme',
          title: 'Use system default theme',
          description: 'Change the theme of the site to System Default',
          section: 'Theme',
          handler: () => {
            setThemeSetting("system");
          },
        },];
  </script>


    <script src="/assets/js/shortcut-key.js"></script>
  </body>
</html>

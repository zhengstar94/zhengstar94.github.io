<!DOCTYPE html>
<html lang="en">
  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <!-- Metadata, OpenGraph and Schema.org -->




<!-- Standard metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>
  
  
    
      NLP: Word2Vec Skip-Gram(CS224n) implemented in raw way and in Tensorflow | Xingxing Zheng
    
  
</title>
<meta name="author" content="Xingxing Zheng">
<meta name="description" content="Welcome to Xingxing's blog, where I share my thoughts and experiences on various topics.
">










<!-- Bootstrap & MDB -->
<link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

<!-- Bootstrap Table -->


<!-- Fonts & Icons -->
<link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5">
<link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap">

<!-- Code Syntax Highlighting -->
<link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light">


  <!-- Sidebar Table of Contents -->
  <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet">


<!-- Styles -->

<!-- pseudocode -->



  <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">

<link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
<link rel="canonical" href="http://0.0.0.0:8080/blog/2018/nlp-word2vec-skip-gramcs224n/">

<!-- Dark Mode -->
<script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script>

  <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark">
  <script>
    initTheme();
  </script>


<!-- GeoJSON support via Leaflet -->


<!-- diff2html -->






  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">
    <!-- Header -->
    <header>
  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation">
    <div class="container">
      
        <a class="navbar-brand title font-weight-lighter" href="/">
          
            
              <span class="font-weight-bold">Xingxing</span>
            
            
            Zheng
          
        </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">about
              
            </a>
          </li>

          <!-- Other pages -->
          
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
              
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/blog/">blog
                    
                  </a>
                </li>
              
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
              
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/repositories/">repositories
                    
                  </a>
                </li>
              
            
          
            
              
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/cv/">CV
                    
                  </a>
                </li>
              
            
          
            
              
                
                
                <li class="nav-item dropdown ">
                  <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">archive
                    
                  </a>
                  <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
                    
                      
                        <a class="dropdown-item " href="/archive_by_year">by year</a>
                      
                    
                      
                        <a class="dropdown-item " href="/archive_by_tag">by category</a>
                      
                    
                  </div>
                </li>
              
            
          
          
            <!-- Search -->
            <li class="nav-item">
              <button id="search-toggle" title="Search" onclick="openSearchModal()">
                <span class="nav-link">ctrl k <i class="ti ti-search"></i></span>
              </button>
            </li>
          
          
            <!-- Toogle theme mode -->
            <li class="toggle-container">
              <button id="light-toggle" title="Change theme">
                <i class="ti ti-sun-moon" id="light-toggle-system"></i>
                <i class="ti ti-moon-filled" id="light-toggle-dark"></i>
                <i class="ti ti-sun-filled" id="light-toggle-light"></i>
              </button>
            </li>
          
        </ul>
      </div>
    </div>
  </nav>
  
    <!-- Scrolling Progress Bar -->
    <progress id="progress" value="0">
      <div class="progress-container">
        <span class="progress-bar"></span>
      </div>
    </progress>
  
</header>


    <!-- Content -->
    <div class="container mt-5" role="main">
      
        
          <div class="row">
            <!-- sidebar, which will move to the top on a small screen -->
            <div class="col-sm-3">
              <nav id="toc-sidebar" class="sticky-top"></nav>
            </div>
            <!-- main content area -->
            <div class="col-sm-9">







<div class="post">
  <header class="post-header">
    <h1 class="post-title">NLP: Word2Vec Skip-Gram(CS224n) implemented in raw way and in Tensorflow</h1>
    <p class="post-meta">
      Created in August 17, 2018
      
      
      
    </p>
    <p class="post-tags">
      
        <a href="/blog/2018"> <i class="fa-solid fa-calendar fa-sm"></i> 2018 </a>
      
      

      
          ·  
        
          
            <a href="/blog/category/dl-ml-python">
              <i class="fa-solid fa-tag fa-sm"></i> dl-ml-python</a>
          
          
        
      
    </p>
  </header>

  <article class="post-content">
    
    <div id="markdown-content">
      <p>The following contents and images are from cs224n and <a href="http://www.hankcs.com/nlp/cs224n-assignment-1.html/2" rel="external nofollow noopener" target="_blank">hankcs</a>.</p>

<hr>

<p>Before getting into the word2vec part, let’s talk about how do you understand a sentence or say when you’re reading, how do you figure out the meaning of the whole bunch of words, the meaning and the specific image of the words right? So, when it comes to Computer Science, how do you teach computer to know what’s the “meaning” of a word, or a sentence? In the last couple of decades, scientists were using classifying dictionary like wordnet, but it takes massive amount of time for people to put words in order, and it cannot tackle the problem of word similarity.</p>

<p>Then a linguist called J. R. Firth came up with an idea that a word can be understood through its context, it’s the basic idea of NLP statistics. it is also called distributed representations.</p>

<p><img src="https://zhengliangliang.files.wordpress.com/2018/08/006Fmjmcly1fgcgiwa1j4j317w08mtaj.jpg" alt="hankcs.com 2017-06-07 上午11.04.07.png" title="hankcs.com 2017-06-07 上午11.04.07.png"></p>

<p>So the word2vec means we’re using “center words” and its context to predict each other. In cs224n there are 2 algorithms:</p>

<ul>
  <li>Skip-grams: using center words to predict its context</li>
  <li>CBOW(Continuous Bag of Words): using context to predict center words</li>
</ul>

<p>Anther algorithm will be more efficient called <strong>Negative Sampling</strong>.</p>

<p><strong>Skip-gram:</strong></p>

<p><img src="https://zhengliangliang.files.wordpress.com/2018/08/006Fmjmcly1fgcmzglo19j31ay0n41kx.jpg" alt="hankcs.com 2017-06-07 下午2.47.31.png" title="hankcs.com 2017-06-07 下午2.47.31.png"></p>

<p>We are using conditional probability to describe how precise we can predict its context, our task is to maximize all of the conditional probabilities, when doing so, we can get its context well. then we can write down its (Likelihood function)</p>

<p><img src="https://zhengliangliang.files.wordpress.com/2018/08/2018-08-17_205348.jpg" alt="2018-08-17_205348.jpg"></p>

<p>Then the objective function will be :</p>

<p><img src="https://zhengliangliang.files.wordpress.com/2018/08/2018-08-17_205501.jpg" alt="2018-08-17_205501.jpg"></p>

<p>We take the negative log likelihood of the likelihood function, then we need to minimize the objective function.</p>

<p>So, how to calculate all the conditional probabilities?  we use softmax(the reason we use softmax is that it can map arbitrary values Xi to a probability ditribution Pi)</p>

<p><img src="https://zhengliangliang.files.wordpress.com/2018/08/2018-08-17_205806.jpg" alt="2018-08-17_205806.jpg"></p>

<p>Uo is one context word(outside word) and Vc is the vector of center words, and Uw is the whoe contexts words.</p>

<p>some fundamental math:</p>

<p><img src="https://zhengliangliang.files.wordpress.com/2018/08/2018-08-17_210256.jpg" alt="2018-08-17_210256.jpg"></p>

<p>And a ppt from manning can show all the stages of Skipgram:</p>

<p><img src="https://zhengliangliang.files.wordpress.com/2018/08/006Fmjmcly1fgco3v2ca7j30pq0j7drt.jpg" alt="2017-06-07_15-24-56.png" title="2017-06-07_15-24-56.png"></p>

<p>First we look up the center word from word embedding using one hot vector * word embedding matrix, the dot product result can be the representation of center word Vc, and then it times the output representation to calculate the similarity of every words with respect to Vc. then we doing the softmax to get the right probability.</p>

<p>First we should know how to normalizeRows: Implement a function that normalizes each row of a matrix to have unit length.</p>

<p>1 def normalizeRows(x):
 2    “”” Row normalization function
 3 
 4    Implement a function that normalizes each row of a matrix to have
 5    unit length.
 6    “””
 7 
 8    ### YOUR CODE HERE
 9    denominator = np.apply_along_axis(lambda x:np.sqrt(x.T.dot(x)),1,x)#跨列
10     x /= denominator[:,None] #将整个
11     #raise NotImplementedError
12     ### END YOUR CODE
13   <br>
14     return x</p>

<p>Then comes the softmaxCostAndGradient: First we calculate the dot product of the v_hat(predicted word or say center word) , then through softmax and cross entropy to calculate its loss, then doing the Gradient,in the function, we should return cost(loss) gradPred (gradients)for center word. and gradients for other word(outside word).</p>

<ul>
  <li>take the derivative wrt vc</li>
</ul>

<p><img src="https://zhengliangliang.files.wordpress.com/2018/08/2018-08-21_143055.jpg" alt="2018-08-21_143055.jpg"></p>

<p>U = [u1,u2,….uw] means the matrix made of all word vectors, y_hat - y means the probability vector.</p>

<ul>
  <li>tkae the derivative wrt U</li>
</ul>

<p><img src="https://zhengliangliang.files.wordpress.com/2018/08/2018-08-21_143328.jpg" alt="2018-08-21_143328.jpg"></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mi">1</span> <span class="k">def</span> <span class="nf">softmaxCostAndGradient</span><span class="p">(</span><span class="n">predicted</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">outputVectors</span><span class="p">,</span> <span class="n">dataset</span><span class="p">):</span>
 <span class="mi">2</span>    <span class="sh">"""</span><span class="s"> Softmax cost function for word2vec models
 3 
 4    Implement the cost and gradients for one predicted word vector
 5    and one target word vector as a building block for word2vec
 6    models, assuming the softmax prediction function and cross
 7    entropy loss.
 8 
 9    Arguments:
10     predicted -- numpy ndarray, predicted word vector (hat{v} in
11                  the written component)
12     target -- integer, the index of the target word
13     outputVectors -- </span><span class="sh">"</span><span class="s">output</span><span class="sh">"</span><span class="s"> vectors (as rows) for all tokens
14     dataset -- needed for negative sampling, unused here.
15 
16     Return:
17     cost -- cross entropy cost for the softmax word prediction
18     gradPred -- the gradient with respect to the predicted word
19            vector
20     grad -- the gradient with respect to all the other word
21            vectors
22 
23     We will not provide starter code for this function, but feel
24     free to reference the code you previously wrote for this
25     assignment!
26     </span><span class="sh">"""</span>
<span class="mi">27</span> 
<span class="mi">28</span>     <span class="c1">### YOUR CODE HERE
</span><span class="mi">29</span>     <span class="c1">#softmax
</span><span class="mi">30</span>     <span class="n">vhat</span> <span class="o">=</span> <span class="n">predicted</span> 
<span class="mi">31</span>     <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">outputVectors</span><span class="p">,</span><span class="n">vhat</span><span class="p">)</span>
<span class="mi">32</span>     <span class="n">preds</span> <span class="o">=</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<span class="mi">33</span>     <span class="c1"># cross entropy
</span><span class="mi">34</span>     <span class="n">cost</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">preds</span><span class="p">[</span><span class="n">target</span><span class="p">])</span>
<span class="mi">35</span>     
<span class="mi">36</span>     <span class="c1"># Gradient
</span><span class="mi">37</span>     <span class="n">z</span> <span class="o">=</span> <span class="n">preds</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
<span class="mi">38</span>     <span class="n">z</span><span class="p">[</span><span class="n">target</span><span class="p">]</span> <span class="o">-=</span> <span class="mf">1.0</span>
<span class="mi">39</span>     <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">z</span><span class="p">,</span><span class="n">vhat</span><span class="p">)</span> <span class="c1"># wrt outside words
</span><span class="mi">40</span>     <span class="n">gradPred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">outputVectors</span><span class="p">.</span><span class="n">T</span><span class="p">,</span><span class="n">z</span><span class="p">)</span> <span class="c1"># wrt center word
</span><span class="mi">41</span>     <span class="c1">### END YOUR CODE
</span><span class="mi">42</span> 
<span class="mi">43</span>     <span class="k">return</span> <span class="n">cost</span><span class="p">,</span> <span class="n">gradPred</span><span class="p">,</span> <span class="n">grad</span>
</code></pre></div></div>

<p>Then we implemented the skipgram part, all we have to do is also compute all the cost(loss) and gradients, and we got bunch of parameter in this function</p>

<ol>
  <li>
<strong>currentWord</strong> – a string of the current <strong>center</strong> word</li>
  <li>
<strong>C</strong> – integer, context size</li>
  <li>
<strong>contextWords</strong> – list of no more than 2*C strings, the context words</li>
  <li>
<strong>tokens</strong> – a dictionary that maps words to their indices in the word vector list(impotant in implementation)</li>
  <li>
<strong>inputVectors</strong> – “input” word vectors (as rows) for all tokens</li>
  <li>
<strong>outputVectors</strong> – “output” word vectors (as rows) for all tokens</li>
  <li>
<strong>word2vecCostAndGradient</strong> – the cost and gradient function for a prediction vector given the target word vectors, could be one of the two cost functions you implemented above.</li>
</ol>

<p>Recalled that the objective function is a neg log of the likelihood function:</p>

<p><img src="https://zhengliangliang.files.wordpress.com/2018/08/2018-08-17_205501.jpg" alt="2018-08-17_205501.jpg"></p>

<p>in the for loop, we just scan all the contextWords and calculate the gradient<img src="https://zhengliangliang.files.wordpress.com/2018/08/2018-08-21_144422.jpg" alt="2018-08-21_144422.jpg"></p>

<p>and all of the cost and gradient will be summation:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mi">1</span> <span class="k">def</span> <span class="nf">skipgram</span><span class="p">(</span><span class="n">currentWord</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">contextWords</span><span class="p">,</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">inputVectors</span><span class="p">,</span> <span class="n">outputVectors</span><span class="p">,</span>
 <span class="mi">2</span>             <span class="n">dataset</span><span class="p">,</span> <span class="n">word2vecCostAndGradient</span><span class="o">=</span><span class="n">softmaxCostAndGradient</span><span class="p">):</span>
 <span class="mi">3</span>    <span class="sh">"""</span><span class="s"> Skip-gram model in word2vec
 4 
 5    Implement the skip-gram model in this function.
 6 
 7    Arguments:
 8    currentWord -- a string of the current center word
 9    C -- integer, context size
10     contextWords -- list of no more than 2*C strings, the context words
11     tokens -- a dictionary that maps words to their indices in
12               the word vector list
13     inputVectors -- </span><span class="sh">"</span><span class="s">input</span><span class="sh">"</span><span class="s"> word vectors (as rows) for all tokens
14     outputVectors -- </span><span class="sh">"</span><span class="s">output</span><span class="sh">"</span><span class="s"> word vectors (as rows) for all tokens
15     word2vecCostAndGradient -- the cost and gradient function for
16                                a prediction vector given the target
17                                word vectors, could be one of the two
18                                cost functions you implemented above.
19 
20     Return:
21     cost -- the cost function value for the skip-gram model
22     grad -- the gradient with respect to the word vectors
23     </span><span class="sh">"""</span>
<span class="mi">24</span> 
<span class="mi">25</span>     <span class="n">cost</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="mi">26</span>     <span class="n">gradIn</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">inputVectors</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="mi">27</span>     <span class="n">gradOut</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">outputVectors</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="mi">28</span> 
<span class="mi">29</span>     <span class="c1">### YOUR CODE HERE
</span><span class="mi">30</span>     <span class="c1">#tokens是词到idx的映射 得到idx再回输入中去找到词向量
</span><span class="mi">31</span>     <span class="n">centerword_idx</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">[</span><span class="n">currentWord</span><span class="p">]</span>
<span class="mi">32</span>     <span class="n">vhat</span> <span class="o">=</span> <span class="n">inputVectors</span><span class="p">[</span><span class="n">centerword_idx</span><span class="p">]</span>
<span class="mi">33</span>     
<span class="mi">34</span>     <span class="c1"># 对每一个上个文的单词进行word2vec训练 计算累计cost与gradients
</span><span class="mi">35</span>     <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">contextWords</span><span class="p">:</span>
<span class="mi">36</span>         <span class="n">u_idx</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
<span class="mi">37</span>         <span class="n">c_cost</span><span class="p">,</span> <span class="n">c_grad_in</span><span class="p">,</span><span class="n">c_grad_out</span> <span class="o">=</span> 
<span class="mi">38</span>             <span class="nf">word2vecCostAndGradient</span><span class="p">(</span><span class="n">vhat</span><span class="p">,</span> <span class="n">u_idx</span><span class="p">,</span> <span class="n">outputVectors</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
<span class="mi">39</span>         <span class="n">cost</span> <span class="o">+=</span> <span class="n">c_cost</span>
<span class="mi">40</span>         <span class="n">gradIn</span><span class="p">[</span><span class="n">centerword_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="n">c_grad_in</span>
<span class="mi">41</span>         <span class="n">gradOut</span> <span class="o">+=</span> <span class="n">c_grad_out</span>
<span class="mi">42</span>     <span class="c1">#raise NotImplementedError
</span><span class="mi">43</span>     <span class="c1">### END YOUR CODE
</span><span class="mi">44</span> 
<span class="mi">45</span>     <span class="k">return</span> <span class="n">cost</span><span class="p">,</span> <span class="n">gradIn</span><span class="p">,</span> <span class="n">gradOut</span>
</code></pre></div></div>
<hr>

<p>Implement skipgram in tensorflow:</p>

<p>some packages needed to be imported:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mi">1</span> <span class="kn">import</span> <span class="n">os</span>
 <span class="mi">2</span> <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">'</span><span class="s">TF_CPP_MIN_LOG_LEVEL</span><span class="sh">'</span><span class="p">]</span><span class="o">=</span><span class="sh">'</span><span class="s">2</span><span class="sh">'</span>
 <span class="mi">3</span> 
 <span class="mi">4</span> <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
 <span class="mi">5</span> <span class="kn">from</span> <span class="n">tensorflow.contrib.tensorboard.plugins</span> <span class="kn">import</span> <span class="n">projector</span>
 <span class="mi">6</span> <span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
 <span class="mi">7</span> 
 <span class="mi">8</span> <span class="kn">import</span> <span class="n">utils</span>
 <span class="mi">9</span> <span class="kn">import</span> <span class="n">word2vec_utils</span>
</code></pre></div></div>
<p>First we need to know our model hyperparameters:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mi">1</span> <span class="c1"># Model hyperparameters
</span> <span class="mi">2</span> <span class="n">VOCAB_SIZE</span> <span class="o">=</span> <span class="mi">50000</span>
 <span class="mi">3</span> <span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">128</span>
 <span class="mi">4</span> <span class="n">EMBED_SIZE</span> <span class="o">=</span> <span class="mi">128</span>            <span class="c1"># dimension of the word embedding vectors
</span> <span class="mi">5</span> <span class="n">SKIP_WINDOW</span> <span class="o">=</span> <span class="mi">1</span>             <span class="c1"># the context window
</span> <span class="mi">6</span> <span class="n">NUM_SAMPLED</span> <span class="o">=</span> <span class="mi">64</span>            <span class="c1"># number of negative examples to sample
</span> <span class="mi">7</span> <span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">1.0</span>
 <span class="mi">8</span> <span class="n">NUM_TRAIN_STEPS</span> <span class="o">=</span> <span class="mi">100000</span>
 <span class="mi">9</span> <span class="n">VISUAL_FLD</span> <span class="o">=</span> <span class="sh">'</span><span class="s">visualization</span><span class="sh">'</span>
<span class="mi">10</span> <span class="n">SKIP_STEP</span> <span class="o">=</span> <span class="mi">5000</span>
</code></pre></div></div>
<p>In tensorflow, normally we will build a graph for model , in every def, we have a name_scope for valuable sharing:</p>

<p>in the SkipGramModel, or in any model, first we should do is to create an iterator to get dataset:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> 1 # Step 1: get input, output from the dataset
 2    iterator = dataset.make_initializable_iterator()
 3    center_words, target_words = iterator.get_next()
</code></pre></div></div>

<p>After this, we neen to define weights, and the weights are for embed matrix, and in this step we initialize it.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> 1 # Step 2: define weights. 
 2    # In word2vec, it's the weights that we care about
 3    embed_matrix = tf.get_variable('embed_matrix', 
 4                                    shape=[VOCAB_SIZE, EMBED_SIZE],
 5                                    initializer=tf.random_uniform_initializer())
</code></pre></div></div>

<p>notice that the shape is [VOCAB_SIZE, EMBED_SIZE], in the following step, we define the inference. This is a function in tf.nn , embedding——lookup means that we return the index(actually in this case ceter_words is just position,indices) in the embed_matrix.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> 1 # Step 3: define the inference
 2 embed = tf.nn.embedding_lookup(embed_matrix, center_words, name='embed')
</code></pre></div></div>
<p>Next step is to define loss function, recall that in skip_gram model, we use softmax for probability, and in this tf version, we use nce,which is another loss function.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mi">1</span>    <span class="c1"># Step 4: define loss function
</span> <span class="mi">2</span>    <span class="c1"># construct variables for NCE loss
</span> <span class="mi">3</span>    <span class="n">nce_weight</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">get_variable</span><span class="p">(</span><span class="sh">'</span><span class="s">nce_weight</span><span class="sh">'</span><span class="p">,</span> 
 <span class="mi">4</span>                                 <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">VOCAB_SIZE</span><span class="p">,</span> <span class="n">EMBED_SIZE</span><span class="p">],</span>
 <span class="mi">5</span>                                 <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="nf">truncated_normal_initializer</span><span class="p">(</span><span class="n">stddev</span><span class="o">=</span><span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">EMBED_SIZE</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)))</span>
 <span class="mi">6</span>    <span class="n">nce_bias</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">get_variable</span><span class="p">(</span><span class="sh">'</span><span class="s">nce_bias</span><span class="sh">'</span><span class="p">,</span> <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="nf">zeros</span><span class="p">([</span><span class="n">VOCAB_SIZE</span><span class="p">]))</span>
</code></pre></div></div>
<p>for the whole loss, we need to summation, and in tensorflow, simply we just convey nce loss in the tf.reduce_mean, when the model was trained, it will sum all of the loss automatically.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mi">1</span>    <span class="c1"># define loss function to be NCE loss function
</span> <span class="mi">2</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">nce_loss</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">nce_weight</span><span class="p">,</span> 
 <span class="mi">3</span>                                        <span class="n">biases</span><span class="o">=</span><span class="n">nce_bias</span><span class="p">,</span> 
 <span class="mi">4</span>                                        <span class="n">labels</span><span class="o">=</span><span class="n">target_words</span><span class="p">,</span> 
 <span class="mi">5</span>                                        <span class="n">inputs</span><span class="o">=</span><span class="n">embed</span><span class="p">,</span> 
 <span class="mi">6</span>                                        <span class="n">num_sampled</span><span class="o">=</span><span class="n">NUM_SAMPLED</span><span class="p">,</span> 
 <span class="mi">7</span>                                        <span class="n">num_classes</span><span class="o">=</span><span class="n">VOCAB_SIZE</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">loss</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>
<p>Then comes the optimizer, the reduce_mean and train_GradientDescentOptimizer function should be remembered since they are frequently used function.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> 1    # Step 5: define optimizer that follows gradient descent update rule
 2    # to minimize loss
 3    optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)
</code></pre></div></div>
<p>And for the training part: 1 . initiliaze ierator and variables,  and in each epoch, we sess.run the loss and optimizer, then print out loss in each 5000 step,  and the loss will be total_loss / 5000, in the code below you can find it : </p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mi">1</span>    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nc">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
 <span class="mi">2</span> 
 <span class="mi">3</span>        <span class="c1"># Step 6: initialize iterator and variables
</span> <span class="mi">4</span>        <span class="n">sess</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="n">iterator</span><span class="p">.</span><span class="n">initializer</span><span class="p">)</span>
 <span class="mi">5</span>        <span class="n">sess</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">global_variables_initializer</span><span class="p">())</span>
 <span class="mi">6</span> 
 <span class="mi">7</span>        <span class="n">total_loss</span> <span class="o">=</span> <span class="mf">0.0</span> 
 <span class="mi">8</span>        <span class="n">writer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">summary</span><span class="p">.</span><span class="nc">FileWriter</span><span class="p">(</span><span class="sh">'</span><span class="s">graphs/word2vec_simple</span><span class="sh">'</span><span class="p">,</span> <span class="n">sess</span><span class="p">.</span><span class="n">graph</span><span class="p">)</span>
 <span class="mi">9</span> 
<span class="mi">10</span>         <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">NUM_TRAIN_STEPS</span><span class="p">):</span>
<span class="mi">11</span>             <span class="k">try</span><span class="p">:</span>
<span class="mi">12</span>                 <span class="c1"># Step 7: execute optimizer and fetch loss
</span><span class="mi">13</span>                 <span class="n">loss_batch</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sess</span><span class="p">.</span><span class="nf">run</span><span class="p">([</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">])</span>
<span class="mi">14</span> 
<span class="mi">15</span>                 <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss_batch</span>
<span class="mi">16</span> 
<span class="mi">17</span>                 <span class="nf">if </span><span class="p">(</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">SKIP_STEP</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span class="mi">18</span>                     <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Average loss at step {}: {:5.1f}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="n">SKIP_STEP</span><span class="p">))</span>
<span class="mi">19</span>                     <span class="n">total_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="mi">20</span>             <span class="k">except</span> <span class="n">tf</span><span class="p">.</span><span class="n">errors</span><span class="p">.</span><span class="n">OutOfRangeError</span><span class="p">:</span>
<span class="mi">21</span>                 <span class="n">sess</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="n">iterator</span><span class="p">.</span><span class="n">initializer</span><span class="p">)</span>
<span class="mi">22</span>         <span class="n">writer</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
</code></pre></div></div>
<p>After training on spyder, the result:</p>

<p><img src="https://zhengliangliang.files.wordpress.com/2018/08/2018-08-21_163627.jpg" alt="2018-08-21_163627.jpg"></p>

    </div>
  </article>

  

  

  
    
      

  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2>
    <p class="mb-2">Here are some more articles you might like to read next:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/displaying-external-posts-on-your-al-folio-blog/">Displaying External Posts on Your al-folio Blog</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/What-is-Mathematics-CH1-Solution/">What is Mathematics: Solution Chapter 1</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/small-guide-to-supplements-what-you-need-to-know/">A small guide to supplements: What you need to know</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/%E6%B7%B7%E4%B9%B1%E4%B8%8E%E7%A7%A9%E5%BA%8F/">混乱与秩序</a>
  </li>


    
  

  
  
    <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;">
  
    <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'zhengstar94/zhengstar94.github.io',
        'data-repo-id': 'R_kgDOMJ4qqQ',
        'data-category': 'Comments',
        'data-category-id': 'DIC_kwDOMJ4qqc4CgHUg',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a>
</noscript>
  
</div>

  
</div>
</div>
          </div>
        
      
    </div>

    <!-- Footer -->
    
  <footer class="fixed-bottom" role="contentinfo">
    <div class="container mt-0">
      © Copyright 2024
      Xingxing
      
      Zheng. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      
      
    </div>
  </footer>



    <!-- JavaScripts -->
    <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
<script src="/assets/js/bootstrap.bundle.min.js"></script>
<!-- <script src="/assets/js/mdb.min.js"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
  <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>


    

    

    

    

    

    

    

    

    

  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script>


  <!-- Sidebar Table of Contents -->
  <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script>


<!-- Bootstrap Table -->


<!-- Load Common JS -->
<script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script>
<script defer src="/assets/js/common.js?06cae41083477f121be8cd9797ad8e2f"></script>
<script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script>

<!-- Jupyter Open External Links New Tab -->
<script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script>



    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>


  <script async src="https://badge.dimensions.ai/badge.js"></script>


    
  
    <!-- MathJax -->
    <script type="text/javascript">
      window.MathJax = {
        tex: {
          tags: 'ams',
        },
      };
    </script>
    <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script>
    <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script>
  


    

    


    
  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    /*
     * This JavaScript code has been adapted from the article
     * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar,
     * published on the website https://css-tricks.com on the 7th of May, 2014.
     * Couple of changes were made to the original code to make it compatible
     * with the `al-foio` theme.
     */
    const progressBar = $('#progress');
    /*
     * We set up the bar after all elements are done loading.
     * In some cases, if the images in the page are larger than the intended
     * size they'll have on the page, they'll be resized via CSS to accomodate
     * the desired size. This mistake, however, breaks the computations as the
     * scroll size is computed as soon as the elements finish loading.
     * To account for this, a minimal delay was introduced before computing the
     * values.
     */
    window.onload = function () {
      setTimeout(progressBarSetup, 50);
    };
    /*
     * We set up the bar according to the browser.
     * If the browser supports the progress element we use that.
     * Otherwise, we resize the bar thru CSS styling
     */
    function progressBarSetup() {
      if ('max' in document.createElement('progress')) {
        initializeProgressElement();
        $(document).on('scroll', function () {
          progressBar.attr({ value: getCurrentScrollPosition() });
        });
        $(window).on('resize', initializeProgressElement);
      } else {
        resizeProgressBar();
        $(document).on('scroll', resizeProgressBar);
        $(window).on('resize', resizeProgressBar);
      }
    }
    /*
     * The vertical scroll position is the same as the number of pixels that
     * are hidden from view above the scrollable area. Thus, a value > 0 is
     * how much the user has scrolled from the top
     */
    function getCurrentScrollPosition() {
      return $(window).scrollTop();
    }

    function initializeProgressElement() {
      let navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      progressBar.css({ top: navbarHeight });
      progressBar.attr({
        max: getDistanceToScroll(),
        value: getCurrentScrollPosition(),
      });
    }
    /*
     * The offset between the html document height and the browser viewport
     * height will be greater than zero if vertical scroll is possible.
     * This is the distance the user can scroll
     */
    function getDistanceToScroll() {
      return $(document).height() - $(window).height();
    }

    function resizeProgressBar() {
      progressBar.css({ width: getWidthPercentage() + '%' });
    }
    // The scroll ratio equals the percentage to resize the bar
    function getWidthPercentage() {
      return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
    }
  </script>


    

    

    

    
  <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script>
  <script>
    addBackToTop();
  </script>


    
  <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script>
  <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys>
  <script>
    let searchTheme = determineComputedTheme();
    const ninjaKeys = document.querySelector('ninja-keys');

    if (searchTheme === 'dark') {
      ninjaKeys.classList.add('dark');
    } else {
      ninjaKeys.classList.remove('dark');
    }

    const openSearchModal = () => {
      // collapse navbarNav if expanded on mobile
      const $navbarNav = $('#navbarNav');
      if ($navbarNav.hasClass('show')) {
        $navbarNav.collapse('hide');
      }
      ninjaKeys.open();
    };
  </script>
  <script>
    // get the ninja-keys element
    const ninja = document.querySelector('ninja-keys');

    // add the home and posts menu items
    ninja.data = [{
        id: "nav-about",
        title: "about",
        section: "Navigation",
        handler: () => {
          window.location.href = "/";
        },
      },{id: "nav-blog",
              title: "blog",
              description: "",
              section: "Navigation",
              handler: () => {
                window.location.href = "/blog/";
              },
            },{id: "nav-repositories",
              title: "repositories",
              description: "",
              section: "Navigation",
              handler: () => {
                window.location.href = "/repositories/";
              },
            },{id: "nav-cv",
              title: "CV",
              description: "Personal CV, updated on 15 Jun, 2024",
              section: "Navigation",
              handler: () => {
                window.location.href = "/cv/";
              },
            },{id: "dropdown-by-year",
                  title: "by year",
                  description: "",
                  section: "Dropdown",
                  handler: () => {
                    window.location.href = "";
                  },
                },{id: "dropdown-by-category",
                  title: "by category",
                  description: "",
                  section: "Dropdown",
                  handler: () => {
                    window.location.href = "";
                  },
                },{id: "post-google-gemini-updates-flash-1-5-gemma-2-and-project-astra",
          
            // TODO: fix the svg icon position for external posts
            // title: 'Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra <svg width="2rem" height="2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',
            title: "Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra",
          
          description: "We’re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.",
          section: "Posts",
          handler: () => {
            
              window.open("https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/", "_blank");
            
          },
        },{id: "post-what-is-mathematics-solution-chapter-1",
          
            title: "What is Mathematics: Solution Chapter 1",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2024/What-is-Mathematics-CH1-Solution/";
            
          },
        },{id: "post-a-small-guide-to-supplements-what-you-need-to-know",
          
            title: "A small guide to supplements: What you need to know",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2023/small-guide-to-supplements-what-you-need-to-know/";
            
          },
        },{id: "post-混乱与秩序",
          
            title: "混乱与秩序",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2022/%E6%B7%B7%E4%B9%B1%E4%B8%8E%E7%A7%A9%E5%BA%8F/";
            
          },
        },{id: "post-podcast-notes-huberman-lab-dopomine",
          
            title: "Podcast Notes: Huberman Lab Dopomine",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2022/podcast-notes-huberman-lab-dopomine/";
            
          },
        },{id: "post-displaying-external-posts-on-your-al-folio-blog",
          
            // TODO: fix the svg icon position for external posts
            // title: 'Displaying External Posts on Your al-folio Blog <svg width="2rem" height="2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',
            title: "Displaying External Posts on Your al-folio Blog",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.open("https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2", "_blank");
            
          },
        },{id: "post-erc-721",
          
            title: "ERC 721",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2022/erc-721/";
            
          },
        },{id: "post-cost-functions-and-its-properties-in-deep-learning-tbc",
          
            title: "Cost Functions and its properties in Deep Learning (TBC)",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2022/cost-functions-and-its-properties-in-deep-learning-tbc/";
            
          },
        },{id: "post-cv-modifying",
          
            title: "CV: Modifying",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2022/cv-modifying/";
            
          },
        },{id: "post-cheatsheet-pandas-dataframe-commonly-used",
          
            title: "Cheatsheet: Pandas, Dataframe, (commonly used)",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/cheatsheet-pandas-dataframe-seriescommonly-used/";
            
          },
        },{id: "post-人生何处不滥情",
          
            title: "人生何处不滥情",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/%E4%BA%BA%E7%94%9F%E4%BD%95%E5%A4%84%E4%B8%8D%E6%BB%A5%E6%83%85/";
            
          },
        },{id: "post-hash-bang-bin-bash-tbc",
          
            title: "hash bang/bin/bash (tbc)",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/hash-bang-bin-bash-tbc/";
            
          },
        },{id: "post-mit-15-s12-blockchain-and-money-note-lec-7-9-tbc",
          
            title: "MIT 15.S12 Blockchain and Money Note (Lec 7~9) tbc",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/mit-15-s12-blockchain-and-money-note-lec-79-tbc/";
            
          },
        },{id: "post-mit-15-s12-blockchain-and-money-note-lec-6",
          
            title: "MIT 15.S12 Blockchain and Money Note (Lec 6)",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/mit-15-s12-blockchain-and-money-note-lec-6-tbc/";
            
          },
        },{id: "post-mit-15-s12-blockchain-and-money-note-lec-5",
          
            title: "MIT 15.S12 Blockchain and Money Note (Lec 5)",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/mit-15-s12-blockchain-and-money-note-lec-5/";
            
          },
        },{id: "post-awk-sed-grep-总结",
          
            title: "awk, sed, grep 总结",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/awk-sed-grep-%E6%80%BB%E7%BB%93/";
            
          },
        },{id: "post-mit-15-s12-blockchain-and-money-note-lec-4",
          
            title: "MIT 15.S12 Blockchain and Money Note (Lec 4)",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/mit-15-s12-blockchain-and-money-note-lec-4/";
            
          },
        },{id: "post-mit-15-s12-blockchain-and-money-note-lec-3",
          
            title: "MIT 15.S12 Blockchain and Money Note (Lec 3)",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/mit-15-s12-blockchain-and-money-note-lec-3/";
            
          },
        },{id: "post-mit-15-s12-blockchain-and-money-note-lec-2",
          
            title: "MIT 15.S12 Blockchain and Money Note (Lec 2)",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/mit-15-s12-blockchain-and-money-note-lec-2/";
            
          },
        },{id: "post-mit-15-s12-blockchain-and-money-note-lec-1",
          
            title: "MIT 15.S12 Blockchain and Money Note (Lec 1)",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/15-s12-blockchain-and-money-note1/";
            
          },
        },{id: "post-浅尝-39-则-39-止-正则",
          
            title: "浅尝'则'止: 正则",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/%E6%B5%85%E5%B0%9D%E5%88%99%E6%AD%A2-%E6%AD%A3%E5%88%99%E4%B8%8E%E5%85%B6%E5%BC%95%E6%93%8E%E5%AE%9E%E7%8E%B0tbc/";
            
          },
        },{id: "post-python巩固整理",
          
            title: "Python巩固整理",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2021/python%E8%BF%9B%E9%98%B6%E5%B7%A9%E5%9B%BA%E6%95%B4%E7%90%86/";
            
          },
        },{id: "post-别虽然但是了-2020过去了",
          
            title: "别虽然但是了，2020过去了．",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2020/%E5%88%AB%E8%99%BD%E7%84%B6%E4%BD%86%E6%98%AF%E4%BA%86-2020%E8%BF%87%E5%8E%BB%E4%BA%86/";
            
          },
        },{id: "post-sql规范与技巧-持续更新",
          
            title: "SQL规范与技巧(持续更新)",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2020/sql%E8%A7%84%E8%8C%83%E4%B8%8E%E6%8A%80%E5%B7%A7/";
            
          },
        },{id: "post-金融读书笔记-3",
          
            title: "金融读书笔记(3)",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2020/%E9%87%91%E8%9E%8D%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B03/";
            
          },
        },{id: "post-金融读书笔记-2",
          
            title: "金融读书笔记(2)",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2020/%E9%87%91%E8%9E%8D%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B02/";
            
          },
        },{id: "post-金融读书笔记-1",
          
            title: "金融读书笔记(1)",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2020/%E9%87%91%E8%9E%8D%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B01/";
            
          },
        },{id: "post-gpt-3是什么",
          
            title: "GPT-3是什么?",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2020/gpt-3/";
            
          },
        },{id: "post-前脑白质切除术",
          
            title: "前脑白质切除术",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2020/%E5%89%8D%E8%84%91%E7%99%BD%E8%B4%A8%E5%88%87%E9%99%A4%E6%9C%AF/";
            
          },
        },{id: "post-personal-github",
          
            title: "Personal Github",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2020/person-github/";
            
          },
        },{id: "post-永别了呀2019",
          
            title: "永别了呀2019",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2019/%E6%B0%B8%E5%88%AB%E4%BA%86%E5%91%802019/";
            
          },
        },{id: "post-emacs-入门-占坑",
          
            title: "Emacs 入门(占坑)",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2019/emacs-%E5%85%A5%E9%97%A8%E5%8D%A0%E5%9D%91/";
            
          },
        },{id: "post-我在备忘录都写些什么",
          
            title: "我在备忘录都写些什么？",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2019/%E6%8C%96%E5%9D%91%E8%BF%87%E5%8E%BB%E7%9A%84%E4%B8%80%E5%B9%B4/";
            
          },
        },{id: "post-adam-vs-radam",
          
            title: "Adam vs. RAdam",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2019/%E5%8D%A0%E5%9D%91-adam-vs-radam/";
            
          },
        },{id: "post-硕士开学第一学期-关于琐事和学习及方法",
          
            title: "硕士开学第一学期——关于琐事和学习及方法",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2018/%E7%A1%95%E5%A3%AB%E5%BC%80%E5%AD%A6%E7%AC%AC%E4%B8%80%E5%AD%A6%E6%9C%9F-%E5%85%B3%E4%BA%8E%E7%90%90%E4%BA%8B%E5%92%8C%E5%AD%A6%E4%B9%A0%E5%8F%8A%E6%96%B9%E6%B3%95/";
            
          },
        },{id: "post-emotion-recognition-based-on-tensorflow",
          
            title: "Emotion Recognition Based on Tensorflow",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2018/emotion-recognition-based-on-tensorflow/";
            
          },
        },{id: "post-python-review-amp-amp-some-simple-algorithms",
          
            title: "Python_review &amp; Some simple algorithms",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2018/python_review-some-simple-algorithms/";
            
          },
        },{id: "post-nlp-word2vec-skip-gram-cs224n-implemented-in-raw-way-and-in-tensorflow",
          
            title: "NLP: Word2Vec Skip-Gram(CS224n) implemented in raw way and in Tensorflow",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2018/nlp-word2vec-skip-gramcs224n/";
            
          },
        },{id: "post-cs231n-assignment3-image-captioning-with-rnns",
          
            title: "CS231n Assignment3_Image Captioning with RNNs",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2018/cs231n-assignment3_image-captioning-with-rnns/";
            
          },
        },{id: "post-pytorch-vs-tensorflow",
          
            title: "Pytorch VS Tensorflow",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2018/pytorch-vs-tensorflow/";
            
          },
        },{id: "post-cs231n-cnn-notes-amp-amp-assignment2",
          
            title: "CS231N_CNN_Notes&amp;Assignment2",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2018/cs231n_cnn_notesassignment2/";
            
          },
        },{id: "post-web-crawler爬top100电影信息",
          
            title: "Web_Crawler爬top100电影信息",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2018/web_crawler%E7%88%ACtop100%E7%94%B5%E5%BD%B1%E4%BF%A1%E6%81%AF/";
            
          },
        },{id: "post-牛客算法直播题目总结day2t1234",
          
            title: "牛客算法直播题目总结Day2T1234",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2017/%E7%89%9B%E5%AE%A2%E7%AE%97%E6%B3%95%E7%9B%B4%E6%92%AD%E9%A2%98%E7%9B%AE%E6%80%BB%E7%BB%93day2t1234/";
            
          },
        },{id: "post-牛客算法直播题目总结day1t234",
          
            title: "牛客算法直播题目总结Day1T234",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2017/%E7%89%9B%E5%AE%A2%E7%AE%97%E6%B3%95%E7%9B%B4%E6%92%AD%E9%A2%98%E7%9B%AE%E6%80%BB%E7%BB%93day1t234%E9%9B%B6%E5%92%8C%E5%8D%9A%E5%BC%88/";
            
          },
        },{id: "post-牛客算法直播题目总结day1t1-零和博弈",
          
            title: "牛客算法直播题目总结Day1T1(零和博弈)",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2017/%E7%89%9B%E5%AE%A2%E7%AE%97%E6%B3%95%E7%9B%B4%E6%92%AD%E9%A2%98%E7%9B%AE%E6%80%BB%E7%BB%93/";
            
          },
        },{id: "post-java学习笔记-六-装饰-观察者模式及监听器",
          
            title: "Java学习笔记(六):装饰、观察者模式及监听器",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2017/java%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E5%85%AD%E8%A3%85%E9%A5%B0-%E8%A7%82%E5%AF%9F%E8%80%85%E6%A8%A1%E5%BC%8F%E5%8F%8A%E7%9B%91%E5%90%AC%E5%99%A8/";
            
          },
        },{id: "post-java学习笔记-四-filter",
          
            title: "Java学习笔记(四):Filter",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2017/java%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E5%9B%9Bfilter/";
            
          },
        },{id: "post-java学习笔记-五-文件的上传和下载",
          
            title: "Java学习笔记(五):文件的上传和下载",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2017/java%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%BA%94%E6%96%87%E4%BB%B6%E7%9A%84%E4%B8%8A%E4%BC%A0%E5%92%8C%E4%B8%8B%E8%BD%BD/";
            
          },
        },{id: "post-jdbc学习笔记-三",
          
            title: "JDBC学习笔记(三)",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2017/jdbc%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%89/";
            
          },
        },{id: "post-jdbc学习笔记-二",
          
            title: "JDBC学习笔记(二)",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2017/jdbc%E7%AC%94%E8%AE%B0%E4%BA%8C/";
            
          },
        },{id: "post-jdbc学习笔记-一",
          
            title: "JDBC学习笔记(一)",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2017/jdbc%E7%AC%94%E8%AE%B0%E5%8F%8A%E5%85%B6%E4%BB%96/";
            
          },
        },{id: "post-j2ee-cookie与session知识点回顾",
          
            title: "J2EE_Cookie与Session知识点回顾",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2017/j2ee_cookie%E4%B8%8Esession%E7%9F%A5%E8%AF%86%E7%82%B9%E5%9B%9E%E9%A1%BE/";
            
          },
        },{id: "post-bug-解决-nginx-502-bad-gateway",
          
            title: "Bug：解决 nginx 502 Bad GateWay",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2017/bug-%E8%A7%A3%E5%86%B3-nginx-502-bad-gateway/";
            
          },
        },{id: "post-windows-laravel环境部署",
          
            title: "Windows Laravel环境部署",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2016/windows-laravel%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/";
            
          },
        },{id: "post-git基础记录",
          
            title: "Git基础记录",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2016/git%E5%9F%BA%E7%A1%80%E8%AE%B0%E5%BD%95/";
            
          },
        },{id: "post-算法导论读书笔记-6",
          
            title: "算法导论读书笔记(6)",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2016/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B06/";
            
          },
        },{id: "post-qt实现电子词典gui-project",
          
            title: "QT实现电子词典GUI（project）",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2016/qt%E5%AE%9E%E7%8E%B0%E7%94%B5%E5%AD%90%E8%AF%8D%E5%85%B8gui-project/";
            
          },
        },{id: "post-算法导论读书笔记-5",
          
            title: "算法导论读书笔记(5)",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2016/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B05/";
            
          },
        },{id: "post-算法导论读书笔记-4",
          
            title: "算法导论读书笔记(4)",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2016/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B04/";
            
          },
        },{id: "post-算法导论读书笔记-3",
          
            title: "算法导论读书笔记(3)",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2016/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B03/";
            
          },
        },{id: "post-算法导论读书笔记-2",
          
            title: "算法导论读书笔记(2)",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2016/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B02/";
            
          },
        },{id: "post-aws免费服务器申请和中继器搭建",
          
            title: "AWS免费服务器申请和中继器搭建",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2016/aws%E5%85%8D%E8%B4%B9%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%94%B3%E8%AF%B7%E5%92%8C%E4%B8%AD%E7%BB%A7%E5%99%A8%E6%90%AD%E5%BB%BA/";
            
          },
        },{id: "post-算法导论读书笔记-1",
          
            title: "算法导论读书笔记（1）",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2016/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-1/";
            
          },
        },{id: "post-数据结构之-排序",
          
            title: "数据结构之——排序",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2015/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B-%E6%8E%92%E5%BA%8F/";
            
          },
        },{id: "post-数据结构之-图",
          
            title: "数据结构之——图",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2015/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B-%E5%9B%BE/";
            
          },
        },{id: "post-stl之vector用法",
          
            title: "STL之Vector用法",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2015/stl%E4%B9%8Bvector%E7%94%A8%E6%B3%95/";
            
          },
        },{id: "post-数据结构之-树",
          
            title: "数据结构之——树",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2015/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B-%E6%A0%91/";
            
          },
        },{id: "post-回溯法经典应用-n皇后-amp-amp-迷宫问题",
          
            title: "回溯法经典应用——N皇后&amp;迷宫问题",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2015/%E5%9B%9E%E6%BA%AF%E6%B3%95%E7%BB%8F%E5%85%B8%E5%BA%94%E7%94%A8-n%E7%9A%87%E5%90%8E%E8%BF%B7%E5%AE%AB%E9%97%AE%E9%A2%98/";
            
          },
        },{id: "post-数据结构之-队列",
          
            title: "数据结构之——队列",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2015/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B-%E9%98%9F%E5%88%97/";
            
          },
        },{id: "post-数据结构之-栈",
          
            title: "数据结构之——栈",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2015/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B-%E6%A0%88/";
            
          },
        },{id: "post-大二第一学期计划-备表及其他",
          
            title: "大二第一学期计划/备表及其他",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2015/%E5%A4%A7%E4%BA%8C%E7%AC%AC%E4%B8%80%E5%AD%A6%E6%9C%9F%E8%AE%A1%E5%88%92%E5%A4%87%E8%A1%A8%E5%8F%8A%E5%85%B6%E4%BB%96/";
            
          },
        },{id: "post-利用靓汤beautifulsoup4写一个简易爬虫",
          
            title: "利用靓汤BeautifulSoup4写一个简易爬虫",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2015/%E5%88%A9%E7%94%A8%E9%9D%93%E6%B1%A4beautifulsoup4%E5%86%99%E4%B8%80%E4%B8%AA%E7%AE%80%E6%98%93%E7%88%AC%E8%99%AB/";
            
          },
        },{id: "post-数据结构之-链表",
          
            title: "数据结构之——链表",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2015/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B-%E9%93%BE%E8%A1%A8/";
            
          },
        },{id: "post-数据结构之-向量",
          
            title: "数据结构之——向量",
          
          description: "",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2015/vector/";
            
          },
        },{id: "news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",
          title: 'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',
          description: "",
          section: "News",},{id: "news-a-long-announcement-with-details",
          title: 'A long announcement with details',
          description: "",
          section: "News",handler: () => {
              window.location.href = "/news/announcement_2/";
            },},{id: "news-a-simple-inline-announcement",
          title: 'A simple inline announcement.',
          description: "",
          section: "News",},{id: "projects-project-1",
              title: 'project 1',
              description: "with background image",
              section: "Projects",handler: () => {
                  window.location.href = "/projects/1_project/";
                },},{id: "projects-project-2",
              title: 'project 2',
              description: "a project with a background image and giscus comments",
              section: "Projects",handler: () => {
                  window.location.href = "/projects/2_project/";
                },},{id: "projects-project-3-with-very-long-name",
              title: 'project 3 with very long name',
              description: "a project that redirects to another website",
              section: "Projects",handler: () => {
                  window.location.href = "/projects/3_project/";
                },},{id: "projects-project-4",
              title: 'project 4',
              description: "another without an image",
              section: "Projects",handler: () => {
                  window.location.href = "/projects/4_project/";
                },},{id: "projects-project-5",
              title: 'project 5',
              description: "a project with a background image",
              section: "Projects",handler: () => {
                  window.location.href = "/projects/5_project/";
                },},{id: "projects-project-6",
              title: 'project 6',
              description: "a project with no image",
              section: "Projects",handler: () => {
                  window.location.href = "/projects/6_project/";
                },},{id: "projects-project-7",
              title: 'project 7',
              description: "with background image",
              section: "Projects",handler: () => {
                  window.location.href = "/projects/7_project/";
                },},{id: "projects-project-8",
              title: 'project 8',
              description: "an other project with a background image and giscus comments",
              section: "Projects",handler: () => {
                  window.location.href = "/projects/8_project/";
                },},{id: "projects-project-9",
              title: 'project 9',
              description: "another project with an image 🎉",
              section: "Projects",handler: () => {
                  window.location.href = "/projects/9_project/";
                },},{
            id: 'socials-email',
            title: 'Send email',
            section: 'Socials',
            handler: () => {
              window.open("mailto:%7A%68%65%6E%67%73%74%61%72%73@%67%6D%61%69%6C.%63%6F%6D", "_blank");
            },
          },{
            id: 'socials-rss',
            title: 'RSS Feed',
            section: 'Socials',
            handler: () => {
              window.open("/feed.xml", "_blank");
            },
          },{
          id: 'light-theme',
          title: 'Change theme to light',
          description: 'Change the theme of the site to Light',
          section: 'Theme',
          handler: () => {
            setThemeSetting("light");
          },
        },
        {
          id: 'dark-theme',
          title: 'Change theme to dark',
          description: 'Change the theme of the site to Dark',
          section: 'Theme',
          handler: () => {
            setThemeSetting("dark");
          },
        },
        {
          id: 'system-theme',
          title: 'Use system default theme',
          description: 'Change the theme of the site to System Default',
          section: 'Theme',
          handler: () => {
            setThemeSetting("system");
          },
        },];
  </script>


    <script src="/assets/js/shortcut-key.js"></script>
  </body>
</html>
